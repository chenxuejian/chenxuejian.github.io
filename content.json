{"meta":{"title":"Adolf","subtitle":"xxx","description":"曾国藩：一勤天下无难事","author":"Adolf","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2019-02-20T22:27:16.000Z","updated":"2021-01-26T16:33:18.018Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"个人信息 姓名：陈xx 现居：北京 年龄：不详 联系方式: Email: &#54;&#51;&#50;&#56;&#52;&#48;&#x38;&#49;&#x35;&#x40;&#113;&#113;&#x2e;&#99;&#x6f;&#x6d; 工作 2012.03–2015.11 （乐视网） 2015.11–2020.06 （灵雀云） 2020.08–至今 （又是一家互联网公司） 摄影一位爱好摄影的瞎拍者： 目前所用设备：iPhoneX 目前展示网站：朋友圈，Ins，微博等。 坚持的原则 说「谢谢」，即使那是别人的义务。 知之为知之,不知为不知。 不使用抖音、快手等，但不对使用的人有偏见。 再一再二不再三。 对任何人都保持尊重。 要真诚。 灵感来自《2019 年，如何成为一个更好的人？我们给出 100 条真诚建议 | 100 个生活大问题》"},{"title":"categories","date":"2021-01-26T02:33:12.000Z","updated":"2021-01-28T04:32:43.706Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Link","date":"2021-01-26T14:36:08.000Z","updated":"2021-01-26T14:36:37.485Z","comments":true,"path":"link/index.html","permalink":"http://example.com/link/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-01-26T02:33:30.000Z","updated":"2021-01-26T14:24:43.380Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"如何有效的修改Linux云服务器的hosts配置","slug":"如何有效的修改Linux云服务器的hosts配置","date":"2021-01-28T06:42:35.000Z","updated":"2021-01-28T07:03:09.822Z","comments":true,"path":"2021/01/28/如何有效的修改Linux云服务器的hosts配置/","link":"","permalink":"http://example.com/2021/01/28/%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E7%9A%84%E4%BF%AE%E6%94%B9Linux%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84hosts%E9%85%8D%E7%BD%AE/","excerpt":"","text":"操作场景在2018年3月1号之后，腾讯云官网提供的 Linux 公有镜像预安装了纯开源的工具 Cloud-Init，并通过 Cloud-Init 实现了实例的所有初始化操作，使得整个实例内部的操作更加的透明，详情请参见 Cloud-Init。Cloud-Init 在每次启动时会根据 /etc/cloud/templates/hosts.${os_type}.tmpl 模板生成一份新的 /etc/hosts 文件覆盖实例原有的 /etc/hosts 文件，导致用户在实例内部手动修改 /etc/hosts 配置并重启实例后， /etc/hosts 配置又变为原始默认配置。 前提条件腾讯云针对 Cloud-Init 的覆盖操作已经做了优化，2018年9月后使用公共镜像创建的实例不会出现 /etc/hosts 配置在重启后被覆盖的问题。若您的实例创建于2018年9月前，请通过下面的解决方案进行修改。 操作步骤方案一 登录 Linux 服务器。 执行以下命令，将 /etc/cloud/cloud.cfg 配置文件中的 - update_etc_hosts 修改为 - [‘update-etc-hosts’, ‘once-per-instance’]。 1sed -i &quot;&#x2F;update_etc_hosts&#x2F;c \\ - [&#39;update_etc_hosts&#39;, &#39;once-per-instance&#39;]&quot; &#x2F;etc&#x2F;cloud&#x2F;cloud.cfg 执行以下命令，在 /var/lib/cloud/instance/sem/ 路径下创建 config_update_etc_hosts 文件。 1touch &#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instance&#x2F;sem&#x2F;config_update_etc_hosts 方案二 注：此方案以 CentOS7.2 操作系统为例。 获取 hosts 模版文件路径 登录 Linux 服务器。 执行以下命令，查看系统 hosts 模版文件。 1cat &#x2F;etc&#x2F;hosts hosts 模版文件如下图所示： 修改 hosts 模版文件 注：以添加 127.0.0.1 test test 为例，您可按需修改 hosts 模版文件与 /etc/hosts 文件。 执行以下命令，修改 hosts 模版文件。1vim &#x2F;etc&#x2F;cloud&#x2F;templates&#x2F;hosts.redhat.tmpl 按 “i” 切换至编辑模式。 在文件末尾输入以下内容。1127.0.0.1 test test 输入完成后，按 “Esc” ，输入 “:wq”，保存文件并返回。 修改 /etc/hosts 文件 执行以下命令，修改 /etc/hosts 文件。1vim &#x2F;etc&#x2F;hosts 按 “i” 切换至编辑模式。 在文件末尾输入以下内容。1127.0.0.1 test test 输入完成后，按 “Esc” ，输入 “:wq”，保存文件并返回。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"腾讯云","slug":"腾讯云","permalink":"http://example.com/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"}]},{"title":"hexo中图片无法加载","slug":"hexo中图片无法加载","date":"2021-01-26T15:59:21.000Z","updated":"2021-01-26T16:24:21.163Z","comments":true,"path":"2021/01/26/hexo中图片无法加载/","link":"","permalink":"http://example.com/2021/01/26/hexo%E4%B8%AD%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD/","excerpt":"","text":"配置开关首先确认根目录_config.yml中有: 1post_asset_folder: true 安装工具然后在blog/下执行 1npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save 创建同名的图片目录确保在blog/source/_posts下创建和md文件同名的目录，在里面放该md需要的图片，然后在md中插入 1![](目录名&#x2F;文件名.png) 修改md文件在md文件中插入图片时只需写 1![](post1&#x2F;pic1.png) 配置生效配置完成之后,执行下列操作 123$ hexo c$ hexo g$ hexo s","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-01-25T10:57:47.958Z","updated":"2021-01-25T10:57:47.958Z","comments":true,"path":"2021/01/25/hello-world/","link":"","permalink":"http://example.com/2021/01/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"SLUB:Unable to Allocate Memory","slug":"SLUB:Unable to allocate memory","date":"2020-04-22T05:40:38.000Z","updated":"2021-01-27T02:25:26.042Z","comments":true,"path":"2020/04/22/SLUB:Unable to allocate memory/","link":"","permalink":"http://example.com/2020/04/22/SLUB:Unable%20to%20allocate%20memory/","excerpt":"","text":"故障 如图：系统日志中报出不能分配内存 解决方法： 临时解决：重启相关节点 永久解决： 升级内核：3.10.0-1062.XXX.el7.x86_64123yum provides kernelyum install -y kernel-3.10.0-1062.9.1.el7.x86_64awk -F\\&#39; &#39;$1&#x3D;&#x3D;&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#39; &#x2F;etc&#x2F;grub2.cfg 添加内核参数 cgroup.memory=nokmem 1234567891011[root@acp2-node-1 ~]# cat &#x2F;etc&#x2F;default&#x2F;grubGRUB_TIMEOUT&#x3D;5GRUB_DISTRIBUTOR&#x3D;&quot;$(sed &#39;s, release .*$,,g&#39; &#x2F;etc&#x2F;system-release)&quot;GRUB_DEFAULT&#x3D;savedGRUB_DISABLE_SUBMENU&#x3D;trueGRUB_TERMINAL&#x3D;&quot;serial console&quot;GRUB_TERMINAL_OUTPUT&#x3D;&quot;serial console&quot;GRUB_CMDLINE_LINUX&#x3D;&quot;crashkernel&#x3D;auto cgroup.memory&#x3D;nokmem console&#x3D;ttyS0 console&#x3D;tty0 panic&#x3D;5 net.ifnames&#x3D;0 biosdevname&#x3D;0&quot;GRUB_DISABLE_RECOVERY&#x3D;&quot;true&quot;GRUB_SERIAL_COMMAND&#x3D;&quot;serial --speed&#x3D;9600 --unit&#x3D;0 --word&#x3D;8 --parity&#x3D;no --stop&#x3D;1&quot;[root@acp2-node-1 ~]# 保存退出，刷新grup菜单，并重启节点使永久生效12[root@acp2-node-1 ~]# grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg[root@acp2-node-1 ~]# reboot 查看是否生效12[root@region ~]# cat &#x2F;proc&#x2F;cmdlineBOOT_IMAGE&#x3D;&#x2F;vmlinuz-3.10.0-1062.9.1.el7.x86_64 root&#x3D;&#x2F;dev&#x2F;mapper&#x2F;centos-root ro crashkernel&#x3D;auto cgroup.memory&#x3D;nokmem rd.lvm.lv&#x3D;centos&#x2F;root rhgb quiet","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"重启服务器导致docker中redis无法启动的问题解决","slug":"重启服务器导致docker中redis无法启动的问题解决","date":"2020-04-21T23:11:33.000Z","updated":"2021-01-27T02:25:26.055Z","comments":true,"path":"2020/04/22/重启服务器导致docker中redis无法启动的问题解决/","link":"","permalink":"http://example.com/2020/04/22/%E9%87%8D%E5%90%AF%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%BC%E8%87%B4docker%E4%B8%ADredis%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","excerpt":"","text":"故障现像 harbor服务里的redis容器启动失败 1234567891011121314151617[root@acp2-master-1 ~]# kubectl get po -n defaultNAME READY STATUS RESTARTS AGEdocker-registry-fb854474f-jmwq5 1&#x2F;1 Running 16 212dgitlab-ce-gitlab-ce-5c7b984fc-85clk 1&#x2F;1 Running 8 9dgitlab-ce-gitlab-ce-database-8f7d789ff-hm2rf 1&#x2F;1 Running 8 9dgitlab-ce-gitlab-ce-redis-c6b479b95-t5rjr 1&#x2F;1 Running 8 9dharbor-harbor-chartmuseum-7bfd86c887-7dvnt 1&#x2F;1 Running 0 33mharbor-harbor-clair-5d6bd4fdf-nxcw8 1&#x2F;1 Running 3 33mharbor-harbor-core-d95c5f884-5x2cm 0&#x2F;1 CrashLoopBackOff 5 14mharbor-harbor-database-0 1&#x2F;1 Running 0 32mharbor-harbor-jobservice-f5d9c4995-nh8qk 1&#x2F;1 Running 6 14mharbor-harbor-nginx-774f9569cb-njxtn 1&#x2F;1 Running 0 33mharbor-harbor-notary-server-867d58d99f-hdq8v 1&#x2F;1 Running 0 33mharbor-harbor-notary-signer-6f6955b4fc-z99sh 1&#x2F;1 Running 0 33mharbor-harbor-portal-65fd74dcbb-8pctx 1&#x2F;1 Running 0 33mharbor-harbor-redis-bd8dbdf49-kttnr 0&#x2F;1 CrashLoopBackOff 6 7m36sharbor-harbor-registry-7bbf6cb89f-ncxz9 2&#x2F;2 Running 0 33m 日志报错 123456789101112131415161718192021[root@acp2-master-1 ~]# kubectl logs --tail 20 -f -n default harbor-harbor-redis-bd8dbdf49-kttnr( &#39; , .-&#96; | &#96;, ) Running in standalone mode|&#96;-._&#96;-...-&#96; __...-.&#96;&#96;-._|&#39;&#96; _.-&#39;| Port: 6379| &#96;-._ &#96;._ &#x2F; _.-&#39; | PID: 1 &#96;-._ &#96;-._ &#96;-.&#x2F; _.-&#39; _.-&#39;|&#96;-._&#96;-._ &#96;-.__.-&#39; _.-&#39;_.-&#39;|| &#96;-._&#96;-._ _.-&#39;_.-&#39; | http:&#x2F;&#x2F;redis.io &#96;-._ &#96;-._&#96;-.__.-&#39;_.-&#39; _.-&#39;|&#96;-._&#96;-._ &#96;-.__.-&#39; _.-&#39;_.-&#39;|| &#96;-._&#96;-._ _.-&#39;_.-&#39; | &#96;-._ &#96;-._&#96;-.__.-&#39;_.-&#39; _.-&#39; &#96;-._ &#96;-.__.-&#39; _.-&#39; &#96;-._ _.-&#39; &#96;-.__.-&#39;1:M 22 Apr 2020 06:14:17.117 # WARNING: The TCP backlog setting of 511 cannot be enforced because &#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn is set to the lower value of 128.1:M 22 Apr 2020 06:14:17.117 # Server initialized1:M 22 Apr 2020 06:14:17.117 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &#39;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&#39; as root, and add it to your &#x2F;etc&#x2F;rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.1:M 22 Apr 2020 06:14:17.120 * Reading RDB preamble from AOF file...1:M 22 Apr 2020 06:14:17.730 * Reading the remaining AOF tail...1:M 22 Apr 2020 06:14:18.095 # Bad file format reading the append only file: make a backup of your AOF file, then use .&#x2F;redis-check-aof --fix &lt;filename&gt; 主要是4个问题，导致redis启动不了的主要是最后一行。“Bad file format reading the append only file: make a backup of your AOF file, then use ./redis-check-aof –fix ” ，翻译一下读取仅追加文件的错误文件格式：备份AOF文件，然后使用./redis-check-a of–fix,有一个AOF的备份文件，通过这个./redis-check-a of–fix还原，查了一下redis的备份文件为“appendonly.aof”. 解决，其中有WARNING的告警可以不需要处理 第一个警告：WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 12345方法1: 临时设置生效: sysctl -w net.core.somaxconn &#x3D; 511方法2: 永久生效: 修改&#x2F;etc&#x2F;sysctl.conf文件，增加一行net.core.somaxconn&#x3D; 511然后执行命令sysctl -p 第二个警告：WARNING overcommitmemory is set to 0! Background save may fail under low memory condition. To fix this issue add ‘vm.overcommitmemory = 1’ to /etc/sysctl.conf and then reboot or run the command ‘sysctl vm.overcommit_memory=1’ for this to take effect. 12345678910111213141516解决方案方法1: 临时设置生效: sysctl -w vm.overcommit_memory &#x3D; 1方法2: 永久生效: 修改&#x2F;etc&#x2F;sysctl.conf文件，增加一行vm.overcommit_memory &#x3D; 1然后执行命令sysctl -p补充: overcommit_memory参数说明：设置内存分配策略（可选，根据服务器的实际情况进行设置）&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;overcommit_memory可选值：0、1、2。0， 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。2， 表示内核允许分配超过所有物理内存和交换空间总和的内存注意：redis在dump数据的时候，会fork出一个子进程，理论上child进程所占用的内存和parent是一样的，比如parent占用的内存为8G，这个时候也要同样分配8G的内存给child,如果内存无法负担，往往会造成redis服务器的down机或者IO负载过高，效率下降。所以这里比较优化的内存分配策略应该设置为 1（表示内核允许分配所有的物理内存，而不管当前的内存状态如何）。 第三个警告：WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command ‘echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled’ as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 12解决方案：上面也提供了解决方案，&#39;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&#39; 这一行，直接执行就好了，但是这样的话，只是当前生效而已，如果电脑重启之后，又是需要重新设置的，所以把这个命令加入到启动过程中。编辑&#x2F;etc&#x2F;rc.local，加入echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled。 三个警告都已经解决，可以不解决，不影响服务，接下为就处理最后导致起不来的问题 通过inspect redis容器找到数据在本地的目录 1234567891011121314151617181920212223242526272829303132333435&#125;,&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f&#x2F;volumes&#x2F;kubernetes.io~cephfs&#x2F;pvc-37d4ca75-5316-11ea-b6ed-525400d63265&quot;, &quot;Destination&quot;: &quot;&#x2F;bitnami&#x2F;redis&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f&#x2F;volumes&#x2F;kubernetes.io~secret&#x2F;default-token-tv2rp&quot;, &quot;Destination&quot;: &quot;&#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&quot;, &quot;Mode&quot;: &quot;ro&quot;, &quot;RW&quot;: false, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f&#x2F;etc-hosts&quot;, &quot;Destination&quot;: &quot;&#x2F;etc&#x2F;hosts&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f&#x2F;containers&#x2F;harbor-harbor&#x2F;fdc4d83f&quot;, &quot;Destination&quot;: &quot;&#x2F;dev&#x2F;termination-log&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 进入宿主机的目录找到 /var/lib/kubelet/pods/965118f1-a115-43f1-b517-870028afb64f/ 可以使用find 找到备份文件appendonly.aof 123[root@acp2-node-1 ~]# cd &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f[root@acp2-node-1 volumes]# find .&#x2F;* -name appendonly.aof.&#x2F;kubernetes.io~cephfs&#x2F;pvc-37d4ca75-5316-11ea-b6ed-525400d63265&#x2F;data&#x2F;appendonly.aof 然后再查找恢复工具redis-check-aof，也可使用find命令 1234567891011[root@acp2-node-1 pods]# find &#x2F;* -name redis-check-aof&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;b7d53e5601b2f83193463f47c11ac363cb15d4e7152935484944ff94d8ff0e49&#x2F;diff&#x2F;opt&#x2F;bitnami&#x2F;redis&#x2F;bin&#x2F;redis-check-aof[root@acp2-node-1 pods]# cd &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;b7d53e5601b2f83193463f47c11ac363cb15d4e7152935484944ff94d8ff0e49&#x2F;diff&#x2F;opt&#x2F;bitnami&#x2F;redis&#x2F;bin&#x2F;[root@acp2-node-1 bin]# lltotal 6732-rwxrwxr-x 1 root root 679512 Sep 21 2019 redis-benchmark-rwxrwxr-x 1 root root 1786296 Sep 21 2019 redis-check-aof-rwxrwxr-x 1 root root 1786296 Sep 21 2019 redis-check-rdb-rwxrwxr-x 1 root root 841624 Sep 21 2019 redis-cli-rwxrwxr-x 1 root root 1786296 Sep 21 2019 redis-server[root@acp2-node-1 bin]# 然后就是使用以下命令开始恢复 123456789101112131415161718192021222324[root@acp2-node-1 bin]# .&#x2F;redis-check-aof --fix &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;965118f1-a115-43f1-b517-870028afb64f&#x2F;volumes&#x2F;kubernetes.io~cephfs&#x2F;pvc-37d4ca75-5316-11ea-b6ed-525400d63265&#x2F;data&#x2F;appendonly.aofThe AOF appears to start with an RDB preamble.Checking the RDB preamble to start:[offset 0] Checking RDB file --fix[offset 26] AUX FIELD redis-ver &#x3D; &#39;5.0.5&#39;[offset 40] AUX FIELD redis-bits &#x3D; &#39;64&#39;[offset 52] AUX FIELD ctime &#x3D; &#39;1587523879&#39;[offset 67] AUX FIELD used-mem &#x3D; &#39;79175024&#39;[offset 83] AUX FIELD aof-preamble &#x3D; &#39;1&#39;[offset 85] Selecting DB ID 0[offset 30115710] Selecting DB ID 1[offset 30117290] Selecting DB ID 2[offset 30596730] Checksum OK[offset 30596730] \\o&#x2F; RDB looks OK! \\o&#x2F;[info] 472735 keys read[info] 470549 expires[info] 470549 already expiredRDB preamble is OK, proceeding with AOF tail...0x 2b98f71: Expected prefix &#39;*&#39;, got: &#39;AOF analyzed: size&#x3D;45715470, ok_up_to&#x3D;45715313, diff&#x3D;157This will shrink the AOF from 45715470 bytes, with 157 bytes, to 45715313 bytesContinue? [y&#x2F;N]: ySuccessfully truncated AOF[root@acp2-node-1 bin]# 启动容器 123456789[root@acp2-node-1 bin]# docker ps -a | grep harbor-harbor-redis9c917c4164c8 180c2ecb6e22 &quot;&#x2F;entrypoint.sh &#x2F;run…&quot; About a minute ago Exited (1) About a minute ago k8s_harbor-harbor_harbor-harbor-redis-bd8dbdf49-kttnr_default_965118f1-a115-43f1-b517-870028afb64f_6cb1ce5f0547c 10.0.129.100:60080&#x2F;claas&#x2F;pause:3.1 &quot;&#x2F;pause&quot; About an hour ago Up About an hour k8s_POD_harbor-harbor-redis-bd8dbdf49-kttnr_default_965118f1-a115-43f1-b517-870028afb64f_0[root@acp2-node-1 bin]# docker restart 9c917c4164c89c917c4164c8[root@acp2-node-1 bin]# docker ps -a | grep harbor-harbor-redis08ecd6aa40b1 180c2ecb6e22 &quot;&#x2F;entrypoint.sh &#x2F;run…&quot; About a minute ago Up About a minute k8s_harbor-harbor_harbor-harbor-redis-bd8dbdf49-kttnr_default_965118f1-a115-43f1-b517-870028afb64f_7cb1ce5f0547c 10.0.129.100:60080&#x2F;claas&#x2F;pause:3.1 &quot;&#x2F;pause&quot; About an hour ago Up About an hour k8s_POD_harbor-harbor-redis-bd8dbdf49-kttnr_default_965118f1-a115-43f1-b517-870028afb64f_0[root@acp2-node-1 bin]# 测试通过测试发现已经没有问题，pod也都正常 12345678910111213[root@acp2-master-1 ~]# kubectl get po -n default | grep harborharbor-harbor-chartmuseum-7bfd86c887-7dvnt 1&#x2F;1 Running 0 101mharbor-harbor-clair-5d6bd4fdf-nxcw8 1&#x2F;1 Running 3 101mharbor-harbor-core-d95c5f884-b9cp6 1&#x2F;1 Running 0 28mharbor-harbor-database-0 1&#x2F;1 Running 0 100mharbor-harbor-jobservice-f5d9c4995-ljz8p 1&#x2F;1 Running 0 28mharbor-harbor-nginx-774f9569cb-njxtn 1&#x2F;1 Running 0 101mharbor-harbor-notary-server-867d58d99f-hdq8v 1&#x2F;1 Running 0 101mharbor-harbor-notary-signer-6f6955b4fc-z99sh 1&#x2F;1 Running 0 101mharbor-harbor-portal-65fd74dcbb-8pctx 1&#x2F;1 Running 0 101mharbor-harbor-redis-bd8dbdf49-kttnr 1&#x2F;1 Running 7 75mharbor-harbor-registry-7bbf6cb89f-ncxz9 2&#x2F;2 Running 0 101m[root@acp2-master-1 ~]#","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"使用etcd快照恢复集群数据","slug":"使用etcd快照恢复集群数据","date":"2020-04-16T22:34:44.000Z","updated":"2021-01-27T02:25:26.054Z","comments":true,"path":"2020/04/17/使用etcd快照恢复集群数据/","link":"","permalink":"http://example.com/2020/04/17/%E4%BD%BF%E7%94%A8etcd%E5%BF%AB%E7%85%A7%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE/","excerpt":"","text":"备份etcd及相关证书123456789101112131415161718192021222324252627282930313233#!&#x2F;bin&#x2F;bashset -euxmkdir -p &#x2F;cpaas&#x2F;&#123;etcd_bak,pki_bak&#125;BACKUP_ETC_DIR&#x3D;&#x2F;cpaas&#x2F;etcd_bakBACKUP_PKI_DIR&#x3D;&#x2F;cpaas&#x2F;pki_bak&#x2F;IP&#x3D;&#96;&#x2F;usr&#x2F;sbin&#x2F;ifconfig eth0 | grep -w &#39;inet&#39; | awk &#39;&#123;print $2&#125;&#39;&#96;ETCDCTL&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;etcdctlTAR&#x3D;&#x2F;usr&#x2F;bin&#x2F;tarbackup_etcd()&#123; ETCDCTL_API&#x3D;3 $&#123;ETCDCTL&#125; --endpoints $&#123;IP&#125;:2379 \\ --cert&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt&quot; \\ --key&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key&quot; \\ --cacert&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt&quot; \\ snapshot save $&#123;BACKUP_ETC_DIR&#125;&#x2F;snap-$(date +%Y%m%d%H%M).db&#125;backup_pki()&#123; $&#123;TAR&#125; -cvf $&#123;BACKUP_PKI_DIR&#125;pki-$(date +%Y%m%d%H%M).tar &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;&#125;del_backup()&#123; find $&#123;BACKUP_ETC_DIR&#125; -mtime +5 -a -name &#39;*.db&#39; | xargs rm -rf find $&#123;BACKUP_PKI_DIR&#125; -mtime +5 -a -name &#39;*.tar&#39; | xargs rm -rf&#125;backup_etcd ## 备份etcdbackup_pki ## 备份证书del_backup ## 删除7天前日志 恢复注意：恢复的顺序是 global 集群 、 业务集群。如果只有业务集群升级失败，不支持只回滚业务集群，必须都要回滚，因为 global 集群上，存有业务集群的 hr 资源。 获取 etcd 地址，并停掉所有(业务集群和 global 集群） master 节点上的 kubelet 服务。 执行命令的环境： 业务集群和 global 集群的所有 master 节点上 执行的命令： 1234567ETCD_SERVER&#x3D;($(kubectl get pod -n kube-system $(kubectl get pod -n kube-system | \\grep etcd | awk &#39;NR&#x3D;&#x3D;1 &#123;print $1&#125;&#39;) -o yaml | \\awk &#39;&#x2F;--initial-cluster&#x3D;&#x2F;&#123;print&#125;&#39; | \\sed -e &#39;s&#x2F;,&#x2F; &#x2F;g&#39; -e&#39;s&#x2F;^.*cluster&#x3D;&#x2F;&#x2F;&#39; | \\sed -e &#39;s#[0-9\\.]*&#x3D;https:&#x2F;&#x2F;##g&#39; -e &#39;s&#x2F;:2380&#x2F;&#x2F;g&#39;))systemctl stop kubelet 删掉 kube-apiserver 容器，目的是恢复过程和恢复之后，global 不要通过调用 kubeapi 写数据到业务集群的 etcd 内。 执行命令的环境： 要恢复 Kubernetes 集群的所有 master 节点上 执行的命令： docker rm -f $(docker ps -a | awk &#39;/_kube-api/&#123;print $NF&#125;&#39;) 命令的结果： 所有 kube-api 的容器都被删除。 判断 etcdctl 命令是否存在第一台 master 节点上，一般执行 backup_recovery.sh 这个备份 etcd 的脚本，会自动将 etcdctl 拷贝到 /usr/bin/etcdctl，如果不存在，需要自行手动拷贝出来。 执行命令的环境： 要恢复 Kubernetes 集群的第一台 master 节点上 执行的命令： whereis etcdctl 命令的结果： 应该打印处 etcdctl 的路径，如果没有就表明是错的。 通过备份的快照恢复 etcd。 执行命令的环境： 要恢复 Kubernetes 集群的第一台 master 节点上 执行的命令： 12345678910111213141516mkdir &#x2F;tmp&#x2F;ddETCD_SERVER&#x3D;($(kubectl get pod -n kube-system $(kubectl get pod -n kube-system | grep etcd | awk &#39;NR&#x3D;&#x3D;1 &#123;print $1&#125;&#39;) -o yaml | awk &#39;&#x2F;--initial-cluster&#x3D;&#x2F;&#123;print&#125;&#39; | sed -e &#39;s&#x2F;,&#x2F; &#x2F;g&#39; -e&#39;s&#x2F;^.*cluster&#x3D;&#x2F;&#x2F;&#39; | sed -e &#39;s#[0-9\\.]*&#x3D;https:&#x2F;&#x2F;##g&#39; -e &#39;s&#x2F;:2380&#x2F;&#x2F;g&#39;)) ## 这个地址在回滚的第一步已经获取了，执行回滚之前，echo 这个变量检查地址是否获取成功snapshot_db&#x3D;&lt;备份时，导出的 etcd 快照文件名，必须是绝对路径&gt;for i in $&#123;ETCD_SERVER[@]&#125;do export ETCDCTL_API&#x3D;3 etcdctl snapshot restore $&#123;snapshot_db&#125; \\ --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt \\ --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key \\ --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt \\ --data-dir&#x3D;&#x2F;tmp&#x2F;dd&#x2F;etcd \\ --name $&#123;i&#125; \\ --initial-cluster $&#123;ETCD_SERVER[0]&#125;&#x3D;https:&#x2F;&#x2F;$&#123;ETCD_SERVER[0]&#125;:2380,$&#123;ETCD_SERVER[1]&#125;&#x3D;https:&#x2F;&#x2F;$&#123;ETCD_SERVER[1]&#125;:2380,$&#123;ETCD_SERVER[2]&#125;&#x3D;https:&#x2F;&#x2F;$&#123;ETCD_SERVER[2]&#125;:2380 \\ --initial-advertise-peer-urls https:&#x2F;&#x2F;$i:2380 &amp;&amp; \\mv &#x2F;tmp&#x2F;dd&#x2F;etcd etcd_$idone 命令的结果： 会生成 etcd_&lt;ip 地址&gt; 这样的三个目录，将这三个目录拷贝到对应ip的服务器的 /root 内。 删掉 etcd 容器。 执行命令的环境： 要恢复 Kubernetes 集群的所有 master 节点上 执行的命令： docker rm -f $(docker ps -a | awk &#39;/_etcd/&#123;print $NF&#125;&#39;) 命令的结果： 所有 etcd 的容器都被删除。 迁移恢复的数据。 执行命令的环境： 要恢复 Kubernetes 集群的所有 master 节点上 执行的命令： 123docker ps -a | awk &#39;&#x2F;_etcd&#x2F;&#123;print $NF&#125;&#39; ##确保没有 etcd 容器mv &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member &#x2F;cpaas&#x2F;backupmv &#x2F;root&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member &#x2F;var&#x2F;lib&#x2F;etcd 命令的结果： 会把 etcd 的数据挪到备份目录下，然后将上一步生成的目录拷贝到 /var/lib/etcd 里。 启动 etcd 和 kube-api。 执行命令的环境： 要恢复 Kubernetes 集群的所有 master 节点上 执行的命令： systemctl start kubelet 命令的结果： kubelet 服务启动后，会自动创建 etcd 的 pod，这个时候执行 docker ps -a | grep -E &#39;etcd|kube-api&#39; 会找到 etcd 和 kube-api 容器。 问题解决 回滚之后，如果出现在kubelet和页面查看所有资源都存在，但是在业务节点没有资源的情况，重启k8s 集群内所有节点的 kubelet 和 docker 服务，也可采用重启集群内所有服务器的方式来解决","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"理解和配置Out of Memory: Kill Process","slug":"理解和配置Out of memory: Kill process","date":"2020-04-16T22:24:15.000Z","updated":"2021-01-27T02:25:26.055Z","comments":true,"path":"2020/04/17/理解和配置Out of memory: Kill process/","link":"","permalink":"http://example.com/2020/04/17/%E7%90%86%E8%A7%A3%E5%92%8C%E9%85%8D%E7%BD%AEOut%20of%20memory:%20Kill%20process/","excerpt":"","text":"理解 OOM killer最近有位 VPS 客户抱怨 MySQL 无缘无故挂掉，还有位客户抱怨 VPS 经常死机，登陆到终端看了一下，都是常见的 Out of memory 问题。这通常是因为某时刻应用程序大量请求内存导致系统内存不足造成的，这通常会触发 Linux 内核里的 Out of Memory (OOM) killer，OOM killer 会杀掉某个进程以腾出内存留给系统用，不致于让系统立刻崩溃。如果检查相关的日志文件（/var/log/messages）就会看到下面类似的 Out of memory: Kill process 信息： 1234567891011121314151617...Out of memory: Kill process 9682 (mysqld) score 9 or sacrifice childKilled process 9682, UID 27, (mysqld) total-vm:47388kB, anon-rss:3744kB, file-rss:80kBhttpd invoked oom-killer: gfp_mask&#x3D;0x201da, order&#x3D;0, oom_adj&#x3D;0, oom_score_adj&#x3D;0httpd cpuset&#x3D;&#x2F; mems_allowed&#x3D;0Pid: 8911, comm: httpd Not tainted 2.6.32-279.1.1.el6.i686 #1...21556 total pagecache pages21049 pages in swap cacheSwap cache stats: add 12819103, delete 12798054, find 3188096&#x2F;4634617Free swap &#x3D; 0kBTotal swap &#x3D; 524280kB131071 pages RAM0 pages HighMem3673 pages reserved67960 pages shared124940 pages non-shared Linux 内核根据应用程序的要求分配内存，通常来说应用程序分配了内存但是并没有实际全部使用，为了提高性能，这部分没用的内存可以留作它用，这部分内存是属于每个进程的，内核直接回收利用的话比较麻烦，所以内核采用一种过度分配内存（over-commit memory）的办法来间接利用这部分 “空闲” 的内存，提高整体内存的使用效率。一般来说这样做没有问题，但当大多数应用程序都消耗完自己的内存的时候麻烦就来了，因为这些应用程序的内存需求加起来超出了物理内存（包括 swap）的容量，内核（OOM killer）必须杀掉一些进程才能腾出空间保障系统正常运行。用银行的例子来讲可能更容易懂一些，部分人取钱的时候银行不怕，银行有足够的存款应付，当全国人民（或者绝大多数）都取钱而且每个人都想把自己钱取完的时候银行的麻烦就来了，银行实际上是没有这么多钱给大家取的。 内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码 linux/mm/oom_kill.c，当系统内存不足的时候，out_of_memory() 被触发，然后调用 select_bad_process() 选择一个 “bad” 进程杀掉，如何判断和选择一个 “bad” 进程呢，总不能随机选吧？挑选的过程由 oom_badness() 决定，挑选的算法和想法都很简单很朴实：最 bad 的那个进程就是那个最占用内存的进程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#x2F;** * oom_badness - heuristic function to determine which candidate task to kill * @p: task struct of which task we should calculate * @totalpages: total present RAM allowed for page allocation * * The heuristic for determining which task to kill is made to be as simple and * predictable as possible. The goal is to return the highest value for the * task consuming the most memory to avoid subsequent oom failures. *&#x2F;unsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg, const nodemask_t *nodemask, unsigned long totalpages)&#123; long points; long adj; if (oom_unkillable_task(p, memcg, nodemask)) return 0; p &#x3D; find_lock_task_mm(p); if (!p) return 0; adj &#x3D; (long)p-&gt;signal-&gt;oom_score_adj; if (adj &#x3D;&#x3D; OOM_SCORE_ADJ_MIN) &#123; task_unlock(p); return 0; &#125; &#x2F;* * The baseline for the badness score is the proportion of RAM that each * task&#39;s rss, pagetable and swap space use. *&#x2F; points &#x3D; get_mm_rss(p-&gt;mm) + p-&gt;mm-&gt;nr_ptes + get_mm_counter(p-&gt;mm, MM_SWAPENTS); task_unlock(p); &#x2F;* * Root processes get 3% bonus, just like the __vm_enough_memory() * implementation used by LSMs. *&#x2F; if (has_capability_noaudit(p, CAP_SYS_ADMIN)) adj -&#x3D; 30; &#x2F;* Normalize to oom_score_adj units *&#x2F; adj *&#x3D; totalpages &#x2F; 1000; points +&#x3D; adj; &#x2F;* * Never return 0 for an eligible task regardless of the root bonus and * oom_score_adj (oom_score_adj can&#39;t be OOM_SCORE_ADJ_MIN here). *&#x2F; return points &gt; 0 ? points : 1;&#125; 理解了这个算法我们就理解了为啥 MySQL 躺着也能中枪了，因为它的体积总是最大（一般来说它在系统上占用内存最多），所以如果 Out of Memeory (OOM) 的话总是不幸第一个被 kill 掉。解决这个问题最简单的办法就是增加内存，或者想办法优化 MySQL 使其占用更少的内存，除了优化 MySQL 外还可以优化系统（优化 Debian 5，优化 CentOS 5.x），让系统尽可能使用少的内存以便应用程序（如 MySQL) 能使用更多的内存，还有一个临时的办法就是调整内核参数，让 MySQL 进程不容易被 OOM killer 发现。 配置解决 OOM killer我们可以通过一些内核参数来调整 OOM killer 的行为，避免系统在那里不停的杀进程。比如我们可以在触发 OOM 后立刻触发 kernel panic，kernel panic 10秒后自动重启系统。 12345678# sysctl -w vm.panic_on_oom&#x3D;1vm.panic_on_oom &#x3D; 1# sysctl -w kernel.panic&#x3D;10kernel.panic &#x3D; 10# echo &quot;vm.panic_on_oom&#x3D;1&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf# echo &quot;kernel.panic&#x3D;10&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf 从上面的 oom_kill.c 代码里可以看到 oom_badness() 给每个进程打分，根据 points 的高低来决定杀哪个进程，这个 points 可以根据 adj 调节，root 权限的进程通常被认为很重要，不应该被轻易杀掉，所以打分的时候可以得到 3% 的优惠（adj -= 30; 分数越低越不容易被杀掉）。我们可以在用户空间通过操作每个进程的 oom_adj 内核参数来决定哪些进程不这么容易被 OOM killer 选中杀掉。比如，如果不想 MySQL 进程被轻易杀掉的话可以找到 MySQL 运行的进程号后，调整 oom_score_adj 为 -15（注意 points 越小越不容易被杀）： 123456# ps aux | grep mysqldmysql 2196 1.6 2.1 623800 44876 ? Ssl 09:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;mysqld# cat &#x2F;proc&#x2F;2196&#x2F;oom_score_adj0# echo -15 &gt; &#x2F;proc&#x2F;2196&#x2F;oom_score_adj 当然，如果需要的话可以完全关闭 OOM killer（不推荐用在生产环境）： 123# sysctl -w vm.overcommit_memory&#x3D;2# echo &quot;vm.overcommit_memory&#x3D;2&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Captain 运维手册","slug":"captain 运维手册","date":"2020-04-03T22:48:54.000Z","updated":"2021-01-27T02:25:26.043Z","comments":true,"path":"2020/04/04/captain 运维手册/","link":"","permalink":"http://example.com/2020/04/04/captain%20%E8%BF%90%E7%BB%B4%E6%89%8B%E5%86%8C/","excerpt":"","text":"captain 安装 安装包 1helm install --version &lt;captain chart version&gt; --debug --namespace&#x3D;&lt;ns&gt; --set global.registry.address&#x3D;&lt;init registry&gt; --set alaudaChartRepoURL&#x3D;&lt;init 节点上的 chart repo&gt; --set namespace&#x3D;&lt;ns&gt; --name&#x3D;captain stable&#x2F;captain --wait --timeout 3000 kubectl-captain 在安装目录下的 other 目录里 安装要求 软件依赖 captain 依赖 cert-manager ，必须在 cert-manager 部署成功后，安装 captain 硬件依赖 均可 用 captain 替换 helm 已经通过 helm 部署的 chart 怎样迁移到 helm 上 详见升级说明文档，大致流程如下： 通过 kubectl captain 命令或者直接创建 helmrequest 资源，hr 资源存在 global 的集群内captain 侦测到新的 helmrequest 资源，会自动部署 chart ，成功后，会生成 release 资源。chart 安装到那个集群，release 资源就存在那个集群内 2.2. 迁移到 captain 的 chart 如何在 helm 里删掉 详见升级说明文档，大致流程如下： helm 每成功安装一次 chart ，就会在 kube-system 的 ns 下创建一个 cm，cm 的名字是 chart 的 release 名字+ v[0-9]，比如 cert-manager.v1，是 cert-manager 这个 chart 第一次安装成功的版本，以后每更新一次这个 chart ，后面的数字就加1找到这个 cm，删掉之后执行 helm list -a 命令就看不到 chart 了，但是 chart 创建的资源还存在 captain 运维 captain 安装 chart 1kubectl captain create ，比如kubectl captain create --version 2.1 --namespace&#x3D;cpaas-system --chart&#x3D;stable&#x2F;amp-minio --set global.registry.address&#x3D;10.0.128.172:60080 amp-minio 即创建了一个名为 amp-minio 的 HelmRequest 资源写 yaml ，然后 kubectl create -f captain 更新、升级 chart 1kubectl edit &lt;hr name&gt; -n &lt;ns&gt; 修改 chart 的变量和版本 captain 删除 chart 1kubectl delete &lt;hr name&gt; -n &lt;ns&gt; 修改 chart 的变量和版本 captain 查看 chart 状态 12345kubectl get &lt;hr name&gt; -n &lt;ns&gt; ## 查看 hr 的状态kubectl describe &lt;hr name&gt; -n &lt;ns&gt; ## 查看 hr 详细情况kubectl get release -n &lt;ns&gt; ## 查看通过 captain 部署的 chart captain 列出 chart 资源 1kubectl get &lt;hr name&gt; -n &lt;ns&gt; 和 helm 命令对应功能的 captain 命令 12345678910111213helm list kubectl get &lt;hr name&gt; -n &lt;ns&gt; ； kubectl get release -n &lt;ns&gt; | grep &lt;hr name&gt; # 这个命令在 chart 安装的目的集群的 master 上执行helm repo list kubectl get chartrepo -n &lt;ns&gt; help repo uphelm install kubectl captain createhelm upgrade kubectl edit &lt;hr name&gt; -n &lt;ns&gt; helm get values kubectl get &lt;hr name&gt; -n &lt;ns&gt; -o yamlhelm status captain 错误排障","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"captain","slug":"captain","permalink":"http://example.com/tags/captain/"}]},{"title":"Helm常用命令手册","slug":"helm常用手册","date":"2020-04-03T22:35:29.000Z","updated":"2021-01-27T02:25:26.044Z","comments":true,"path":"2020/04/04/helm常用手册/","link":"","permalink":"http://example.com/2020/04/04/helm%E5%B8%B8%E7%94%A8%E6%89%8B%E5%86%8C/","excerpt":"","text":"Helm 常用命令 查看版本 1helm version 查看当前安装的charts1helm list 查询 charts1helm search nginx 下载远程安装包到本地1helm fetch rancher-stable&#x2F;rancher 查看package详细信息1helm inspect chart 安装charts1#helm install --name nginx --namespaces prod bitnami&#x2F;nginx 查看charts状态1#helm status nginx 删除charts1#helm delete --purge nginx 增加repo12#helm repo add stable https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts#helm repo add --username admin --password password myps https:&#x2F;&#x2F;harbor.pt1.cn&#x2F;chartrepo&#x2F;charts 更新repo仓库资源1#helm repo update 创建charts1#helm create helm_charts 测试charts语法1#helm lint 打包charts1#cd helm_charts &amp;&amp; helm package .&#x2F; 查看生成的yaml文件1#helm template helm_charts-0.1.1.tgz 更新image1#helm upgrade --set image.tag&#x3D;‘v201908‘ test update myharbor&#x2F;study-api-en-oral 回滚relase1#helm rollback 2 自定义 package 的选项： 查询支持的选项1#helm inspect values stable&#x2F;mysql 自定义 password 持久化存储1#helm install --name db-mysql --set mysqlRootPassword&#x3D;anoyi stable&#x2F;mysql Helm 使用 查询 charts1#helm search mysql 查询 package 详细信息1#helm inspect stable&#x2F;mysql 部署 package1#helm install stable&#x2F;mysql 查看服务状态1#helm status existing-serval 删除服务1#helm delete --purge existing-serval 再次查看显示状态为删除123456helm status existing-serval -LAST DEPLOYED: Mon Aug 12 19:09:51 2019NAMESPACE: defaultSTATUS: DELETED- 部署之前可以自定义 package 的选项： 查询支持的选项1#helm inspect values stable&#x2F;mysql 自定义 password 持久化存储1#helm install --name pttestdb --set mysqlRootPassword&#x3D;test stable&#x2F;mysql 查看密码变量1234#helm get values pttestdb-mysqlRootPassword: test- Chart.yaml 文件1234567891011├── charts #该目录中放置当前Chart依赖的其它Chart├── Chart.yaml #用于描述Chart的相关信息，包括名字、描述信息以及等。├── templates #部署文件模版目录，模版使用的值来自values.yaml和由Tiller提供的值│ ├── deployment.yaml #kubernetes Deployment object│ ├── _helpers.tpl #用于修改kubernetes objcet配置的模板│ ├── ingress.yaml│ ├── NOTES.txt #用于介绍 Chart 部署后的一些信息，例如：如何使用这个 Chart、列出缺省的设置等。│ ├── service.yaml #kubernetes Serivce│ └── tests│ └── test-connection.yaml└── values.yaml #用于存储 templates 目录中模板文件中用到变量的值。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"helm","slug":"helm","permalink":"http://example.com/tags/helm/"}]},{"title":"升级docker1.12.6到18.09,并切换存储direct-lvm到overlay2","slug":"升级docker1.12.6到18.09.5，并切换存储direct-lvm到overlay2","date":"2020-04-03T08:46:41.000Z","updated":"2021-01-27T02:25:26.053Z","comments":true,"path":"2020/04/03/升级docker1.12.6到18.09.5，并切换存储direct-lvm到overlay2/","link":"","permalink":"http://example.com/2020/04/03/%E5%8D%87%E7%BA%A7docker1.12.6%E5%88%B018.09.5%EF%BC%8C%E5%B9%B6%E5%88%87%E6%8D%A2%E5%AD%98%E5%82%A8direct-lvm%E5%88%B0overlay2/","excerpt":"","text":"先确保操作系统内核为：3.10.0.862及其以上版本，可通过 uname -r 查看 停止并卸载docker1systemctl stop docker &amp;&amp; yum remove -y docker* 删除docker存储，并删除docker目录123vgremove docker #若这一步报devicebusy，则reboot节点之后重新vgremove。pvremove &#x2F;dev&#x2F;***rm -rf &#x2F;var&#x2F;lib&#x2F;docker&#x2F;* 格式化overlay2存储12mkfs.xfs -n ftype&#x3D;1 &#x2F;dev&#x2F;***mount &#x2F;dev&#x2F;*** &#x2F;var&#x2F;lib&#x2F;docker 加入开机启动/etc/fstab1&#x2F;dev&#x2F;*** &#x2F;var&#x2F;lib&#x2F;docker xfs defaults 0 0 安装18.09.2的docker，并修改daemon.json配置文件12345yum install yum-utils -yyum-config-manager --add-repo https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;centos&#x2F;docker-ce.repoyum clean all yum install docker-ce-18.09.5* -ysystemctl enable docker 修改daemon.json配置文件，默认lvm的配置如下格式：12345678910111213141516171819202122232425[root@cloud-cn-master-1 ~]# cat &#x2F;etc&#x2F;docker&#x2F;daemon.json&#123; &quot;insecure-registries&quot;: [ &quot;192.168.0.4:60080&quot;, &quot;&quot;, &quot;&quot; ], &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;storage-opts&quot;: [ &quot;dm.thinpooldev&#x3D;&#x2F;dev&#x2F;mapper&#x2F;docker-thinpool&quot;, &quot;dm.min_free_space&#x3D;0%&quot;, &quot;dm.use_deferred_deletion&#x3D;true&quot;, &quot;dm.use_deferred_removal&#x3D;true&quot;, &quot;dm.fs&#x3D;ext4&quot; ]&#125;修改后如下所示：&#123; &quot;insecure-registries&quot;: [ &quot;192.168.0.4:60080&quot;, &quot;10.0.0.1:5000&quot; ], &quot;storage-driver&quot;: &quot;overlay2&quot;&#125; 启动docker12systemctl start dockersystemctl enable docker","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"direct-lvm","slug":"direct-lvm","permalink":"http://example.com/tags/direct-lvm/"},{"name":"overlay2","slug":"overlay2","permalink":"http://example.com/tags/overlay2/"},{"name":"18.09.5","slug":"18-09-5","permalink":"http://example.com/tags/18-09-5/"},{"name":"docker升级","slug":"docker升级","permalink":"http://example.com/tags/docker%E5%8D%87%E7%BA%A7/"}]},{"title":"overlay2 use xfs filesystem cause system hang","slug":"overlay2 use xfs filesystem cause system hang","date":"2020-04-03T08:38:33.000Z","updated":"2021-01-27T02:25:26.050Z","comments":true,"path":"2020/04/03/overlay2 use xfs filesystem cause system hang/","link":"","permalink":"http://example.com/2020/04/03/overlay2%20use%20xfs%20filesystem%20cause%20system%20hang/","excerpt":"","text":"日志报错 报错信息 1234567[166973.065674] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166974.848634] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166976.857584] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166978.697604] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166980.524526] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166982.529419] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250)[166984.534372] XFS: runc:[1:CHILD](13230) possible memory allocation deadlock in kmem_zone_alloc (mode:0x8250) 排查问题 排查 123456789101112131415161718192021https:&#x2F;&#x2F;access.redhat.com&#x2F;solutions&#x2F;532663决议This is a long standing issue with xfs and highly fragmented files.Our engineering team is working on a long term resolution for this issue.WorkaroundsThere are several solutions that can be used to avoid high file fragmentation:Preallocate the space to be used by the file with unwritten extents. This gives the allocator the opportunity to allocate the whole file in one go and use the least amount of extents. As the blocks are written they will break up the unwritten extents into written&#x2F;unwritten space and when all of the unwritten space has been converted the extent map will match the original optimal preallocated state.Use the extent size hint feature of XFS. This feature tells the allocator to allocate more space than may be needed by the current write request so that a minimum extent size is used. The extent will initially be allocated as an unwritten extent and will be converted as the individual blocks within the extent are written. As with preallocated files, when the entire extent has been written the extent size will match the original unwritten extent. The extent size hint feature can be set on a file or directory with this command:Raw$ xfs_io -c &quot;extsize &lt;extent size&gt;&quot; &lt;dir or file&gt;If set on a directory then all files created within that directory after the hint is set will inherit the feature. You cannot set the hint on files that already have extents allocated. If it is not possible to modify the application then this is the suggested option to use.Use asynchronous buffered I&#x2F;O. This will offer the chance to have many logically consecutive pages build up in the cache before being written out. Extents can then be allocated for the entire range of outstanding pages instead of each page individually. This will not only reduce fragmentation but means less I&#x2F;Os need to be issued to the storage device.Avoid writing the file in a random order. If blocks can be coalesced within the application before being written out using direct I&#x2F;O then there&#39;s a chance the file can be written sequentially which the allocator can use to allocate extents contiguously.Use xfs_fsr to defragment individual large files. Note xfs_fsr is unable to defragment files that are currently in use, using the -v option is recommended to report on any issues that prevent defragmentation.Rawxfs_fsr -v &#x2F;path&#x2F;to&#x2F;large&#x2F;file 临时解决办法 清空cache 123echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_cachesecho 2 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_cachesecho 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches 碎片整理 12xfs_db -c frag -r &lt;dev&gt;xfs_fsr -v &lt;dev&gt; 调整系统参数 参考：http://www.cnblogs.com/itfriend/archive/2011/12/14/2287160.html Linux 提供了这样一个参数min_free_kbytes，用来确定系统开始回收内存的阀值，控制系统的空闲内存。值越高，内核越早开始回收内存，空闲内存越高。 设置/proc/sys/vm/min_free_kbytes的值为4G bytes 1echo 4194304 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes 升级内核yum provides kernel yum install -y kernel-3.10.0-1062.9.1.el7.x86_64〔拼音〕","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"xfs","slug":"xfs","permalink":"http://example.com/tags/xfs/"}]},{"title":"K8s环境使用老IP添加一个新的master节点","slug":"k8s环境使用老IP添加一个新的master节点","date":"2020-04-03T03:58:53.000Z","updated":"2021-01-27T02:25:26.047Z","comments":true,"path":"2020/04/03/k8s环境使用老IP添加一个新的master节点/","link":"","permalink":"http://example.com/2020/04/03/k8s%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8%E8%80%81IP%E6%B7%BB%E5%8A%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84master%E8%8A%82%E7%82%B9/","excerpt":"","text":"注：以下为k8s 1.16版本，并且是新加节点，无备份的操作 备份 配置etcd 312docker cp &#96;docker ps |grep etcd |grep -v pause |awk &#39;&#123;print $1&#125;&#39;&#96;:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;etcdctl &#x2F;tmp&#x2F;export &#96;cat &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;etcd.yaml |grep ETCDCTL_API -A1 |xargs |sed &#39;s&#x2F;^.&#x2F;&#x2F;g&#39; |awk &#39;&#123;print $1 &#125;&#39;&#96; ;echo $ETCDCTL_API 获取etcd指令:1export etcdctl&#x3D;&#96;cat &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;etcd.yaml |grep ETCDCTL_API -A1 |xargs |sed &#39;s&#x2F;^.&#x2F;&#x2F;g&#39; |sed &#39;s&#x2F;ETCDCTL_API&#x3D;3 &#x2F;\\&#x2F;tmp\\&#x2F;&#x2F;g&#39;&#96; 备份etcd：(最好/etc/kubernetes目录)1mkdir &#x2F;back ; cd &#x2F;back ; $etcdctl snapshot save snapshot.db 根据ID删除坏的etcd节点： 12$etcdctl member list$etcdctl member remove &lt;ID&gt; 删除坏掉的node节点： 1kubectl delete nodes &lt;node&gt; 使用ake将这个节点重新加回来 12ake addnodesake join --apiserver 192.168.16.40:6443 --pkg-repo http:&#x2F;&#x2F;192.168.16.40:7000 --registry 192.168.16.40:60080 --debug 给加入进来的node打上master的标签 1kubectl label no &lt;node&gt; node-role.kubernetes.io&#x2F;master&#x3D; 如果/etc/kubernetes/有备份，以下步骤不需要 将其他节点的/etc/kubernetes/整个目录拷贝过来,并修改/etc/kubernetes/manifests/下yaml文件，将其中的相关的ip换成本机的ip 重新添加etcd节点 1$etcdctl member add 10.0.129.124 --peer-urls&#x3D;https:&#x2F;&#x2F;10.0.129.124:2380 重启kubelet 重新签发etcd证书看文档，在执行脚本前添加当前机器的IP ，请看文档k8s 1.13证书升级(包含etcd证书) 旧版本k8s可用： 把其他 master 节点的 /etc/kubeadm/etcdcfg.yaml 拷贝过来，然后把其中的 ip 信息改成本机的 ip 把其他 master 节点的 /etc/kubernetes/pki/etcd/ca.crt 和 /etc/kubernetes/pki/etcd/ca.key 拷贝过来 然后使用 kubeadm init phase certs etcd-server etcd-peer apiserver-etcd-client –config /etc/kubeadm/etcdcfg.yaml 生成新的证书 最后用 kubeadm init phase etcd local –config /etc/kubeadm/etcdcfg.yaml 生成新的 etcd manifest 就可以了","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"k8s1.13证书升级(包含etcd证书)","slug":"k8s1.13证书升级(包含etcd证书)","date":"2020-04-03T03:53:32.000Z","updated":"2021-01-27T02:25:26.045Z","comments":true,"path":"2020/04/03/k8s1.13证书升级(包含etcd证书)/","link":"","permalink":"http://example.com/2020/04/03/k8s1.13%E8%AF%81%E4%B9%A6%E5%8D%87%E7%BA%A7(%E5%8C%85%E5%90%ABetcd%E8%AF%81%E4%B9%A6)/","excerpt":"","text":"一、etcd备份： 脚本中对k8s做了备份，但是没有对etcd数据做备份，需要对etcd数据做备份。 参考etcd(V3版api)备份和恢复二 、master节点k8s证书更换（分别在每个master上执行即可） 脚本最好放在一个空目录下执行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#!&#x2F;bin&#x2F;bashif [ ! -d &quot;&#x2F;root&#x2F;tmp&#x2F;&quot; ]; then mkdir &#x2F;root&#x2F;tmp&#x2F;ficp -rf &#x2F;etc&#x2F;kubernetes &#x2F;root&#x2F;tmp&#x2F;kubernetes_&#96;date &#39;+%Y%m%d_%H.%M.%S&#39;&#96;#更换apiserver证书cat &lt;&lt;EOF&gt;ssl.conf[req]req_extensions &#x3D; v3_reqdistinguished_name &#x3D; req_distinguished_name[req_distinguished_name][v3_req]keyUsage &#x3D;critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; TLS Web Server Authentication, TLS Web Client AuthenticationsubjectAltName &#x3D; @alt_names[alt_names]EOFip&#x3D;1dns&#x3D;1for aa in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.crt -noout -text |grep &quot;DNS&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;DNS&quot;|tr -d &quot; &quot;&#96;; do echo $aa| sed &quot;s&#x2F;DNS:&#x2F;DNS.$dns &#x3D; &#x2F;&quot; &gt;&gt; ssl.conf;let dns+&#x3D;1; donefor aa in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.crt -noout -text |grep &quot;IP Address&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;IP Address&quot;|tr -d &quot; &quot; &#96;;do echo $aa| sed &quot;s&#x2F;IPAddress:&#x2F;IP.$ip &#x3D; &#x2F;&quot; &gt;&gt; ssl.conf;let ip+&#x3D;1; doneopenssl req -new -key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.key -out apiserver.csr -subj &quot;&#x2F;CN&#x3D;kube-apiserver&quot; -config ssl.confopenssl x509 -req -in apiserver.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -out apiserver.crt -days 10950 -extensions v3_req -extfile ssl.confmv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.crt &#x2F;tmpcp apiserver.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;#更换apiserver-kubelet-client证书cat &lt;&lt;EOF&gt;client.conf [ v3_ca ]keyUsage &#x3D; critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; clientAuthEOFopenssl genrsa -out admin.key 2048openssl req -new -key admin.key -out admin.csr -subj &quot;&#x2F;O&#x3D;system:masters&#x2F;CN&#x3D;kube-apiserver-kubelet-client&quot;openssl x509 -req -in admin.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -sha256 -out admin.crt -extensions v3_ca -extfile client.conf -days 3650mv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-kubelet-client.key &#x2F;tmpmv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-kubelet-client.crt &#x2F;tmpmv admin.key apiserver-kubelet-client.keymv admin.crt apiserver-kubelet-client.crtcp apiserver-kubelet-client.key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;cp apiserver-kubelet-client.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;kubectl delete pod -n kube-system kube-apiserver-$HOSTNAMEdocker ps |grep &quot;kube-apiserver&quot; | awk &#39;&#123;print $1&#125;&#39; | xargs docker rm -f#更换front-proxy-client证书openssl genrsa -out client.key 2048openssl req -new -key client.key -out client.csr -subj &quot;&#x2F;O&#x3D;system:masters&#x2F;CN&#x3D;front-proxy-client&quot;openssl x509 -req -in client.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-ca.key -CAcreateserial -sha256 -out client.crt -extensions v3_ca -extfile client.conf -days 3650mv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-client.key &#x2F;tmpmv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-client.crt &#x2F;tmpmv client.key front-proxy-client.keymv client.crt front-proxy-client.crtcp front-proxy-client.key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;cp front-proxy-client.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;sleep 8#更换Kubelet证书#Master节点直接执行此脚本cat &lt;&lt;EOF&gt;kube.conf [ v3_ca ]keyUsage &#x3D; critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; clientAuthEOFopenssl genrsa -out kubelet.key 2048openssl req -new -key kubelet.key -out kubelet.csr -subj &quot;&#x2F;O&#x3D;system:nodes&#x2F;CN&#x3D;system:node:$HOSTNAME&quot;openssl x509 -req -in kubelet.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -sha256 -out kubelet.crt -extensions v3_ca -extfile kube.conf -days 3650base64 kubelet.crt | tr -d &#39;\\n&#39; &gt; kubelet-crt-database64 kubelet.key | tr -d &#39;\\n&#39; &gt; kubelet-key-dataKUBELET_CRT_DATA&#x3D;$(cat kubelet-crt-data)KUBELET_KEY_DATA&#x3D;$(cat kubelet-key-data)sed -i \\-e &#39;s&#x2F;client-certifica..*:.*&#x2F;&#39;&quot;client-certificate-data: $KUBELET_CRT_DATA&quot;&#39;&#x2F;&#39; \\-e &#39;s&#x2F;client-k..*:.*&#x2F;&#39;&quot;client-key-data: $KUBELET_KEY_DATA&quot;&#39;&#x2F;&#39; \\ &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf#controller证书更换openssl genrsa -out controller.key 2048openssl req -new -key controller.key -out controller.csr -subj &quot;&#x2F;CN&#x3D;system:kube-controller-manager&quot;openssl x509 -req -in controller.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -sha256 -out controller.crt -extensions v3_ca -extfile client.conf -days 3650base64 controller.crt | tr -d &#39;\\n&#39; &gt; controller-crt-database64 controller.key | tr -d &#39;\\n&#39; &gt; controller-key-dataCONTROLLER_CRT_DATA&#x3D;$(cat controller-crt-data)CONTROLLER_KEY_DATA&#x3D;$(cat controller-key-data)sed -i \\-e &#39;s&#x2F;client-certifica..*:.*&#x2F;&#39;&quot;client-certificate-data: $CONTROLLER_CRT_DATA&quot;&#39;&#x2F;&#39; \\-e &#39;s&#x2F;client-k..*:.*&#x2F;&#39;&quot;client-key-data: $CONTROLLER_KEY_DATA&quot;&#39;&#x2F;&#39; \\ &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.confkubectl delete pod -n kube-system kube-controller-manager-$HOSTNAMEdocker ps |grep &quot;k8s_kube-controller-manager&quot; |awk &#39;&#123;print $1&#125;&#39; | xargs docker rm -f#scheduler证书更换openssl genrsa -out scheduler.key 2048openssl req -new -key scheduler.key -out scheduler.csr -subj &quot;&#x2F;CN&#x3D;system:kube-scheduler&quot;openssl x509 -req -in scheduler.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -sha256 -out scheduler.crt -extensions v3_ca -extfile client.conf -days 3650base64 scheduler.crt | tr -d &#39;\\n&#39; &gt; scheduler-crt-database64 scheduler.key | tr -d &#39;\\n&#39; &gt; scheduler-key-dataSCHEDULER_CRT_DATA&#x3D;$(cat scheduler-crt-data)SCHEDULER_KEY_DATA&#x3D;$(cat scheduler-key-data)sed -i \\-e &#39;s&#x2F;client-certifica..*:.*&#x2F;&#39;&quot;client-certificate-data: $SCHEDULER_CRT_DATA&quot;&#39;&#x2F;&#39; \\-e &#39;s&#x2F;client-k..*:.*&#x2F;&#39;&quot;client-key-data: $SCHEDULER_KEY_DATA&quot;&#39;&#x2F;&#39; \\ &#x2F;etc&#x2F;kubernetes&#x2F;scheduler.confkubectl delete pod -n kube-system kube-scheduler-$HOSTNAMEdocker ps |grep &quot;k8s_kube-scheduler&quot; |awk &#39;&#123;print $1&#125;&#39; | xargs docker rm -fsystemctl restart kubeletecho -e &quot;\\033[5;33;40m master单节点证书签发成功\\033[0m \\n&quot;echo -e &quot;\\033[5;33;40m 验证证书，请仔细查看证书签发时间\\033[0m \\n&quot;echo &quot;--------------------------------&quot;echo &quot;apiserver证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;kubelet证书&quot;cat &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf |grep client-certificate-data |awk -F: &#39;&#123;print $2&#125;&#39; | awk &#39;&#123;print $1&#125;&#39; | base64 -d &gt; a.crtecho &#96;openssl x509 -in a.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;controller-manager证书&quot;cat &#x2F;etc&#x2F;kubernetes&#x2F;controller-manager.conf |grep client-certificate-data |awk -F: &#39;&#123;print $2&#125;&#39; | awk &#39;&#123;print $1&#125;&#39; | base64 -d &gt; a.crtecho &#96;openssl x509 -in a.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;scheduler证书&quot;cat &#x2F;etc&#x2F;kubernetes&#x2F;scheduler.conf |grep client-certificate-data |awk -F: &#39;&#123;print $2&#125;&#39; | awk &#39;&#123;print $1&#125;&#39; | base64 -d &gt; a.crtecho &#96;openssl x509 -in a.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;apiserver-kubelet-client证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-kubelet-client.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;front-proxy-client证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-client.crt -noout -dates&#96;rm -rf a.crt 三、master节点etcd证书更换（分别在每个master上执行即可） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283if [ ! -d &quot;&#x2F;root&#x2F;tmp&#x2F;&quot; ]; then mkdir &#x2F;root&#x2F;tmp&#x2F;ficp -rf &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd &#x2F;root&#x2F;tmp&#x2F;etcd_&#96;date &#39;+%Y%m%d_%H.%M.%S&#39;&#96;#etcd的healthcheck-client-client证书更换cat &lt;&lt;EOF&gt;healthcheck.conf [ v3_ca ]keyUsage &#x3D; critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; clientAuth, serverAuth,TLS Web Server Authentication, TLS Web Client AuthenticationEOF#openssl genrsa -out healthcheck-client.key 2048openssl req -new -key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;healthcheck-client.key -out healthcheck-client.csr -subj &quot;&#x2F;O&#x3D;system:masters&#x2F;CN&#x3D;kube-etcd-healthcheck-client&quot;openssl x509 -req -in healthcheck-client.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.key -CAcreateserial -sha256 -out healthcheck-client.crt -extensions v3_ca -extfile healthcheck.conf -days 3650mv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;healthcheck-client.crt &#x2F;tmpcp healthcheck-client.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;#etcd的peer证书更换cat &lt;&lt;EOF&gt;peer-ssl.conf[req]req_extensions &#x3D; v3_reqdistinguished_name &#x3D; req_distinguished_name[req_distinguished_name][v3_req]keyUsage &#x3D;critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; TLS Web Server Authentication, TLS Web Client AuthenticationsubjectAltName &#x3D; @alt_names[alt_names]EOFippeer&#x3D;1dnspeer&#x3D;1for bb in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt -noout -text |grep &quot;DNS&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;DNS&quot;|tr -d &quot; &quot;&#96;; do echo $bb| sed &quot;s&#x2F;DNS:&#x2F;DNS.$dnspeer &#x3D; &#x2F;&quot; &gt;&gt; peer-ssl.conf;let dnspeer+&#x3D;1; donefor bb in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt -noout -text |grep &quot;IP Address&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;IP Address&quot;|tr -d &quot; &quot; &#96;;do echo $bb| sed &quot;s&#x2F;IPAddress:&#x2F;IP.$ippeer &#x3D; &#x2F;&quot; &gt;&gt; peer-ssl.conf;let ippeer+&#x3D;1; doneopenssl req -new -key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key -out peer.csr -subj &quot;&#x2F;CN&#x3D;$HOSTNAME&quot; -config peer-ssl.confopenssl x509 -req -in peer.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.key -CAcreateserial -out peer.crt -days 3650 -extensions v3_req -extfile peer-ssl.confmv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt &#x2F;tmpcp peer.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd#etcd的server证书更换cat &lt;&lt;EOF&gt;server-ssl.conf[req]req_extensions &#x3D; v3_reqdistinguished_name &#x3D; req_distinguished_name[req_distinguished_name][v3_req]keyUsage &#x3D;critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; TLS Web Server Authentication, TLS Web Client AuthenticationsubjectAltName &#x3D; @alt_names[alt_names]EOFipserver&#x3D;1dnsserver&#x3D;1for cc in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt -noout -text |grep &quot;DNS&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;DNS&quot;|tr -d &quot; &quot;&#96;; do echo $cc| sed &quot;s&#x2F;DNS:&#x2F;DNS.$dnsserver &#x3D; &#x2F;&quot; &gt;&gt; server-ssl.conf;let dnsserver+&#x3D;1; donefor cc in &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt -noout -text |grep &quot;IP Address&quot;|sed &quot;s&#x2F;,&#x2F;\\n&#x2F;g&quot;|grep &quot;IP Address&quot;|tr -d &quot; &quot; &#96;;do echo $cc| sed &quot;s&#x2F;IPAddress:&#x2F;IP.$ipserver &#x3D; &#x2F;&quot; &gt;&gt; server-ssl.conf;let ipserver+&#x3D;1; doneopenssl req -new -key &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key -out server.csr -subj &quot;&#x2F;CN&#x3D;$HOSTNAME&quot; -config server-ssl.confopenssl x509 -req -in server.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.key -CAcreateserial -out server.crt -days 3650 -extensions v3_req -extfile server-ssl.confmv &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt &#x2F;tmpcp server.crt &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcdkubectl delete pod -n kube-system etcd-$HOSTNAMEdocker ps |grep etcd | awk &#39;&#123;print $1&#125;&#39; | xargs docker rm -fsleep 2echo -e &quot;\\033[5;33;40m master单节点etcd证书签发成功\\033[0m \\n&quot;echo -e &quot;\\033[5;33;40m 验证证书，请仔细查看证书签发时间\\033[0m \\n&quot;echo &quot;--------------------------------&quot;echo &quot;etcd-healthcheck-client证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;healthcheck-client.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;etcd-peer证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt -noout -dates&#96;echo &quot;--------------------------------&quot;echo &quot;etcd-server证书&quot;echo &#96;openssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt -noout -dates&#96; 四、slave的kubelet证书升级(分别在每个slave上执行即可) 把master节点的/etc/kubernetes/pki目录下的ca.key拷贝到slave的/etc/kubernetes/pki目录下 12345678910111213141516171819202122232425262728293031323334353637&#x2F;bin&#x2F;bashif [ ! -d &quot;&#x2F;root&#x2F;tmp&#x2F;&quot; ]; then mkdir &#x2F;root&#x2F;tmp&#x2F;ficp -rf &#x2F;etc&#x2F;kubernetes &#x2F;root&#x2F;tmp&#x2F;kubernetes_&#96;date &#39;+%Y%m%d_%H.%M.%S&#39;&#96; cat &lt;&lt;EOF&gt;kube.conf [ v3_ca ]keyUsage &#x3D; critical, digitalSignature, keyEnciphermentextendedKeyUsage &#x3D; clientAuthEOF openssl genrsa -out kubelet.key 2048openssl req -new -key kubelet.key -out kubelet.csr -subj &quot;&#x2F;O&#x3D;system:nodes&#x2F;CN&#x3D;system:node:$HOSTNAME&quot;openssl x509 -req -in kubelet.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -sha256 -out kubelet.crt -extensions v3_ca -extfile kube.conf -days 3650base64 kubelet.crt | tr -d &#39;\\n&#39; &gt; kubelet-crt-database64 kubelet.key | tr -d &#39;\\n&#39; &gt; kubelet-key-dataKUBELET_CRT_DATA&#x3D;$(cat kubelet-crt-data)KUBELET_KEY_DATA&#x3D;$(cat kubelet-key-data)sed -i \\-e &#39;s&#x2F;client-certifica..*:.*&#x2F;&#39;&quot;client-certificate-data: $KUBELET_CRT_DATA&quot;&#39;&#x2F;&#39; \\-e &#39;s&#x2F;client-k..*:.*&#x2F;&#39;&quot;client-key-data: $KUBELET_KEY_DATA&quot;&#39;&#x2F;&#39; \\ &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.confsystemctl restart kubelet echo -e &quot;\\033[5;33;40m slave单节点kubelet证书签发成功\\033[0m \\n&quot; echo -e &quot;\\033[5;33;40m 验证证书，请仔细查看证书签发时间\\033[0m \\n&quot; cat &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf |grep client-certificate-data |awk -F: &#39;&#123;print $2&#125;&#39; | awk &#39;&#123;print $1&#125;&#39; | base64 -d &gt; a.crtecho &#96;openssl x509 -in a.crt -noout -dates&#96;rm -rf a.crt 五、删除dns、proxy、flannel的pod 123kubectl get pod -n kube-system |grep proxy |awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete pod -n kube-systemkubectl get pod -n kube-system |grep dns |awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete pod -n kube-systemkubectl get pod -n kube-system |grep flannel |awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete pod -n kube-system 六、功能验证","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"}]},{"title":"Etcd集群运维","slug":"etcd集群运维","date":"2019-08-29T00:24:19.000Z","updated":"2021-01-27T02:25:26.043Z","comments":true,"path":"2019/08/29/etcd集群运维/","link":"","permalink":"http://example.com/2019/08/29/etcd%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/","excerpt":"","text":"集群的备份和恢复添加备份12345678#!&#x2F;bin&#x2F;bashIP&#x3D;123.123.123.123BACKUP_DIR&#x3D;&#x2F;alauda&#x2F;etcd_bak&#x2F;mkdir -p $BACKUP_DIRexport ETCDCTL_API&#x3D;3etcdctl --endpoints&#x3D;http:&#x2F;&#x2F;$IP:2379 snapshot save $BACKUP&#x2F;snap-$(date +%Y%m%d%H%M).db# 备份一个节点的数据就可以恢复，实践中，为了防止定时任务配置的节点异常没有生成备份，建议多加几个 恢复集群1234567891011121314151617181920212223242526#!&#x2F;bin&#x2F;bash# 使用 etcdctl snapshot restore 生成各个节点的数据# 比较关键的变量是# --data-dir 需要是实际 etcd 运行时的数据目录# --name --initial-advertise-peer-urls 需要用各个节点的配置# --initial-cluster initial-cluster-token 需要和原集群一致ETCD_1&#x3D;10.1.0.5ETCD_2&#x3D;10.1.0.6ETCD_3&#x3D;10.1.0.7for i in ETCD_1 ETCD_2 ETCD_3doexport ETCDCTL_API&#x3D;3etcdctl snapshot restore snapshot.db \\--data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd \\--name $i \\--initial-cluster $&#123;ETCD_1&#125;&#x3D;http:&#x2F;&#x2F;$&#123;ETCD_1&#125;:2380,$&#123;ETCD_2&#125;&#x3D;http:&#x2F;&#x2F;$&#123;ETCD_2&#125;:2380,$&#123;ETCD_3&#125;&#x3D;http:&#x2F;&#x2F;$&#123;ETCD_3&#125;:2380 \\--initial-cluster-token k8s_etcd_token \\--initial-advertise-peer-urls http:&#x2F;&#x2F;$i:2380 &amp;&amp; \\mv &#x2F;var&#x2F;lib&#x2F;etcd&#x2F; etcd_$idone# 把 etcd_10.1.0.5 复制到 10.1.0.5节点，覆盖&#x2F;var&#x2F;lib&#x2F;etcd(同--data-dir路径)# 其他节点依次类推 用etcd自动创建的snapdb恢复1234567891011121314#!&#x2F;bin&#x2F;bash export ETCDCTL_API&#x3D;3etcdctl snapshot restore snapshot.db \\--skip-hash-check \\--data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd \\--name 10.1.0.5 \\--initial-cluster 10.1.0.5&#x3D;http:&#x2F;&#x2F;10.1.0.5:2380,10.1.0.6&#x3D;http:&#x2F;&#x2F;10.1.0.6:2380,10.1.0.7&#x3D;http:&#x2F;&#x2F;10.1.0.7:2380 \\--initial-cluster-token k8s_etcd_token \\--initial-advertise-peer-urls http:&#x2F;&#x2F;10.1.0.5:2380# 也是所有节点都需要生成自己的数据目录，参考上一条# 和上一条命令唯一的差别是多了 --skip-hash-check （跳过完整性校验）# 这种方式不能确保 100% 可恢复，建议还是自己加备份# 通常恢复后需要做一下数据压缩和碎片整理，可参考相应章节 踩过的坑[ 3.0.14版 etcd restore 功能不可用 ] https://github.com/etcd-io/etcd/issues/7533 使用更新的 etcd 即可。 集群的扩容从1到3 执行添加 12345678#!&#x2F;bin&#x2F;bashexport ETCDCTL_API&#x3D;2etcdctl --endpoints&#x3D;http:&#x2F;&#x2F;10.1.0.6:2379 member add 10.1.0.6 http:&#x2F;&#x2F;10.1.0.6:2380etcdctl --endpoints&#x3D;http:&#x2F;&#x2F;10.1.0.7:2379 member add 10.1.0.7 http:&#x2F;&#x2F;10.1.0.7:2380# ETCD_NAME&#x3D;&quot;etcd_10.1.0.6&quot; # ETCD_INITIAL_CLUSTER&#x3D;&quot;10.1.0.6&#x3D;http:&#x2F;&#x2F;10.1.0.6:2380,10.1.0.5&#x3D;http:&#x2F;&#x2F;10.1.0.5:2380&quot;# ETCD_INITIAL_CLUSTER_STATE&#x3D;&quot;existing&quot; 准备添加的节点 etcd 参数配置 1234567891011121314#!&#x2F;bin&#x2F;bash&#x2F;usr&#x2F;local&#x2F;bin&#x2F;etcd --data-dir&#x3D;&#x2F;data.etcd --name 10.1.0.6--initial-advertise-peer-urls http:&#x2F;&#x2F;10.1.0.6:2380--listen-peer-urls http:&#x2F;&#x2F;10.1.0.6:2380--advertise-client-urls http:&#x2F;&#x2F;10.1.0.6:2379--listen-client-urls http:&#x2F;&#x2F;10.1.0.6:2379--initial-cluster 10.1.0.6&#x3D;http:&#x2F;&#x2F;10.1.0.6:2380,10.1.0.5&#x3D;http:&#x2F;&#x2F;10.1.0.5:2380--initial-cluster-state exsiting--initial-cluster-token k8s_etcd_token# --initial-cluster 集群所有节点的 name&#x3D;ip:peer_url# --initial-cluster-state exsiting 告诉 etcd 自己归属一个已存在的集群，不要自立门户 踩过的坑 从 1 到 3 期间，会经过集群是两节点的状态，这时候可能集群的表现就像挂了，endpoint status 这些命令都不能用，所以我们需要用member add先把集群扩到三节点，然后再依次启动etcd实例，这样做就能确保 etcd 就是健康的。 从3到更多，其实还是member add啦，就放心搞吧。 集群加证书生成证书1234curl -s -L -o &#x2F;usr&#x2F;bin&#x2F;cfssl https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64curl -s -L -o &#x2F;usr&#x2F;bin&#x2F;cfssljson https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64chmod +x &#x2F;usr&#x2F;bin&#x2F;&#123;cfssl,cfssljson&#125;cd &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd 123456789101112131415161718# cat ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;100000h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;server&quot;: &#123; &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;], &quot;expiry&quot;: &quot;100000h&quot; &#125;, &quot;client&quot;: &#123; &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;], &quot;expiry&quot;: &quot;100000h&quot; &#125; &#125; &#125;&#125; 1234567891011121314151617# cat ca-csr.json&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;Alauda&quot;, &quot;OU&quot;: &quot;PaaS&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125; 12345678910111213141516171819202122232425# cat server-csr.json&#123; &quot;CN&quot;: &quot;etcd-server&quot;, &quot;hosts&quot;: [ &quot;localhost&quot;, &quot;0.0.0.0&quot;, &quot;127.0.0.1&quot;, &quot;所有master 节点ip &quot;, &quot;所有master 节点ip &quot;, &quot;所有master 节点ip &quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;Alauda&quot;, &quot;OU&quot;: &quot;PaaS&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125; 1234567891011121314151617181920# cat client-csr.json&#123; &quot;CN&quot;: &quot;etcd-client&quot;, &quot;hosts&quot;: [ &quot;&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;Alauda&quot;, &quot;OU&quot;: &quot;PaaS&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125; 1234cd &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcdcfssl gencert -initca ca-csr.json | cfssljson -bare cacfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;server server-csr.json | cfssljson -bare servercfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;client client-csr.json | cfssljson -bare client 参考链接：https://lihaoquan.me/2017/3/29/etcd-https-setup.html 首先更新节点的peer-urls1234567export ETCDCTL_API=3etcdctl --endpoints=http://x.x.x.x:2379 member list # 1111111111 .......... # 2222222222 .......... # 3333333333 ..........etcdctl --endpoints=http://172.30.0.123:2379 member update 1111111111 --peer-urls=https://x.x.x.x:2380 # 执行三次把三个节点的peer-urls都改成https 修改配置123456789101112131415161718192021222324252627282930# vim &#x2F;etc&#x2F;kubernetes&#x2F;main*&#x2F;etcd.yaml# etcd启动命令部分修改 http 为 https，启动状态改成 existing - --advertise-client-urls&#x3D;https:&#x2F;&#x2F;x.x.x.x:2379 - --initial-advertise-peer-urls&#x3D;https:&#x2F;&#x2F;x.x.x.x:2380 - --initial-cluster&#x3D;xxx&#x3D;https:&#x2F;&#x2F;x.x.x.x:2380,xxx&#x3D;https:&#x2F;&#x2F;x.x.x.x:2380,xxx&#x3D;https:&#x2F;&#x2F;x.x.x.x:2380 - --listen-client-urls&#x3D;https:&#x2F;&#x2F;x.x.x.x:2379 - --listen-peer-urls&#x3D;https:&#x2F;&#x2F;x.x.x.x:2380 - --initial-cluster-state&#x3D;existing# etcd 启动命令部分插入 - --cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.pem - --key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server-key.pem - --peer-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.pem - --peer-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server-key.pem - --trusted-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem - --peer-trusted-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem - --peer-client-cert-auth&#x3D;true - --client-cert-auth&#x3D;true# 检索hostPath在其后插入 - hostPath: path: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd type: DirectoryOrCreate name: etcd-certs# 检索mountPath在其后插入 - mountPath: &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd name: etcd-certs 123456# vim &#x2F;etc&#x2F;kubernetes&#x2F;main*&#x2F;kube-apiserver.yaml# apiserver 启动部分插入，修改 http 为https - --etcd-cafile&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem - --etcd-certfile&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;client.pem - --etcd-keyfile&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;client-key.pem - --etcd-servers&#x3D;https:&#x2F;&#x2F;x.x.x.x:2379,https:&#x2F;&#x2F;x.x.x.x:2379,https:&#x2F;&#x2F;x.x.x.x:2379 遇到的坑[ etcd加证书后，apiserver的健康检查还是http请求，etcd会一直刷日志 ] https://github.com/etcd-io/etcd/issues/9285 12018-02-06 12:41:06.905234 I | embed: rejected connection from &quot;127.0.0.1:35574&quot; (error &quot;EOF&quot;, ServerName &quot;&quot;) 解决办法：直接去掉apiserver的健康检查，或者把默认的检查命令换成curl (apiserver的镜像里应该没有curl，如果是刚需的话自己重新build一下吧 ) 集群升级已经是 v3 的的集群不需要太多的配置，保留数据目录，替换镜像(或者二进制)即可； v2到v3的升级需要一个merge的操作，我并没有实际的实践过，也不太推荐这样做。 集群状态检查其实上述所有步骤都需要这些命令的辅助—— 123456789101112131415161718#!&#x2F;bin&#x2F;bash# 如果证书的话，去掉--cert --key --cacert 即可# --endpoints&#x3D; 需要写了几个节点的url，endpoint status就输出几条信息export ETCDCTL_API&#x3D;3etcdctl \\--endpoints&#x3D;https:&#x2F;&#x2F;x.x.x.x:2379 \\ --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;client.pem \\--key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;client-key.pem \\--cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.pem \\endpoint status -w tableetcdctl --endpoints&#x3D;xxxx endpoint healthetcdctl --endpoints&#x3D;xxxx member listkubectl get cs 数据操作（删除、压缩、碎片整理）删除1234ETCDCTL_API&#x3D;2 etcdctl rm --recursive # v2 的 api 可以这样删除一个“目录”ETCDCTL_API&#x3D;3 etcdctl --endpoints&#x3D;xxx del &#x2F;xxxxx --prefix # v3 的版本# 带证书的话，参考上一条添加 --cert --key --cacert 即可 遇到的坑：在一个客户环境里发现Kubernetes集群里的 “事件” 超级多，就是 kubectl describe xxx 看到的events部分信息，数据太大导致 etcd 跑的很累，我们就用这样的方式删掉没用的这些数据。 碎片整理12ETCDCTL_API&#x3D;3 etcdctl --endpoints&#x3D;xx:xx,xx:xx,xx:xx defragETCDCTL_API&#x3D;3 etcdctl --endpoints&#x3D;xx:xx,xx:xx,xx:xx endpoint status # 看数据量 压缩12345678910ETCDCTL_API&#x3D;3 etcdctl --endpoints&#x3D;xx:xx,xx:xx,xx:xx compact# 这个在只有 K8s 用的 etcd 集群里作用不太大，可能具体场景我没遇到# 可参考这个文档# https:&#x2F;&#x2F;www.cnblogs.com&#x2F;davygeek&#x2F;p&#x2F;8524477.html# 不过跑一下不碍事etcd --auto-compaction-retention&#x3D;1# 添加这个参数让 etcd 运行时自己去做压缩 常见问题 etcd 对时间很依赖，所以集群里的节点时间一定要同步！ 磁盘空间不足，如果磁盘是被 etcd 自己吃完了，就需要考虑压缩和删数据啦 加证书后所有请求就都要带证书了，要不会提示 context deadline exceeded 做各个操作时 etcd 启动参数里标明节点状态的要小心，否则需要重新做一遍前面的步骤很麻烦 日志收集etcd 的日志暂时只支持 syslog 和 stdout 两种—— https://github.com/etcd-io/etcd/issues/7936 etcd 的日志在排查故障时很有用，如果我们用宿主机来部署 etcd，日志可以通过 systemd 检索到，但kubeadm 方式启动的 etcd 在容器重启后就会丢失所有历史。我们可以用以下的方案来做—— shell 的重定向 123etcd --xxxx --xxxx &gt; &#x2F;var&#x2F;log&#x2F;etcd.log # 配合 logratate 来做日志切割# 将日志通过 volume 挂载到宿主机 supervisor supervisor 从容器刚开始流行时，就是保持服务持续运行很有效的工具 sidecar 容器（后续我在github上补充一个例子，github.com/jing2uo） sidecar可以简单理解为一个pod里有多个容器（比如kubedns）他们彼此可以看到对方的进程，因此我们可以用传统的 strace 来捕捉 etcd进程的输出，然后在sidecar这个容器里和shell重定向一样操作。 1strace -e trace&#x3D;write -s 200 -f -p 1 kubeadm 1.13部署的集群 最近我们测试kubernetes 1.13集群时发现了一些有趣的改变，诈一看我们上面的命令就没法用了—— https://kubernetes.io/docs/setup/independent/ha-topology/ 区分了 Stacked etcd topology 和 External etcd topology ，官方的链接了这个图很形象—— 这种模式下的 etcd 集群，最明显的差别是容器内 etcd 的initial-cluster 启动参数只有自己的 ip，会有点懵挂了我这该怎么去恢复。其实基本原理没有变，kubeadm 藏了个 configmap，启动参数被放在了这里 —— 1kubectl get cm etcdcfg -n kube-system -o yaml 1234567891011121314etcd: local: serverCertSANs: - &quot;192.168.8.21&quot; peerCertSANs: - &quot;192.168.8.21&quot; extraArgs: initial-cluster: 192.168.8.21&#x3D;https:&#x2F;&#x2F;192.168.8.21:2380,192.168.8.22&#x3D;https:&#x2F;&#x2F;192.168.8.22:2380,192.168.8.20&#x3D;https:&#x2F;&#x2F;192.168.8.20:2380 initial-cluster-state: new name: 192.168.8.21 listen-peer-urls: https:&#x2F;&#x2F;192.168.8.21:2380 listen-client-urls: https:&#x2F;&#x2F;192.168.8.21:2379 advertise-client-urls: https:&#x2F;&#x2F;192.168.8.21:2379 initial-advertise-peer-urls: https:&#x2F;&#x2F;192.168.8.21:2380 联系我实践过程中 google 了大量文档和教程，整理时我尽量找了印象深刻的文档的历史补充进来，但时间过去了很久不可能搜集完整，如果发现某部分内容侵犯了版权，可以联系我删除内容或者补充参考链接。若文档表述或者知识点有问题，来 https://github.com/jing2uo/Blog 提 issue 。 Q&amp;A Q：请问 etcd 监控和告警如何做的？告警项都有哪些？ A：告警要看用的什么监控吧，和 Kubernetes 配套比较常见的是普罗米修思和 Grafana 了。告警项我没有具体配过，可以关注的点是：endpoint status -w table 里可以看到数据量，endpoints health 看到健康状态，还有内存使用这些，具体可以参考普罗米修思的 exporter 是怎么做的。 Q：使用 Kubeadm 部署高可用集群是不是相当于先部署三个独立的单点 Master，最后靠 etcd 添加节点操作把数据打通？ A：不是，Kubeadm 部署会在最开始就先建一个 etcd 集群，apiserver 启动之前就需要准备好 etcd，否则 apiserver 起不了，集群之间就没法通信。可以尝试手动搭一下集群，不用 Kubeadm，一个个把组件开起来，之后对Kubernetes的组件关系会理解更好的。 Q：etcd 跨机房高可用如何保证呢？管理 etcd 有好的 UI 工具推荐么？ A：etcd 对时间和网络要求很高，所以跨机房的网络不好的话性能很差，光在那边选请输入链接描述举去了。我分享忘了提一个 etcd 的 mirror，可以去参考下做法。跨机房的话，我觉得高速网络是个前提吧，不过还没做过。UI 工具没找过，都是命令行操作来着。 Q：Kubeadm 启动的集群内 etcd节 点，kubectl 操作 etcd 的备份恢复有尝试过吗？ A：没有用 kubectl 去处理过 etcd 的备份恢复。etcd 的恢复依赖用 SnapDb 生成数据目录，把 etcd 进程丢进容器里，类似的操作避免不了，还有启动的状态需要修改。kubeadm 启动的 etcd 可以通过 kubectl 查询和 exec，但是数据操作应该不可以，比如恢复 etcd ing 时，无法连接 etcd，kubectl 还怎么工作？ Q：kubeadm-ha 启动 3 个 Master，有 3 个 etcd 节点，怎么跟集群外的 3 个 etcd 做集群，做成 3 Master 6 etcd？ A：可以参考文档里的扩容部分，只要保证 etcd 的参数正确，即使一个集群一部分容器化，一部分宿主机，都是可以的（当然不建议这么做）。可以先用 kubeadm 搭一个集群，然后用扩容的方式把其他三个节点加进来，或者在 kubeadm 操作之前，先搭一个 etcd 集群。然后 kubeadm 调用它就可以。 Q：有没有试过 Kubeadm 的滚动升级，etcd 版本变更，各 Master 机分别重启，数据同步是否有异常等等？ A：做过。Kubeadm 的滚动升级公司内部有从 1.7 一步步升级到 1.11、1.12 的文档，或多或少有一点小坑，不过今天主题是 etcd 所以没提这部分。各个 Master 分别重启后数据的一致我们测试时没问题，还有比较极端的是直接把三 Master 停机一天，再启动后也能恢复。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"etcd","slug":"etcd","permalink":"http://example.com/tags/etcd/"}]},{"title":"Linux 安装腾讯 COS FTP SERVER","slug":"Linux 安装腾讯 COS FTP SERVER","date":"2019-07-09T23:49:32.000Z","updated":"2021-01-27T02:25:26.040Z","comments":true,"path":"2019/07/10/Linux 安装腾讯 COS FTP SERVER/","link":"","permalink":"http://example.com/2019/07/10/Linux%20%E5%AE%89%E8%A3%85%E8%85%BE%E8%AE%AF%20COS%20FTP%20SERVER/","excerpt":"","text":"官方文档https://cloud.tencent.com/document/product/436/7214 官方github需要先到github 下载ftp服务包 https://github.com/tencentyun/cos-ftp-server-V5 安装部署进入安装目录，运行setup.py 脚本，机器需要连网 1sudo python setup.py install 修改配置文件进入到下载目录下的conf文件下里将vsftpd.conf.example文件重新复制一份命名为vsftpd.conf 以下为配置文件说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[COS_ACCOUNT_0]cos_secretid &#x3D; COS_SECRETID # 替换为您的 SECRETIDcos_secretkey &#x3D; COS_SECRETKEY # 替换为您的 SECRETKEYcos_bucket &#x3D; examplebucket-1250000000cos_region &#x3D; region # 替换为您的存储桶地域cos_protocol &#x3D; https#cos_endpoint &#x3D; region.myqcloud.comhome_dir &#x3D; &#x2F;home&#x2F;user0 # 本机要有此目录ftp_login_user_name&#x3D;user0 # ftp 所用的用户名ftp_login_user_password&#x3D;pass0 #ftp 所用的密码authority&#x3D;RWdelete_enable&#x3D;true # true 为允许该 ftp 用户进行删除操作(默认)，false 为禁止该用户进行删除操作[COS_ACCOUNT_1]cos_secretid &#x3D; COS_SECRETID # 替换为您的 SECRETIDcos_secretkey &#x3D; COS_SECRETKEY # 替换为您的 SECRETKEYcos_bucket &#x3D; examplebucket-1250000000cos_region &#x3D; region # 替换为您的存储桶地域cos_protocol &#x3D; https#cos_endpoint &#x3D; region.myqcloud.comhome_dir &#x3D; &#x2F;home&#x2F;user1 #此目录不能与上个目录一样ftp_login_user_name&#x3D;user1ftp_login_user_password&#x3D;pass1authority&#x3D;RWdelete_enable&#x3D;false[NETWORK]masquerade_address &#x3D; XXX.XXX.XXX.XXX # 如果 FTP SERVER 处于某个网关或NAT后，可以通过该配置项将网关的IP 地址或域名指定给 FTPlisten_port &#x3D; 2121 # Ftp Server的监听端口，默认为2121，注意防火墙需要放行该端口passive_port &#x3D; 60000,65535 # passive_port 可以设置 passive 模式下，端口的选择范围，默认在(60000, 65535)区间上选择[FILE_OPTION]# 默认单文件大小最大支持到200G，不建议设置太大single_file_max_size &#x3D; 21474836480[OPTIONAL]# 以下设置，如无特殊需要，建议保留 default 设置 如需设置，请合理填写一个整数min_part_size &#x3D; defaultupload_thread_num &#x3D; defaultmax_connection_num &#x3D; 512max_list_file &#x3D; 10000 # ls命令最大可列出的文件数目，建议不要设置太大，否则ls命令延时会很高log_level &#x3D; INFO # 设置日志输出的级别log_dir &#x3D; log # 设置日志的存放目录，默认是在ftp server目录下的log目录中 以下为我的配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[COS_ACCOUNT_alaudaace]cos_secretid &#x3D; AKIDj2Axxxxxxxxxxxxxxxxxxxxxxxxxcos_secretkey &#x3D; 4uUpvlxxxxxxxxxxxxxxxxcos_bucket &#x3D; alauda-xxxxxxxxcos_region &#x3D; ap-cxxxgcos_protocol &#x3D; https#cos_endpoint &#x3D; ap-xxx.myqcloud.comhome_dir &#x3D; &#x2F;home&#x2F;alxxftp_login_user_name &#x3D; alaxxeftp_login_user_password &#x3D; cExxxxxxauthority &#x3D; Rdelete_enable &#x3D; false[COS_ACCOUNT_alauda]cos_secretid &#x3D; AKIDj2Axxxxxxxxxxxxxxxxxxxcos_secretkey &#x3D; 4uUpvldxxxxxxxxxxxxcos_bucket &#x3D; alauda-xxxxxcos_region &#x3D; ap-chongqingcos_protocol &#x3D; https#cos_endpoint &#x3D; ap-xxx.myqcloud.comhome_dir &#x3D; &#x2F;home&#x2F;xx ftp_login_user_name &#x3D; xxftp_login_user_password &#x3D; kjsxxxxauthority &#x3D; Rdelete_enable &#x3D; false#[COS_ACCOUNT_3]#cos_secretid &#x3D; XXXX#cos_secretkey &#x3D; XXXXX#cos_bucket &#x3D; &#123;bucket name&#125;-123#cos_region &#x3D; ap-xxx#cos_protocol &#x3D; https##cos_endpoint &#x3D; ap-xxx.myqcloud.com#home_dir &#x3D; &#x2F;home&#x2F;user1#ftp_login_user_name&#x3D;user1#ftp_login_user_password&#x3D;pass1#authority&#x3D;RW#delete_enable&#x3D;true[NETWORK]masquerade_address &#x3D; 11xxx ## 注：此处写本ftp服务器的外网IPlisten_port &#x3D; 2121#passive_port可以设置passive模式下，端口的选择范围，默认在(60000, 65535)区间上选择passive_port &#x3D; 60000,65535[FILE_OPTION]# 默认单文件大小最大支持到200G，不建议设置太大single_file_max_size &#x3D; 214748364800[OPTIONAL]config_check_enable &#x3D; true# 以下设置，如无特殊需要，建议保留default设置 如需设置，请合理填写一个整数min_part_size &#x3D; defaultupload_thread_num &#x3D; defaultmax_connection_num &#x3D; 100max_list_file &#x3D; 10000log_level &#x3D; DEBUGlog_dir &#x3D; log 运行 ftp_server.py 启动 FTP Server： 前台运行 1python ftp_server.py 后台运行 1nohup python ftp_server.py &gt;&gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 &amp; 使用screen运行 12345screen -dmS ftpscreen -r ftppython ftp_server.py#使用快捷键，切回主 screen 即可：Ctrl+A+D 网络及安全设置 机器所在的安全组需要放开2121,60000-65535端口 机器上的iptables需要放行12iptables -A INPUT -p tcp --dport 2121 -j ACCEPTiptables -A OUTPUT -p tcp --dport 2121 -j ACCEPT 常见问題解决 https://cloud.tencent.com/document/product/436/30742 使用客户端下载https://filezilla-project.org/index.php 或其它客户端，请自行下载 使用浏览器访问ftp://118.xxxx xxxx:2121 输入对应的帐号及密码即可 ace下载 服务器地址：11xxxx xxx 服务器端口：2121 用户名：alauxxxx 密码：cxxxx x acp下载 服务器地址：118xxx 服务器端口：2121 用户名：alauda 密码：kjshxxxx 安装包上传到腾讯oss登陆打包机使用以下命令上传 工具安装：https://cloud.tencent.com/document/product/436/10976 1pip install coscmd 上传前请修改配置,上传acp就把acp打开即可 12345678910cat ~&#x2F;.cos.conf [common] secret_id &#x3D; AKIDjxxxxxxxxxxxxxxxxxxxxxx secret_key &#x3D; 4uUpxxxxxxxxxxxxx # bucket &#x3D; alauda-xxxxxxxxxxx bucket &#x3D; alauda-xxxxxxxxxxxxxx4 region &#x3D; ap-chongqing max_thread &#x3D; 5 part_size &#x3D; 1 schema &#x3D; https 命令格式： 1coscmd upload &lt;localpath&gt; &lt;cospath&gt; —skipmd5 (这个参数为跳过md5检查） 示例: 1coscmd upload --skipmd5 acp-2.0.1-190703.tgz &#x2F; 官方命令文档 https://cloud.tencent.com/document/product/436/10976","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"cos","slug":"cos","permalink":"http://example.com/tags/cos/"},{"name":"FTP","slug":"FTP","permalink":"http://example.com/tags/FTP/"}]},{"title":"鼠须管五笔输入法安装配置","slug":"鼠须管五笔输入法安装配置","date":"2019-06-27T17:51:03.000Z","updated":"2021-01-27T02:50:53.383Z","comments":true,"path":"2019/06/28/鼠须管五笔输入法安装配置/","link":"","permalink":"http://example.com/2019/06/28/%E9%BC%A0%E9%A1%BB%E7%AE%A1%E4%BA%94%E7%AC%94%E8%BE%93%E5%85%A5%E6%B3%95%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"写在前边的话由于本人使用五笔已有多年，所以在使用了市面上的大多数五笔后，几乎没有一个能完全符合我心意，前段时间同事推荐才发现这个输入法，接下来就简单记录下安装及配置。 注意本文档适用配置Mac电脑 下载及安装下载及安装非常简单，可以从官网下载页面下载，而且几乎支持全平台，可随意配置你想要的输入法。 使用以下简单一条命令即可安装： 1brew cask install squirrel 安装后会自动添加，如果输入法列表中不存在，需要重启或注销电脑。 配置目录用户目录： 1cd ~&#x2F;Library&#x2F;Rime 如果之前有安装过，建议将之前的目录删除，删除目录后可以再次点击 鼠须管 | 重新部署 即可再次生成用户目录。 词库下载12cd ~&#x2F;Library&#x2F;Rimecurl -fsSL https:&#x2F;&#x2F;git.io&#x2F;rime-install | bash -s -- wubi pinyin-simp rime/rime-wubi: 【五筆字型】官方輸入方案 单字模式词库下载： 链接: https://pan.baidu.com/s/1HdsJvkS6VA5Y7FzX_4cUzg 提取码: pbtk 将下载后的文件放到用户目录下，点击重新部署即可。 平台及皮肤设置以下是我使用的配置，可根据自已喜好设置squirrel.custom.yaml: 12345678910111213141516171819# squirrel.custom.yaml# 自定义样式,配色patch: style: color_scheme: dark_temple # 默认皮肤在squirrel.yaml中查看 horizontal: true # 水平&#x2F;竖直显示 inline_preedit: true font_point: 16 corner_radius: 3 border_height: 0 border_width: 0 line_spacing: 1 spacing: 5 # space between preedit and candidates in non-inline mode #candidate_format: &#39;%c. %@ &#39; font_face: &#39;Lucida Grande&#39; #font_point: 18 #label_font_face: &#39;STHeitiTC-Medium&#39; #label_font_point: 18a 其它配置1234567891011121314151617181920# default.custom.yaml# 设定备选词数量，定义输入方案patch: menu&#x2F;page_size: 5 schema_list: # - schema: luna_pinyin # - schema: cangjie5 # - schema: luna_pinyin_fluency # - schema: wubi86 - schema: wubi_pinyin key_binder&#x2F;bindings: - when: paging accept: bracketleft send: Page_Up - when: has_menu accept: bracketright send: Page_Down 保存应用设置选择”输入法”–”鼠须管”–”重新部署” 使用此输入法学习能力极强，默认词组是不可用的，你要让它学习，即可输入一次即可，以下简单示范： 以“确认”这个词组来示范，第一次输入时它是不知道这个词组的； \b 开始教它记住字根，你要先把第一字能正确打出来，比如打到第三个字母时这个字才出来，那就继续打第二个字的字根，然后开始使用数字键选择你要打的字，然后再选择第二个字，这样就把这个词组打印出来了，意思就是在教它时有可能一次会输入超过4个字母，也有可能超过8个.在我的示例中，我打完 dqeyw 这5个字母时这个词组已经出来了，你不能直接选择第一个词组，你要选择第二个“确”字，然后再云选择“认”这个字，这样它就能记住了。 \b 然后再使用四字母输入时这个词就可以正常输出了 \b 结束在开始这段时间可能会有点慢，但它的可调教性是值得你去做的，相信时间会告诉你，加油吧。","categories":[{"name":"Other","slug":"Other","permalink":"http://example.com/categories/Other/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"五笔","slug":"五笔","permalink":"http://example.com/tags/%E4%BA%94%E7%AC%94/"},{"name":"鼠须管","slug":"鼠须管","permalink":"http://example.com/tags/%E9%BC%A0%E9%A1%BB%E7%AE%A1/"},{"name":"输入法","slug":"输入法","permalink":"http://example.com/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"}]},{"title":"Helm安装使用","slug":"helm安装使用","date":"2019-05-09T00:16:06.000Z","updated":"2021-01-27T02:25:26.044Z","comments":true,"path":"2019/05/09/helm安装使用/","link":"","permalink":"http://example.com/2019/05/09/helm%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/","excerpt":"","text":"简介Helm 是一个类似于 yum/apt/homebrew 的 Kubernetes 应用管理工具。Helm 使用 Chart 来管理 Kubernetes manifest 文件。 安装 Helm 使用官方脚本安装最新版 123curl https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;helm&#x2F;master&#x2F;scripts&#x2F;get &gt; get_helm.shchmod 700 get_helm.sh.&#x2F;get_helm.sh 更新 charts 列表 1helm repo update 为 Tilier 添加权限参考 role-based-access-control 新建 rbac-config.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行以下命令创建 1kubectl create -f rbac-config.yaml 查看 Helm 版本号 123[root@xuejian-1 linux-amd64]# .&#x2F;helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.13.1&quot;, GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.13.1&quot;, GitCommit:&quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4&quot;, GitTreeState:&quot;clean&quot;&#125; 初始化 Helm 并安装 Tiller 服务（需要事先配置好 kubectl） 1helm init --upgrade --service-account tiller --tiller-image registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;tiller:v2.9.1 Helm 工作原理基本概念 Chart：包含了创建一个 Kubernetes 应用的必要信息 Repository：Helm package 存储仓库 Release：是一个 chart 及其配置的一个运行实例 组成结构Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server 是一个部署在 Kubernetes 集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。 Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到 Kubernetes 集群，并跟踪随后的发布 通过与 Kubernetes 交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release helm chartsHelm 使用 Chart 来管理 Kubernetes manifest 文件。每个 chart 都至少包括 应用的基本信息 Chart.yaml 一个或多个 Kubernetes manifest 文件模版（放置于 templates/ 目录中），可以包括 Pod、Deployment、Service 等各种 Kubernetes 资源 模板默认值 values.yaml （可选） 示例 Chart ： Mysql 依赖管理Helm 支持两种方式管理依赖的方式： 直接把依赖的 package 放在 charts/ 目录中 使用 requirements.yaml 并用 helm dep up foochart 来自动下载依赖的 packages dependencies: - name: apache version: 1.2.3 repository: http://example.com/charts - name: mysql version: 3.2.1 repository: http://another.example.com/charts 插件管理插件提供了扩展 Helm 核心功能的方法，它在客户端执行，并放在 $(helm home)/plugins 目录中。一个典型的 helm 插件格式为 $(helm home)/plugins/ |- keybase/ | |- plugin.yaml |- keybase.sh 而 plugin.yaml 格式为 name: &quot;keybase&quot; version: &quot;0.1.0&quot; usage: &quot;Integreate Keybase.io tools with Helm&quot; description: |- This plugin provides Keybase services to Helm. ignoreFlags: false useTunnel: false command: &quot;$HELM_PLUGIN_DIR/keybase.sh&quot; 这样，就可以用 helm keybase 命令来使用这个插件。 Helm 常用命令 查询 charts1helm search mysql 查询 package 详细信息1helm inspect stable&#x2F;mysql 部署 package1helm install stable&#x2F;mysql 部署之前可以自定义 package 的选项：12345# 查询支持的选项helm inspect values stable&#x2F;mysql# 自定义 password 持久化存储helm install --name db-mysql --set mysqlRootPassword&#x3D;anoyi stable&#x2F;mysql 注意： 如果出现无可用 PV， 即 kubectl get pv 提示 No resources found. 需要创建 PV 创建pv 新建文件夹 1mkdir &#x2F;k8s 新建文件 local-pv.yaml，storage 大小依据当前主机的磁盘大小来修改，查看磁盘使用命令 df -lh apiVersion: &quot;v1&quot; kind: &quot;PersistentVolume&quot; metadata: name: &quot;local-pv&quot; spec: capacity: storage: &quot;20Gi&quot; accessModes: - &quot;ReadWriteOnce&quot; persistentVolumeReclaimPolicy: Recycle hostPath: path: /k8s 创建 PV 1kubectl create -f local-pv.yaml 查看 Release 列表 1helm ls 升级 / 回滚 Release 12345# 升级helm upgrade --set mysqlRootPassword&#x3D;passwd db-mysql stable&#x2F;mysql# 回滚helm rollback db-mysql 1 删除 Release 1helm delete --purge db-mysql 管理 repo 12345678# 添加 incubator repohelm repo add my-repo https:&#x2F;&#x2F;kubernetes-charts-incubator.storage.googleapis.com&#x2F;# 查询 repo 列表helm repo list# 生成 repo 索引（用于搭建 helm repository）helm repo index 管理 chart 12345678# 创建一个新的 charthelm create hello-chart# validate charthelm lint# 打包 chart 到 tgzhelm package hello-chart 相关资料 Helm 官方文档 官方 Charts 仓库 在 kubernetes 集群中创建本地 PV Helm UI","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"helm","slug":"helm","permalink":"http://example.com/tags/helm/"}]},{"title":"K8s部署kong之docker","slug":"k8s部署kong之docker","date":"2019-05-08T19:03:41.000Z","updated":"2021-01-27T02:25:26.046Z","comments":true,"path":"2019/05/09/k8s部署kong之docker/","link":"","permalink":"http://example.com/2019/05/09/k8s%E9%83%A8%E7%BD%B2kong%E4%B9%8Bdocker/","excerpt":"","text":"Kong 官方镜像 https://hub.docker.com/_/kong 使用 docker 的安装目前Kong 只支持 postgresql 或 cassandra 两种存储数据，这里我们选用 PG 创建 pg 数据库 容器 123456docker run -d --name kong-database \\-p 5432:5432 \\-e &quot;POSTGRES_USER&#x3D;kong&quot; \\-e &quot;POSTGRES_DB&#x3D;kong&quot; \\-e &quot;POSTGRES_PASSWORD&#x3D;your_pg_password&quot; \\postgres:9.6 kong 数据迁移到 pg 1234567docker run --rm \\--link kong-database:kong-database \\ #将 kong-database 容器的地址引入注册到本容器-e &quot;KONG_DATABASE&#x3D;postgres&quot; \\-e &quot;KONG_PG_HOST&#x3D;kong-database&quot; \\-e &quot;KONG_PG_PASSWORD&#x3D;your_pg_password&quot; \\ # 官方文档未给出此参数 PG 可能不支持无密登录了-e &quot;KONG_CASSANDRA_CONTACT_POINTS&#x3D;kong-database&quot; \\kong kong migrations bootstrap konga 数据 12345678910docker run --rm --network&#x3D;host \\-e &quot;DB_DATABASEa&#x3D;postgres&quot; \\-e &quot;DB_HOST&#x3D;&lt;postgres-ip&gt;&quot; \\-e &quot;DB_PORT&#x3D;15432&quot; \\-e &quot;DB_USER&#x3D;postgres_user&quot; \\-e &quot;DB_PASSWORD&#x3D;postgres_pwd&quot; \\-e &quot;DB_DATABASE&#x3D;konga&quot; \\-e &quot;NODE_ENV&#x3D;development&quot; \\--name kong_dashboard \\pantsel&#x2F;konga 创建 kong 容器 12345678910111213141516docker run -d --name kong \\--link kong-database:kong-database \\-e &quot;KONG_DATABASE&#x3D;postgres&quot; \\-e &quot;KONG_PG_HOST&#x3D;kong-database&quot; \\-e &quot;KONG_PG_PASSWORD&#x3D;your_pg_password&quot; \\-e &quot;KONG_CASSANDRA_CONTACT_POINTS&#x3D;kong-database&quot; \\-e &quot;KONG_PROXY_ACCESS_LOG&#x3D;&#x2F;dev&#x2F;stdout&quot; \\-e &quot;KONG_ADMIN_ACCESS_LOG&#x3D;&#x2F;dev&#x2F;stdout&quot; \\-e &quot;KONG_PROXY_ERROR_LOG&#x3D;&#x2F;dev&#x2F;stderr&quot; \\-e &quot;KONG_ADMIN_ERROR_LOG&#x3D;&#x2F;dev&#x2F;stderr&quot; \\-e &quot;KONG_ADMIN_LISTEN&#x3D;0.0.0.0:8001, 0.0.0.0:8444 ssl&quot; \\-p 8000:8000 \\ # http 代理端口-p 8443:8443 \\ # https 代理端口-p 8001:8001 \\ # http 管理接口-p 8444:8444 \\ # https 管理接口kong 查看是否启动 1docker ps 如未启动通过日志查看问题 1docker logs kong 如正常启动 可访问管理Api（替换成你的IP） 1curl -X GET http:&#x2F;&#x2F;192.168.20.6:8001 因为 kong 服务是在容器中，所以设 KONG_ADMIN_LISTEN 为全局监听才能通过宿主机IP代理访问，宿主机则需对相应的映射端口做访问限制，如本机/内网访问，且不应该对外网可访问，不然谁都可以改你的Api网关策略了。 自定义 kong 配置文件kong docker 镜像的配置文件路径为 /etc/kong/kong.conf 如需自定义配置文件，自行挂载即可。 kong 配置项手册：https://docs.konghq.com/1.0.x… ... -v /opt/kong/kong.conf:/etc/kong/kong.conf ... 管理网关的API的使用教程这里就不写了，自行觅食吧~简单的看看下面这篇可以的 Kong 集成 Jwt 插件：https://www.cnkirito.moe/kong… kong服务网关API:https://www.jianshu.com/p/ef6…","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"kong","slug":"kong","permalink":"http://example.com/tags/kong/"}]},{"title":"K8s部署kong之yaml","slug":"k8s部署kong之yaml","date":"2019-05-08T19:03:31.000Z","updated":"2021-01-27T02:25:26.046Z","comments":true,"path":"2019/05/09/k8s部署kong之yaml/","link":"","permalink":"http://example.com/2019/05/09/k8s%E9%83%A8%E7%BD%B2kong%E4%B9%8Byaml/","excerpt":"","text":"官方文档官方安装：https://getkong.org/install/kubernetes/ 简介：Kong 是在客户端和（微）服务间转发API通信的API网关，通过插件扩展功能。 Kong 有两个主要组件： Kong Server ：基于 nginx 的服务器，用来接收 API 请求。 Apache Cassandra &amp; ：用来存储操作数据。 注：Ambassador没有数据库 - 它依赖于ConfigMap来存储状态 Kong 基本功能： HTTP 基本认证、密钥认证、CORS（ Cross-origin Resource Sharing，跨域资源共享）、TCP、UDP、文件日志、API 请求限流、请求转发以及 nginx 监控 Serf是一个去中心化的集群成员管理、故障检测解决方案，Kong用它做清缓存，看来Kong里面核心的数据consumer、api、plugin都是做了缓存的，还可以做集群节点的监控 Kong \b不足： 数据库不支持常用的mysql，只支持Postgres/Cassandra 扩展Kong需要会写lua脚本 不修改源码的情况下，无法自定义nginx配置文件，因为重启后会重新初始化有变更的nginx配置文件【nginx.conf|nginx-kong.conf】 安装过程中会创建一个 Postgres 的 StatefulSet，前面提到，这一版本对 Kubernetes 集群的最低版本要求是 1.8 Kong的限流 本地限流（local） 数据库限流（cluster） Redis限流 注：Kong的这三种限流方式都没有考虑并发情况 限流：https://my.oschina.net/chinamerp/blog/851613 当到达限流的临界值（max-1）时，此时有多条请求同时执行get_usage，计算结果全部通过，而我们期望的是仅有一条请求能通过 8000端口是可以给用户访问，就是说用户发送请求先到 Kong 项目的 8000 端口，然后Kong 项目帮你转到你的后端应用api。 Kong api简单使用8001 端口是管理端口，比如说，管理员可以通过 8001端口来得到你加入过的 api 列出 所加过的 api 1curl localhost:8001&#x2F;apis&#x2F; 加入 api 1curl -i -X POST --url http:&#x2F;&#x2F;localhost:8001&#x2F;apis&#x2F; --data &#39;upstream_url&#x3D;http:&#x2F;&#x2F;camp.uats.cc&#39; --data &#39;request_path&#x3D;login&#39; 上面这段命令表示： –url：http://localhost:8001/apis/ 固定的，加入 api 就得写这个，表示给 kong管理。 upstream_url：表示我们的网站。相当于一个请求前缀。 request_path：就是具体我们的 api。 删除 api 1curl -i -X DELETE localhost:8001&#x2F;apis&#x2F;00f90ca9-cf2d-4830-c842-3b90f6cd08af 后面 这个串表示 加入的api的 id。 初始设置 下载或克隆以下代码 12git clone https:&#x2F;&#x2F;github.com&#x2F;Kong&#x2F;kong-dist-kubernetes.gitcd kong-dist-kubernetes 修改配置文件主要是k8s对应service的类型修改为type:nodePort apiVersion: v1 kind: Service metadata: name: kong-proxy spec: type: LoadBalancer ## 将这里修改为nodeport loadBalancerSourceRanges: - 0.0.0.0/0 ports: - name: kong-proxy port: 8000 targetPort: 8000 protocol: TCP selector: app: kong 部署数据存储 注意：这里使用pg数据库 创建数据库 apiVersion: v1 kind: Service metadata: name: postgres spec: ports: - name: pgql port: 5432 targetPort: 5432 protocol: TCP selector: app: postgres --- apiVersion: v1 kind: ReplicationController metadata: name: postgres spec: replicas: 1 template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:9.6 env: - name: POSTGRES_USER value: kong - name: POSTGRES_PASSWORD value: kong - name: POSTGRES_DB value: kong - name: PGDATA value: /var/lib/postgresql/data/pgdata ports: - containerPort: 5432 volumeMounts: - mountPath: /var/lib/postgresql/data name: pg-data volumes: - name: pg-data emptyDir: &#123;&#125; 1kubectl create -f postgres.yaml 准备数据库运行迁移作业，以下为kong_migration_postgres.yaml 123456789101112131415161718192021apiVersion: batch&#x2F;v1kind: Jobmetadata:name: kong-migrationspec:template: metadata: name: kong-migration spec: containers: - name: kong-migration image: kong env: - name: KONG_NGINX_DAEMON value: &#39;off&#39; - name: KONG_PG_PASSWORD value: kong - name: KONG_PG_HOST value: postgres.default.svc.cluster.local command: [ &quot;&#x2F;bin&#x2F;sh&quot;, &quot;-c&quot;, &quot;kong migrations bootstrap&quot; ] restartPolicy: Never 1kubectl create -f kong_migration_postgres.yaml 将Kong管理员，代理服务和Deployment控制器部署到集群 apiVersion: v1 kind: Service metadata: name: kong-proxy spec: type: LoadBalancer loadBalancerSourceRanges: - 0.0.0.0/0 ports: - name: kong-proxy port: 8000 targetPort: 8000 protocol: TCP selector: app: kong --- apiVersion: v1 kind: Service metadata: name: kong-proxy-ssl spec: type: LoadBalancer loadBalancerSourceRanges: - 0.0.0.0/0 ports: - name: kong-proxy-ssl port: 8443 targetPort: 8443 protocol: TCP selector: app: kong --- apiVersion: v1 kind: Service metadata: name: kong-admin spec: type: LoadBalancer loadBalancerSourceRanges: - 0.0.0.0/0 ports: - name: kong-admin port: 8001 targetPort: 8001 protocol: TCP selector: app: kong --- apiVersion: v1 kind: Service metadata: name: kong-admin-ssl spec: type: LoadBalancer loadBalancerSourceRanges: - 0.0.0.0/0 ports: - name: kong-admin-ssl port: 8444 targetPort: 8444 protocol: TCP selector: app: kong --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kong-rc spec: replicas: 3 template: metadata: labels: name: kong-rc app: kong spec: containers: - name: kong image: kong env: - name: KONG_ADMIN_LISTEN value: &quot;0.0.0.0:8001, 0.0.0.0:8444 ssl&quot; - name: KONG_PG_PASSWORD value: kong - name: KONG_PG_HOST value: postgres - name: KONG_PROXY_ACCESS_LOG value: &quot;/dev/stdout&quot; - name: KONG_ADMIN_ACCESS_LOG value: &quot;/dev/stdout&quot; - name: KONG_PROXY_ERROR_LOG value: &quot;/dev/stderr&quot; - name: KONG_ADMIN_ERROR_LOG value: &quot;/dev/stderr&quot; ports: - name: admin containerPort: 8001 protocol: TCP - name: proxy containerPort: 8000 protocol: TCP - name: proxy-ssl containerPort: 8443 protocol: TCP - name: admin-ssl containerPort: 8444 protocol: TCP 1kubectl create -f kong_postgres.yaml 验证您的部署您现在可以看到已经使用的资源kubectl： 1kubectl get all 一旦EXTERNAL_IP可用于Kong代理和管理服务，您可以通过发出以下请求来测试Kong： 1234curl &lt;kong-admin-ip-address&gt;:8001curl https:&#x2F;&#x2F;&lt;admin-ssl-ip-address&gt;:8444curl &lt;kong-proxy-ip-address&gt;:8000curl https:&#x2F;&#x2F;&lt;kong-proxy-ssl-ip-address&gt;:8443 我这可以看到kong以及启动了，kong-rc都是running，举例测试例如： 1curl 192.168.1.216:31572 kong UI管理工具–dashboard 使用docker 进行启动 1docker run -d -p 8080:8080 pgbi&#x2F;kong-dashboard:v2 配置管理界面 12API node 输入 k8snodeip:service nodeport 例如我的是：192.168.1.216:31572 操作界面 测试接口 参考资料 https://github.com/PGBI/kong-dashboard https://getkong.org/install/kubernetes/ https://getkong.org/install/kubernetes/ http://www.cnblogs.com/shown1985/p/6484822.html API Gateway 模式： http://microservices.io/patterns/apigateway.html Nginx： https://www.nginx.com/blog/introduction-to-microservices/ Kong 项目官网：https://getkong.org/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"kong","slug":"kong","permalink":"http://example.com/tags/kong/"}]},{"title":"kafka入门教程","slug":"kafka入门教程","date":"2019-05-07T23:44:31.000Z","updated":"2021-01-27T02:25:26.049Z","comments":true,"path":"2019/05/08/kafka入门教程/","link":"","permalink":"http://example.com/2019/05/08/kafka%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/","excerpt":"","text":"1、认识kafka1.1 kafka简介Kafka 是一个分布式流媒体平台 kafka官网：http://kafka.apache.org/ （1）流媒体平台有三个关键功能： 发布和订阅记录流，类似于消息队列或企业消息传递系统。 以容错的持久方式存储记录流。 记录发生时处理流。 （2）Kafka通常用于两大类应用： 构建可在系统或应用程序之间可靠获取数据的实时流数据管道 构建转换或响应数据流的实时流应用程序 要了解Kafka如何做这些事情，让我们深入探讨Kafka的能力。 （3）首先是几个概念： Kafka作为一个集群运行在一个或多个可跨多个数据中心的服务器上。 Kafka集群以称为 topics主题 的类别存储记录流。 每条记录都包含一个键，一个值和一个时间戳。 （4）Kafka有四个核心API： Producer API（生产者API）允许应用程序发布记录流至一个或多个kafka的topics（主题）。 Consumer API（消费者API）允许应用程序订阅一个或多个topics（主题），并处理所产生的对他们记录的数据流。 Streams API（流API）允许应用程序充当流处理器，从一个或多个topics（主题）消耗的输入流，并产生一个输出流至一个或多个输出的topics（主题），有效地变换所述输入流，以输出流。 Connector API（连接器API）允许构建和运行kafka topics（主题）连接到现有的应用程序或数据系统中重用生产者或消费者。例如，关系数据库的连接器可能捕获对表的每个更改。 在Kafka中，客户端和服务器之间的通信是通过简单，高性能，语言无关的TCP协议完成的。此协议已版本化并保持与旧版本的向后兼容性。Kafka提供Java客户端，但客户端有多种语言版本。 1.2 Topics主题 和 partitions分区我们首先深入了解 Kafka 为记录流提供的核心抽象 - 主题topics 一个Topic可以认为是一类消息，每个topic将被分成多个partition(区),每个partition在存储层面是append log文件 主题是发布记录的类别或订阅源名称。Kafka的主题总是多用户; 也就是说，一个主题可以有零个，一个或多个消费者订阅写入它的数据。 对于每个主题，Kafka群集都维护一个如下所示的分区日志： 每个分区都是一个有序的，不可变的记录序列，不断附加到结构化的提交日志中。分区中的记录每个都分配了一个称为偏移的顺序ID号，它唯一地标识分区中的每个记录。 Kafka集群持久保存所有已发布的记录 - 无论是否已使用 - 使用可配置的保留期。例如，如果保留策略设置为两天，则在发布记录后的两天内，它可供使用，之后将被丢弃以释放空间。Kafka的性能在数据大小方面实际上是恒定的，因此长时间存储数据不是问题。 实际上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移或位置。这种偏移由消费者控制：通常消费者在读取记录时会线性地提高其偏移量，但事实上，由于该位置由消费者控制，因此它可以按照自己喜欢的任何顺序消费记录。例如，消费者可以重置为较旧的偏移量来重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。 这些功能组合意味着Kafka 消费者consumers 非常cheap - 他们可以来来往往对集群或其他消费者没有太大影响。例如，您可以使用我们的命令行工具“tail”任何主题的内容，而无需更改任何现有使用者所消耗的内容。 日志中的分区有多种用途。首先，它们允许日志扩展到超出适合单个服务器的大小。每个单独的分区必须适合托管它的服务器，但主题可能有许多分区，因此它可以处理任意数量的数据。其次，它们充当了并行性的单位 - 更多的是它。 1.3 Distribution分配 一个Topic的多个partitions,被分布在kafka集群中的多个server上;每个server(kafka实例)负责partitions中消息的读写操作;此外kafka还可以配置partitions需要备份的个数(replicas),每个partition将会被备份到多台机器上,以提高可用性. 基于replicated方案,那么就意味着需要对多个备份进行调度;每个partition都有一个server为”leader”;leader负责所有的读写操作,如果leader失效,那么将会有其他follower来接管(成为新的leader);follower只是单调的和leader跟进,同步消息即可..由此可见作为leader的server承载了全部的请求压力,因此从集群的整体考虑,有多少个partitions就意味着有多少个”leader”,kafka会将”leader”均衡的分散在每个实例上,来确保整体的性能稳定。 1.4 Producers生产者 和 Consumers消费者1.4.1 Producers生产者 Producers 将数据发布到指定的topics 主题。同时Producer 也能决定将此消息归属于哪个partition;比如基于”round-robin”方式或者通过其他的一些算法等。 1.4.2 Consumers 本质上kafka只支持Topic.每个consumer属于一个consumer group;反过来说,每个group中可以有多个 consumer.发送到Topic的消息,只会被订阅此Topic的每个group中的一个consumer消费。 如果所有使用者实例具有相同的使用者组，则记录将有效地在使用者实例上进行负载平衡。 如果所有消费者实例具有不同的消费者组，则每个记录将广播到所有消费者进程。 分析：两个服务器Kafka群集，托管四个分区（P0-P3），包含两个使用者组。消费者组A有两个消费者实例，B组有四个消费者实例。 在Kafka中实现消费consumption 的方式是通过在消费者实例上划分日志中的分区，以便每个实例在任何时间点都是分配的“公平份额”的独占消费者。维护组中成员资格的过程由Kafka协议动态处理。如果新实例加入该组，他们将从该组的其他成员接管一些分区; 如果实例死亡，其分区将分发给其余实例。 Kafka仅提供分区内记录的总订单，而不是主题中不同分区之间的记录。对于大多数应用程序而言，按分区排序与按键分区数据的能力相结合就足够了。但是，如果您需要对记录进行总订单，则可以使用仅包含一个分区的主题来实现，但这将意味着每个使用者组只有一个使用者进程。 1.5 Consumers kafka确保 发送到partitions中的消息将会按照它接收的顺序追加到日志中。也就是说，如果记录M1由与记录M2相同的生成者发送，并且首先发送M1，则M1将具有比M2更低的偏移并且在日志中更早出现。 消费者实例按照它们存储在日志中的顺序查看记录。对于消费者而言,它们消费消息的顺序和日志中消息顺序一致。 如果Topic的”replicationfactor”为N,那么允许N-1个kafka实例失效，我们将容忍最多N-1个服务器故障，而不会丢失任何提交到日志的记录。 1.6 kafka作为消息系统Kafka的流概念与传统的企业邮件系统相比如何？ （1）传统消息系统 消息传统上有两种模型：queuing排队 and publish-subscribe发布 - 订阅。在队列中，消费者池可以从服务器读取并且每个记录转到其中一个; 在发布 - 订阅中，记录被广播给所有消费者。这两种模型中的每一种都有优点和缺点。排队的优势在于它允许您在多个消费者实例上划分数据处理，从而可以扩展您的处理。不幸的是，一旦一个进程读取它已经消失的数据，队列就不是​​多用户。发布 - 订阅允许您将数据广播到多个进程，但由于每条消息都发送给每个订阅者，因此无法进行扩展处理。 卡夫卡的消费者群体概念概括了这两个概念。与队列一样，使用者组允许您将处理划分为一组进程（使用者组的成员）。与发布 - 订阅一样，Kafka允许您向多个消费者组广播消息。 （2）kafka 的优势 Kafka模型的优势在于每个主题都具有这些属性 - 它可以扩展处理并且也是多用户 - 不需要选择其中一个。 与传统的消息系统相比，Kafka具有更强的订购保证。 传统队列在服务器上按顺序保留记录，如果多个消费者从队列中消耗，则服务器按照存储顺序分发记录。但是，虽然服务器按顺序分发记录，但是记录是异步传递给消费者的，因此它们可能会在不同的消费者处出现故障。这实际上意味着在存在并行消耗的情况下丢失记录的顺序。消息传递系统通常通过具有“独占消费者”概念来解决这个问题，该概念只允许一个进程从队列中消耗，但当然这意味着处理中没有并行性。 kafka做得更好。通过在主题中具有并行性概念 - 分区 - ，Kafka能够在消费者流程池中提供订购保证和负载平衡。这是通过将主题中的分区分配给使用者组中的使用者来实现的，以便每个分区仅由该组中的一个使用者使用。通过这样做，我们确保使用者是该分区的唯一读者并按顺序使用数据。由于有许多分区，这仍然可以平衡许多消费者实例的负载。但请注意，消费者组中的消费者实例不能超过分区。 1.7 kafka作为存储系统 任何允许发布与消费消息分离的消息的消息队列实际上充当了正在进行的消息的存储系统。Kafka的不同之处在于它是一个非常好的存储系统。 写入Kafka的数据将写入磁盘并进行复制以实现容错。Kafka允许生产者等待确认，以便在完全复制之前写入不被认为是完整的，并且即使写入的服务器失败也保证写入仍然存在。 磁盘结构Kafka很好地使用了规模 - 无论服务器上有50 KB还是50 TB的持久数据，Kafka都会执行相同的操作。 由于认真对待存储并允许客户端控制其读取位置，您可以将Kafka视为一种专用于高性能，低延迟提交日志存储，复制和传播的专用分布式文件系统。 有关Kafka的提交日志存储和复制设计的详细信息，请阅读此页面。 1.8 kafka用于流处理 仅仅读取，写入和存储数据流是不够的，目的是实现流的实时处理。 在Kafka中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理以及生成连续数据流以输出主题的任何内容。 例如，零售应用程序可能会接收销售和发货的输入流，并输出重新排序流和根据此数据计算的价格调整。 可以使用生产者和消费者API直接进行简单处理。但是，对于更复杂的转换，Kafka提供了完全集成的Streams API。这允许构建执行非平凡处理的应用程序，这些应用程序可以计算流的聚合或将流连接在一起。 此工具有助于解决此类应用程序面临的难题：处理无序数据，在代码更改时重新处理输入，执行有状态计算等。 流API构建在Kafka提供的核心原语上：它使用生产者和消费者API进行输入，使用Kafka进行有状态存储，并在流处理器实例之间使用相同的组机制来实现容错。 2、kafka使用场景2.1 消息Messaging Kafka可以替代更传统的消息代理。消息代理的使用有多种原因（将处理与数据生成器分离，缓冲未处理的消息等）。与大多数消息传递系统相比，Kafka具有更好的吞吐量，内置分区，复制和容错功能，这使其成为大规模消息处理应用程序的理想解决方案。 根据经验，消息传递的使用通常相对较低，但可能需要较低的端到端延迟，并且通常取决于Kafka提供的强大的耐用性保证。 在这个领域，Kafka可与传统的消息传递系统（如ActiveMQ或 RabbitMQ）相媲美。 2.2 网站活动跟踪 Kafka的原始用例是能够将用户活动跟踪管道重建为一组实时发布 - 订阅源。这意味着站点活动（页面查看，搜索或用户可能采取的其他操作）将发布到中心主题，每个活动类型包含一个主题。这些源可用于订购一系列用例，包括实时处理，实时监控以及加载到Hadoop或离线数据仓库系统以进行脱机处理和报告。 活动跟踪通常非常高，因为为每个用户页面视图生成了许多活动消息。 2.3 度量Metrics Kafka通常用于运营监控数据。这涉及从分布式应用程序聚合统计信息以生成操作数据的集中式提要。 2.4 日志聚合 许多人使用Kafka作为日志聚合解决方案的替代品。日志聚合通常从服务器收集物理日志文件，并将它们放在中央位置（可能是文件服务器或HDFS）进行处理。Kafka抽象出文件的细节，并将日志或事件数据作为消息流更清晰地抽象出来。这允许更低延迟的处理并更容易支持多个数据源和分布式数据消耗。与Scribe或Flume等以日志为中心的系统相比，Kafka提供了同样出色的性能，由于复制而具有更强的耐用性保证，以及更低的端到端延迟。 2.5 流处理 许多Kafka用户在处理由多个阶段组成的管道时处理数据，其中原始输入数据从Kafka主题中消费，然后聚合，丰富或以其他方式转换为新主题以供进一步消费或后续处理。 例如，用于推荐新闻文章的处理管道可以从RSS订阅源抓取文章内容并将其发布到“文章”主题; 进一步处理可能会对此内容进行规范化或重复数据删除，并将已清理的文章内容发布到新主题; 最终处理阶段可能会尝试向用户推荐此内容。此类处理管道基于各个主题创建实时数据流的图形。从0.10.0.0开始，这是一个轻量级但功能强大的流处理库，名为Kafka Streams 在Apache Kafka中可用于执行如上所述的此类数据处理。除了Kafka Streams之外，其他开源流处理工具包括Apache Storm和 Apache Samza。 2.6 Event Sourcing Event Sourcing是一种应用程序设计风格，其中状态更改记录为按时间排序的记录序列。Kafka对非常大的存储日志数据的支持使其成为以这种风格构建的应用程序的出色后端。 2.7 提交日志 Kafka可以作为分布式系统的一种外部提交日志。该日志有助于在节点之间复制数据，并充当故障节点恢复其数据的重新同步机制。Kafka中的日志压缩功能有助于支持此用法。在这种用法中，Kafka类似于Apache BookKeeper项目。 3、kafka安装3.1 下载安装到官网 http://kafka.apache.org/downloads.html 下载想要的版本； 我这里下载的最新稳定版2.1.0 注：由于Kafka控制台脚本对于基于Unix和Windows的平台是不同的，因此在Windows平台上使用bin\\windows\\ 而不是bin/ 将脚本扩展名更改为.bat。 123[root@along ~]# wget http:&#x2F;&#x2F;mirrors.shu.edu.cn&#x2F;apache&#x2F;kafka&#x2F;2.1.0&#x2F;kafka_2.11-2.1.0.tgz[root@along ~]# tar -C &#x2F;data&#x2F; -xvf kafka_2.11-2.1.0.tgz[root@along ~]# cd &#x2F;data&#x2F;kafka_2.11-2.1.0&#x2F; 3.2 配置启动zookeeper kafka正常运行，必须配置zookeeper，否则无论是kafka集群还是客户端的生存者和消费者都无法正常的工作的；所以需要配置启动zookeeper服务。 （1）zookeeper需要java环境 1[root@along ~]# yum -y install java-1.8.0 （2）这里kafka下载包已经包括zookeeper服务，所以只需修改配置文件，启动即可。 如果需要下载指定zookeeper版本；可以单独去zookeeper官网http://mirrors.shu.edu.cn/apache/zookeeper/下载指定版本。 12345[root@along ~]# cd &#x2F;data&#x2F;kafka_2.11-2.1.0&#x2F;[root@along kafka_2.11-2.1.0]# grep &quot;^[^#]&quot; config&#x2F;zookeeper.propertiesdataDir&#x3D;&#x2F;tmp&#x2F;zookeeper #数据存储目录clientPort&#x3D;2181 #zookeeper端口maxClientCnxns&#x3D;0 注：可自行添加修改zookeeper配置 3.3 配置kafka（1）修改配置文件 1234567891011121314151617181920[root@along kafka_2.11-2.1.0]# grep &quot;^[^#]&quot; config&#x2F;server.propertiesbroker.id&#x3D;0listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;localhost:9092num.network.threads&#x3D;3num.io.threads&#x3D;8socket.send.buffer.bytes&#x3D;102400socket.receive.buffer.bytes&#x3D;102400socket.request.max.bytes&#x3D;104857600log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logsnum.partitions&#x3D;1num.recovery.threads.per.data.dir&#x3D;1offsets.topic.replication.factor&#x3D;1transaction.state.log.replication.factor&#x3D;1transaction.state.log.min.isr&#x3D;1log.retention.hours&#x3D;168log.segment.bytes&#x3D;1073741824log.retention.check.interval.ms&#x3D;300000zookeeper.connect&#x3D;localhost:2181zookeeper.connection.timeout.ms&#x3D;6000group.initial.rebalance.delay.ms&#x3D;0 注：可根据自己需求修改配置文件 broker.id：唯一标识ID listeners=PLAINTEXT://localhost:9092：kafka服务监听地址和端口 log.dirs：日志存储目录 zookeeper.connect：指定zookeeper服务 （2）配置环境变量 12345678 [root@along ~]# vim &#x2F;etc&#x2F;profile.d&#x2F;kafka.sh export KAFKA_HOME&#x3D;&quot;&#x2F;data&#x2F;kafka_2.11-2.1.0&quot; export PATH&#x3D;&quot;$&#123;KAFKA_HOME&#125;&#x2F;bin:$PATH&quot; [root@along ~]# source &#x2F;etc&#x2F;profile.d&#x2F;kafka.sh （3）配置服务启动脚本 [root@along ~]# vim /etc/init.d/kafka #!/bin/sh # # chkconfig: 345 99 01 # description: Kafka # # File : Kafka # # Description: Starts and stops the Kafka server # source /etc/rc.d/init.d/functions KAFKA_HOME=/data/kafka_2.11-2.1.0 KAFKA_USER=root export LOG_DIR=/tmp/kafka-logs [ -e /etc/sysconfig/kafka ] &amp;&amp; . /etc/sysconfig/kafka # See how we were called. case &quot;$1&quot; in start) echo -n &quot;Starting Kafka:&quot; /sbin/runuser -s /bin/sh $KAFKA_USER -c &quot;nohup $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties &gt; $LOG_DIR/server.out 2&gt; $LOG_DIR/server.err &amp;&quot; echo &quot; done.&quot; exit 0 ;; stop) echo -n &quot;Stopping Kafka: &quot; /sbin/runuser -s /bin/sh $KAFKA_USER -c &quot;ps -ef | grep kafka.Kafka | grep -v grep | awk &#39;&#123;print \\$2&#125;&#39; | xargs kill \\-9&quot; echo &quot; done.&quot; exit 0 ;; hardstop) echo -n &quot;Stopping (hard) Kafka: &quot; /sbin/runuser -s /bin/sh $KAFKA_USER -c &quot;ps -ef | grep kafka.Kafka | grep -v grep | awk &#39;&#123;print \\$2&#125;&#39; | xargs kill -9&quot; echo &quot; done.&quot; exit 0 ;; status) c_pid=`ps -ef | grep kafka.Kafka | grep -v grep | awk &#39;&#123;print $2&#125;&#39;` if [ &quot;$c_pid&quot; = &quot;&quot; ] ; then echo &quot;Stopped&quot; exit 3 else echo &quot;Running $c_pid&quot; exit 0 fi ;; restart) stop start ;; *) echo &quot;Usage: kafka &#123;start|stop|hardstop|status|restart&#125;&quot; exit 1 ;; esac 1234## 3.4 启动kafka服务（1）后台启动zookeeper服务 [root@along ~]# nohup zookeeper-server-start.sh /data/kafka_2.11-2.1.0/config/zookeeper.properties &amp; 123（2）启动kafka服务[root@along ~]# service kafka startStarting kafka (via systemctl): [ OK ][root@along ~]# service kafka statusRunning 86018[root@along ~]# ss -nutlNetid State Recv-Q Send-Q Local Address:Port Peer Address:Porttcp LISTEN 0 50 :::9092 :::*tcp LISTEN 0 50 :::2181 :::* 1234# 4、kafka使用简单入门## 4.1 创建主题topics创建一个名为“along”的主题，它只包含一个分区，只有一个副本： [root@along ~]# kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic alongCreated topic “along”. 123如果我们运行list topic命令，我们现在可以看到该主题： [root@along ~]# kafka-topics.sh –list –zookeeper localhost:2181along 123456## 4.2 发送一些消息Kafka附带一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。默认情况下，每行将作为单独的消息发送。运行生产者，然后在控制台中键入一些消息以发送到服务器。 [root@along ~]# kafka-console-producer.sh –broker-list localhost:9092 –topic along This is a messageThis is another message 1234## 4.3 启动消费者Kafka还有一个命令行使用者，它会将消息转储到标准输出。 [root@along ~]# kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic along –from-beginningThis is a messageThis is another message 12345678所有命令行工具都有其他选项; 运行不带参数的命令将显示更详细地记录它们的使用信息。 # 5、设置多代理kafka群集 到目前为止，我们一直在与一个broker运行，但这并不好玩。对于Kafka，单个代理只是一个大小为1的集群，因此除了启动一些代理实例之外没有太多变化。但是为了感受它，让我们将我们的集群扩展到三个节点（仍然在我们的本地机器上）。## 5.1 准备配置文件 [root@along kafka_2.11-2.1.0]# cd /data/kafka_2.11-2.1.0/[root@along kafka_2.11-2.1.0]# cp config/server.properties config/server-1.properties[root@along kafka_2.11-2.1.0]# cp config/server.properties config/server-2.properties[root@along kafka_2.11-2.1.0]# vim config/server-1.properties broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1[root@along kafka_2.11-2.1.0]# vim config/server-2.properties broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 123注：该broker.id 属性是群集中每个节点的唯一且永久的名称。我们必须覆盖端口和日志目录，因为我们在同一台机器上运行这些，并且我们希望让所有代理尝试在同一端口上注册或覆盖彼此的数据。## 5.2 开启集群另2个kafka服务 [root@along ~]# nohup kafka-server-start.sh /data/kafka_2.11-2.1.0/config/server-1.properties &amp;[root@along ~]# nohup kafka-server-start.sh /data/kafka_2.11-2.1.0/config/server-2.properties &amp;[root@along ~]# ss -nutlNetid State Recv-Q Send-Q Local Address:Port Peer Address:Porttcp LISTEN 0 50 ::ffff:127.0.0.1:9092 :::*tcp LISTEN 0 50 ::ffff:127.0.0.1:9093 :::*tcp LISTEN 0 50 ::ffff:127.0.0.1:9094 1234 ## 5.3 在集群中进行操作（1）现在创建一个复制因子为3的新主题my-replicated-topic [root@along ~]# kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 3 –partitions 1 –topic my-replicated-topicCreated topic “my-replicated-topic”. 12（2）在一个集群中，运行“describe topics”命令查看哪个broker正在做什么 [root@along ~]# kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 12345678910注释：第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。由于我们只有一个分区用于此主题，因此只有一行。- “leader”是负责给定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。- “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。- “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。请注意，Leader: 2，在我的示例中，节点2 是该主题的唯一分区的Leader。（3）可以在我们创建的原始主题上运行相同的命令，以查看它的位置 [root@along ~]# kafka-topics.sh –describe –zookeeper localhost:2181 –topic alongTopic:along PartitionCount:1 ReplicationFactor:1 Configs: Topic: along Partition: 0 Leader: 0 Replicas: 0 Isr: 0 1234（4）向我们的新主题发布一些消息： [root@along ~]# kafka-console-producer.sh –broker-list localhost:9092 –topic my-replicated-topic my test message 1my test message 2^C 123（5）现在让我们使用这些消息： [root@along ~]# kafka-console-consumer.sh –bootstrap-server localhost:9092 –from-beginning –topic my-replicated-topicmy test message 1my test message 2 1234## 5.4 测试集群的容错性（1）现在让我们测试一下容错性。Broker 2 充当leader 所以让我们杀了它： [root@along ~]# ps aux | grep server-2.properties |awk ‘{print $2}’106737[root@along ~]# kill -9 106737[root@along ~]# ss -nutltcp LISTEN 0 50 ::ffff:127.0.0.1:9092 :::*tcp LISTEN 0 50 ::ffff:127.0.0.1:9093 :::* 12（2）leader 已切换到其中一个从属节点，节点2不再位于同步副本集中： [root@along ~]# kafka-topics.sh –describe –zookeeper localhost:2181 –topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 0 Replicas: 2,0,1 Isr: 0,1 12（3）即使最初接受写入的leader 已经失败，这些消息仍可供消费： [root@along ~]# kafka-console-consumer.sh –bootstrap-server localhost:9092 –from-beginning –topic my-replicated-topicmy test message 1my test message 2 1234567# 6、使用Kafka Connect导入&#x2F;导出数据 从控制台写入数据并将其写回控制台是一个方便的起点，但有时候可能希望使用其他来源的数据或将数据从Kafka导出到其他系统。对于许多系统，您可以使用Kafka Connect导入或导出数据，而不是编写自定义集成代码。 Kafka Connect是Kafka附带的工具，用于向Kafka导入和导出数据。它是一个可扩展的工具，运行连接器，实现与外部系统交互的自定义​​逻辑。在本快速入门中，我们将了解如何使用简单的连接器运行Kafka Connect，这些连接器将数据从文件导入Kafka主题并将数据从Kafka主题导出到文件。（1）首先创建一些种子数据进行测试： [root@along ~]# echo -e “foo\\nbar” &gt; test.txt 12或者在Windows上： echo foo&gt; test.txtecho bar&gt;&gt; test.txt 12345（2）接下来，启动两个以独立模式运行的连接器，这意味着它们在单个本地专用进程中运行。提供三个配置文件作为参数。- 第一个始终是Kafka Connect流程的配置，包含常见配置，例如要连接的Kafka代理和数据的序列化格式。- 其余配置文件均指定要创建的连接器。这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。 [root@along ~]# connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties[2019-01-16 16:16:31,884] INFO Kafka Connect standalone worker initializing … (org.apache.kafka.connect.cli.ConnectStandalone:67)[2019-01-16 16:16:31,903] INFO WorkerInfo values:… … 123456789 注：Kafka附带的这些示例配置文件使用您之前启动的默认本地群集配置并创建两个连接器：第一个是源连接器，它从输入文件读取行并生成每个Kafka主题，第二个是宿连接器从Kafka主题读取消息并将每个消息生成为输出文件中的一行。（3）验证是否导入成功（另起终端）在启动过程中，您将看到许多日志消息，包括一些指示正在实例化连接器的日志消息。① 一旦Kafka Connect进程启动，源连接器应该开始从test.txt主题读取行并将其生成到主题connect-test，并且接收器连接器应该开始从主题读取消息connect-test 并将它们写入文件test.sink.txt。我们可以通过检查输出文件的内容来验证数据是否已通过整个管道传递： [root@along ~]# cat test.sink.txtfoobar 12② 请注意，数据存储在Kafka主题中connect-test，因此我们还可以运行控制台使用者来查看主题中的数据（或使用自定义使用者代码来处理它）： [root@along ~]# kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic connect-test –from-beginning{“schema”:{“type”:”string”,”optional”:false},”payload”:”foo”}{“schema”:{“type”:”string”,”optional”:false},”payload”:”bar”} 12（4）继续追加数据，验证 [root@along ~]# echo Another line&gt;&gt; test.txt[root@along ~]# cat test.sink.txtfoobarAnother line[root@along ~]# kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic connect-test –from-beginning{“schema”:{“type”:”string”,”optional”:false},”payload”:”foo”}{“schema”:{“type”:”string”,”optional”:false},”payload”:”bar”}{“schema”:{“type”:”string”,”optional”:false},”payload”:”Another line”}","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"}]},{"title":"Kubernetes之pod调度","slug":"Kubernetes之pod调度","date":"2019-05-05T23:22:51.000Z","updated":"2021-01-27T02:25:26.038Z","comments":true,"path":"2019/05/06/Kubernetes之pod调度/","link":"","permalink":"http://example.com/2019/05/06/Kubernetes%E4%B9%8Bpod%E8%B0%83%E5%BA%A6/","excerpt":"","text":"Pod的调度在默认情况下是Scheduler Controller 采用默认算法的全自动调度，在实际使用中并不满足我们的需求，因为并不能事先掌握pod被调度到哪个Node之上，所以kubernetes又提供了好几种方式让我们自已选择调度到什么Node中，比如有NodeSelector(定向调度)、NodeAffinity(Node亲和性)、PodAffinity(Pod亲和性)。 NodeSelector调度算法比较简单，NodeSelector 只调度到某个拥有特定标签的Node上，如果没有满足条件的Node，那么此Pod将不会被运行，即使在集群中还有可用Node列表，这就限制了它的使用场景，现在基本上被NodeAffinity取代了，NodeAffinity在NodeSelector的基础之上的进行了扩展，使调度更加灵活，除了有一个必须要满足条件的Node之外，还要可设置优先条件，下面来看下NodeAffinity的使用方式： 一、NodeAffinityNodeAffinity 亲和性有两种表达方式: RequiredDuringSchedulingIgnoredDuringExecution ：必须满足指定的规则才可以调度Pod到Node上，相当于硬限制。 PreferredDuringSchedulingIgnoredDuringExecution：强调优先满足指定的规则，相当于软限制，并不强求，如果多个优先级规则，还可以设置权重，以定义执行顺序。 IgnoredDuringExecution 表示 ，如果一个pod所在的节点 在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略Node上Label的变化，该pod继续在该节点上运行。 示例: 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: ubuntu-aff labels: os: centosspec: affinity: #亲和性设置 nodeAffinity: #设置node亲和性 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: #设置node拥有的标签 - key: disktype operator: In values: - hdd #表示一定会调度在拥有此标签(disktype&#x3D;hdd)的Node上。如果将requiredDuringSchedulingIgnoredDuringExecution改为 preferredDuringSchedulingIgnoredDuringExecution那么就不一定会调度到有disktype&#x3D;hdd标签的Node，它会看情况，如果有这个标签的Node负载已经是很高的了，那么就会调度到负载较低的Node上 从上面的配置中可以看到，它操作符operator为In，NodeAffinity语法支持的操作符有In、NotIn、Exists、DoesNotExist、Gt、Lt虽然没有节点排斥功能，但是用NotIn和DoesNotExist就可以实现排斥功能了。 NodeAffinity规则设置的注意事项如下: 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能最终运行在指定的Node上 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可,nodeSelectorTerms 属性是设置要调度到的node的标签的，因此nodeSelectorTerms 如果有多个说明是可以调度到不同的标签，即不同的Node之上，所以只需要匹配一个就行了 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的 matchExpressions 才能匹配成功。matchExpressions 是在nodeSelectorTerms 下，设置具体的一个Node标签，设置有多个如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的matchExpressions 才能匹配成功。matchExpressions 是在nodeSelectorTerms 下，设置具体的一个Node标签，设置有多个nodeSelectorTerms ，意味着匹配的Node需要有多个标签，且多个标签必须同时要有。nodeSelectorTerms ，意味着匹配的Node需要有多个标签，且多个标签必须同时要有。 上面的例子中，node亲和性条件只有一个，可以设置多个条件： 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Podmetadata: name: ubuntu-aff labels: os: centosspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: #第一条件 nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io&#x2F;arch operator: In values: - amd64 preferredDuringSchedulingIgnoredDuringExecution: #第二个条件 - weight: 1 preference: matchExpressions: - key: disktype operator: In values: - hdd containers: - name: ubuntu-aff image: 10.3.1.15:5000&#x2F;ubuntu:16.04 env: #在容器内配置环境变量 - name: Test value: &quot;7890&quot; command: [&quot;bash&quot;,&quot;-c&quot;,&quot;while true;do date;sleep 1;done&quot;] 如上，创建pod时，节点亲和性有两个，也就是说如果某个Node运行这个Pod，那么这个Node需要满足的条件有两个， 第一个条件是 属性为 requiredDuringSchedulingIgnoredDuringExecution ，此为硬性条件，必须要满足的： beta.kubernetes.io/arch=amd64 第二个条件属性为 preferredDuringSchedulingIgnoredDuringExecution ，此为软性条件，即优先满足的条件，尽可能会满足，如果无法满足就退而求其次，运行在其它Node之上。 那么调度时该如何选择调度到哪个Node上？ 为什么要设置两个条件呢，因为满足第一个条件的Node有很多，即拥有 beta.kubernetes.io/arch=amd64 的标签的Node有很多，这里第二个条件是说 要尽量运行在 disktype=hdd的Node上。 总的来说，即 只运行在arch=amd64的node上，在此条件之下优先运行在disktype=hdd的Node上。 二、PodAffinity： Pod亲和性与互斥性调度根据节点上正在运行的pod的标签来调度，而非node的标签，要求对节点和Pod两个条件进行匹配，其规则为：如果在具有标签X的Node上运行了一个或多个符合条件Y的Pod，那么Pod应该运行在此Node上，如果是互斥，则拒绝运行在此Node上。 也就是说根据某个已存在的pod，来决定要不要和此pod在一个Node上，在一起就需要设置亲和性，不和它在一起就设置互斥性。X指的是一个集群中的节点、机架、、区域等概念，通过Kubernetes内置节点标签中的key来进行声明，这个key的名字为topologyKey，用来表达节点所属的拓朴结构之下。 pod的亲和性表达方式与Node亲和性是一样的表达方式。 kubernetes内置标签： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region beta.kubernetes.io/instance-type beta.kubernetes.io/os beta.kubernetes.io/arch 1、创建参照Pod123456789101112131415161718192021#pod affinity的参照PodapiVersion: v1kind: Podmetadata: name: pod-flag labels: # 定义多个标签，以便其它pod设置亲和与互斥 app: nginx security: s1spec: containers: - name: nginx-1-10 image: 10.3.1.15:5000&#x2F;nginx:1.10 ports: - containerPort: 80#查看调度到哪个Node之上：root@ubuntu:# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEpod-flag 1&#x2F;1 Running 0 4m 192.168.150.232 10.3.1.16#运行在Node 10.3.1.16上 2、创建一个pod，与参照的pod必须要在同一个node上，即，具有亲和性。1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: with-pod-affinity labels: os: ubuntuspec: affinity: #亲和性设置 podAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: #必须要满足的条件 - labelSelector: #与哪个pod有亲和性，在此设置此pod具有的标签 matchExpressions: #要匹配的pod的，标签定义，如果定义了多个matchExpressions，则所有标签必须同时满足。 - key: security #标签的key operator: In #操作符 values: - s1 #标签的值，即拥有label security&#x3D;s1 topologyKey: kubernetes.io&#x2F;hostname #节点所属拓朴域 #上面表达的意思是此处创建的Pod必须要与拥有标签security&#x3D;s1的pod在同一Node上. containers: - name: wish-pod-affinity image: 10.3.1.15:5000&#x2F;ubuntu:16.04 env: - name: Test value: &quot;7890&quot; command: [&quot;bash&quot;,&quot;-c&quot;,&quot;while true;do date;sleep 1;done&quot;] # 因为pod是属于某个命名空间的，所以设置符合条件的目标Pod时，还可以指定哪个命名空间或全部命名空间里的Pod， # namespace的定义与labelSelector同级，如果不指定命名空间，则与此处创建的pod在一个namespace之中 1234567root@ubuntu:# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEwith-pod-affinity 1&#x2F;1 Running 0 11s 192.168.150.252 10.3.1.16pod-flag 1&#x2F;1 Running 0 1h 192.168.150.232 10.3.1.16#的确是在同一个Node上。#如果在创建时，pod状态一直处于Pending状态，很有可能是因为找不到满足条件的Node。 3、创建一个pod，与参照的pod，一定不能在同一个Node上，即具有互斥性。12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: with-pod-antiffinity labels: os: ubuntuspec: affinity: #亲和性设置 podAntiAffinity: #设置pod反亲和性 requiredDuringSchedulingIgnoredDuringExecution: #必须要满足的条件 - labelSelector: #与哪个pod有反亲和性，互斥性，在此设置此pod具有的标签 matchExpressions: - key: app #标签的key operator: In values: - nginx #标签的值，即拥有label app&#x3D;nginx topologyKey: kubernetes.io&#x2F;hostname #节点所属拓朴域 #上面表达的意思是此处创建的Pod必须要与拥有标签app&#x3D;nginx的pod不在在同一个Node上.如果所有 Node将无法满足条件，则创建的pod一直处于Pending状态。 containers: - name: wish-pod-antiaffinity image: 10.3.1.15:5000&#x2F;ubuntu:16.04 env: - name: Test value: &quot;7890&quot; command: [&quot;bash&quot;,&quot;-c&quot;,&quot;while true;do date;sleep 1;done&quot;] 1234567root@ubuntu15:&#x2F;data&#x2F;yaml# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEpod-flag 1&#x2F;1 Running 0 1h 192.168.150.232 10.3.1.16with-pod-affinity 1&#x2F;1 Running 0 1h 192.168.150.252 10.3.1.16with-pod-antiffinity 1&#x2F;1 Running 0 1m 192.168.77.204 10.3.1.17#可以看到与参照pod不在同一个node之上。 更多的的Pod亲和性调度信息可以参考官方文档。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"灵雀云K8s集群对接Prometheus","slug":"k8s集群对接prometheus","date":"2019-03-21T22:46:41.000Z","updated":"2021-01-27T02:25:26.047Z","comments":true,"path":"2019/03/22/k8s集群对接prometheus/","link":"","permalink":"http://example.com/2019/03/22/k8s%E9%9B%86%E7%BE%A4%E5%AF%B9%E6%8E%A5prometheus/","excerpt":"","text":"安装helm安装helmdocker run -ti --rm -v /usr/local/bin/:/var/log/abc index.alauda.cn/claas/helm:v2.10.0-rc.2 sh -c &quot;cp /systembin/helm /var/log/abc&quot; 安装chart_repo源(我自己的平台已经安装，所以不需要操作)docker run -d \\ -p 8088:8080 \\ -e PORT=8080 \\ -e DEBUG=1 \\ -e STORAGE=&quot;local&quot; \\ -e STORAGE_LOCAL_ROOTDIR=&quot;/data&quot; \\ -e BASIC_AUTH_USER=&quot;chartmuseum&quot; \\ -e BASIC_AUTH_PASS=&quot;chartmuseum&quot; \\ -v /data:/data \\ chartmuseum/chartmuseum:latest 检查确定安装成功helm repo list NAME URL local http://127.0.0.1:8879/charts stable http://chartmuseum:chartmuseum@172.16.16.21:8088 执行helm init 初始化helm init --stable-repo-url=http://chartmuseum:chartmuseum@172.16.16.21:8088 --tiller-image=index.alauda.cn/claas/tiller:v2.11.0 添加相关权限kubectl create serviceaccount --namespace kube-system tiller kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#39; kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 等待tiller pod启动kubectl get pods --all-namespaces |grep tiller 检查helm安装成功helm version Client: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdbxxxxxxxxxxxx44a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125; Server: &amp;version.Version&#123;SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdbxxxxxxxxxxxx44a339489417b146b&quot;, GitTreeState:&quot;clean&quot;&#125; 部署prometheus创建所需要的secretkubectl -n alauda-system create secret generic etcd-certs --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key --from-file=/etc/kubernetes/pki/etcd/ca.crt 修改配置 kube-scheduler：在各master节点上，编辑 vi /etc/kubernetes/manifests/kube-scheduler.yaml 将其中的“–address=127.0.0.1”，替换为“–address=0.0.0.0”,保存退出 kube-controller-manager：在各master节点上，编辑 vi /etc/kubernetes/manifests/kube-controller-manager.yaml 将其中的“–address=127.0.0.1”，替换为“–address=0.0.0.0”,保存退出 kube-proxy: 编辑cm kube-proxy kubectl edit -n kube-system cm kube-proxy 将“metricsBindAddress: 127.0.0.1:10249”替换为“metricsBindAddress: 0.0.0.0:10249”，保存退出；并杀掉kube-proxy的pod 注： dockerd： ACE-2.3/ACP-1.4采用docker v1.12.6。这个版本的docker不支持metrics-addr功能，导致prometheus获取不到dockerd相关数据。则grafana关于docker状态获取不到。 安装prometheus-operatorhelm install &lt;stable&gt;/prometheus-operator --namespace=alauda-system --name prometheus-operator --set global.registry.address=&lt;registry&gt; --timeout=3000 注: –set global.registry.address=是设置当前环境的registry地址 stable 为helm repo list 命令执行后，列出来的某一个repo源的name(是name，不是url)，此处可以不同过chart repo源下载安装，也可以将chart下载到master机器上，指定chart路径部署。 安装kube-prometheus 对接LocalVolume作为存储 给集群中的一个node添加 monitoring=enabled的label，用于local volume的调度，命令如下如下： kubectl label --overwrite nodes &lt;node hostname&gt; monitoring=enabled 在该node上创建以下目录用作持久化目录，保证空间 granafa 2G/prometheus 20G/alertmanager 1G，命令如下： mkdir -p /var/lib/monitoring/&#123;grafana,prometheus,alertmanager&#125; 如果要使用上述目录之外的其他目录，安装kube-prometheus指定以下参数即可（同样需要提前创建） --set grafana.storageSpec.persistentVolumeSpec.local.path=&lt;Your Path&gt; --set prometheus.storageSpec.persistentVolumeSpec.local.path=&lt;Your Path&gt; --set alertmanager.storageSpec.persistentVolumeSpec.local.path=&lt;Your Path&gt; 对接StorageClass作为存储 在安装kube-prometheus指定以下参数即可 --set grafana.storageSpec.volumeClaimTemplate.spec.storageClassName=&lt;sc name&gt; # grafana对接现有的StorageClass --set prometheus.storageSpec.volumeClaimTemplate.spec.storageClassName=&lt;sc name&gt; # prometheus对接现有的StorageClass --set alertmanager.storageSpec.volumeClaimTemplate.spec.storageClassName=&lt;sc name&gt; # alertmanager对接现有的StorageClass 安装kube-prometheus helm install stable/kube-prometheus --name kube-prometheus --namespace alauda-system --timeout=30000 \\ --set global.platform=&lt;ACE/ACP&gt; \\ --set prometheus.service.type=NodePort \\ --set grafana.service.type=NodePort \\ --set grafana.crd.accessUrl=http://&lt;ip&gt; \\ --set global.registry.address=&lt;registry&gt; \\ --set deployKubeDNS=&lt;true&gt; --set deployCoreDNS=&lt;false&gt; \\ --set alertmanager.configForACE.global.http_config.basic_auth.username=&lt;username&gt; \\ --set alertmanager.configForACE.global.http_config.basic_auth.password=&lt;password&gt; \\ --set alertmanager.configForACE.receivers[0].name=default-receiver \\ --set alertmanager.configForACE.receivers[0].webhook_configs[0].url=&lt;http://118.24.232.56:20081/v1/alerts/region_name/router&gt; \\ --set exporter-dockerd.endpoints=&lt;dockerd host ip&gt; # 以下参数均是可选的，如果你确认要修改这些配置可以加上这些参数，否则请不要随意设置这些参数 --set exporter-node.resources.requests.memory=300Mi \\ --set exporter-node.resources.limits.memory=500Mi –set global.platform=&lt;ACE/ACP&gt;是设置当前环境是ACP还是ACE【ACP/ACE二者选其一】 –set global.registry.address=是设置当前环境的registry地址【ACP/ACE均要设置】 –set grafana.crd.accessUrl=http://是设置ACP的grafana的访问地址 【ACP需要设置】 –set deployKubeDNS=true –set deployCoreDNS=false如果部署的是kubedns,就像上述设置即可；如果dns是coredns，则需要设置–set deployKubeDNS=false –set deployCoreDNS=true【ACP/ACE均要设置】 –set alertmanager.configForACE.global.http_config.basic_auth.username=是设置登录Global平台的用户名【ACE需要设置】 –set alertmanager.configForACE.global.http_config.basic_auth.password=是设置登录Global平台的密码【ACE需要设置】 –set alertmanager.configForACE.receivers[0].webhook_configs[0].url=http://118.24.232.56:20081/v1/alerts/region_name/router是设置alertmanager的Webhook的地址，其中118.24.232.56:20081是当前环境的jakiro的地址，region_name是当前私有区的名字【ACE需要设置】 –set exporter-dockerd.endpoints= 是指定有dockerd主机上ip地址，不然检测不到相关主机上的dockerd的metrics，参数示例：{10.0.96.19,10.0.96.22,10.0.96.33,10.0.96.30,10.0.96.40,10.0.96.42}(有两个以上的机器时，注意格式，前面的ip后面要加) –set exporter-node.resources.requests.memory=&lt;300Mi&gt;设置node-exporter的resource requests【可选，ACP/ACE均可选配置】 –set exporter-node.resources.limits.memory=&lt;500Mi&gt;设置node-exporter的resource limits【可选，ACP/ACE均可选配置】 检查pod# kubectl get pods -n alauda-system NAME READY STATUS RESTARTS AGE alertmanager-kube-prometheus-0 2/2 Running 0 2h kube-prometheus-exporter-kube-state-78b57fd848-gmsc7 2/2 Running 0 2h kube-prometheus-exporter-node-7klhv 1/1 Running 0 2h kube-prometheus-exporter-node-xsx5g 1/1 Running 0 2h kube-prometheus-grafana-f95c87485-79zt5 3/3 Running 0 2h prometheus-kube-prometheus-0 3/3 Running 1 2h prometheus-operator-6dd688ff7f-gd4px 1/1 Running 0 2h 检查kubelet的配置文件 找到配置文件 systemctl status kubelet kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2019-02-28 11:33:42 CST; 3 weeks 1 days ago Docs: http://kubernetes.io/docs/ Main PID: 21723 (kubelet) 添加以下内容（如果有可不添加） vim /etc/systemd/system/kubelet.service [Service] Environment=&quot;KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --authentication-token-webhook=true&quot; 重启kubelet systemctl daemon-reload systemctl restart kubelet 检查kubelet可以正常启动 system status kublet 访问Grafan/Prometheus grafana kubernetes集群中任何一个节点的ip:30902 prometheus kubernetes集群中任何一个节点的ip:30900，默认用户名和密码为: admin/admin 删除prometheus 删除 kube-prometheus helm delete --purge kube-prometheus kubectl delete pvc -n alauda-system prometheus-kube-prometheus-db-prometheus-kube-prometheus-0 kubectl delete pvc -n alauda-system alertmanager-kube-prometheus-db-alertmanager-kube-prometheus-0 删除 prometheus-operator helm delete --purge prometheus-operator kubectl delete --ignore-not-found customresourcedefinitions alertmanagers.monitoring.coreos.com prometheuses.monitoring.coreos.com prometheusrules.monitoring.coreos.com servicemonitors.monitoring.coreos.com 集群对接Prometheus监控 集成中心添加PrometheusOperator 类型的特性 进入”管理视图”，打开”集成中心”，选择”特性”栏，进入”添加集成”页面 选择 场景”特性”，类型”PrometheusOperator” 填写PrometheusOPerator有关集成信息 点击”创建”后，完成Prometheus集成实例的创建 集群对接Prometheus监控服务 进入”管理视图”，进入对应的集群。并依次点击”操作”、”集群特性管理”，进入”集群特性管理”页面 进入集群特性管理页面，查看”监控服务”，点击”对接第三方”。选择对应的Prometheus实例 查看集群相关的Cluster、Node、应用有监控数据出现","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"prometheus","slug":"prometheus","permalink":"http://example.com/tags/prometheus/"}]},{"title":"K8s默认的services ip不通","slug":"k8s默认的services-ip不通","date":"2019-03-21T04:09:54.000Z","updated":"2021-01-27T02:25:26.047Z","comments":true,"path":"2019/03/21/k8s默认的services-ip不通/","link":"","permalink":"http://example.com/2019/03/21/k8s%E9%BB%98%E8%AE%A4%E7%9A%84services-ip%E4%B8%8D%E9%80%9A/","excerpt":"","text":"写在前边的话今天团队的小测试集群突然挂掉，完全不能访问，紧急任务排查问题 问题现象平台挂掉，集群内有一台master节点的容器全部down掉，其中大部分pod显示Evicted状态 排查登录三台master节点分别get node发现其中有一台一直未返回，docker ps发现此节点的所有容器down掉，其它两台节点正常。通过其它两台正常节点排查发现kube-dns\\flanne\\kube-proxy等pod全部是失败状态 在有问题节点查看相关route \\ ip等信息，发现也全是缺少相关路由，cni0等网卡； 通过查看flannel容器日志发现： Failed to create SubnetManager: error retrieving pod spec for ‘kube-system/kube-flannel-8hpcf’: Get https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-8hpcf: dial tcp 10.96.0.1:443: i/o timeout 通过curl 10.96.0.1:443 发现不通 通过journalctl -fu kubelet也未发现有用信息 iptable也是正常 selinux也是关闭状态 此时已经暂时无思路，直接来个简单粗暴的reboot，但重启后问题依旧 解决本想着将此节点变成NotReady状态，然后再继续查问题 移动网卡文件 mv /etc/cni/net.d/10-flannel.conflist ~/ systemctl restart kubelet 重启服务 systemctl restart kubelet 此时节点已经变成NotReady状态 kubectl get no xuejian-global-01 Ready master 73d v1.11.6 xuejian-global-03 Ready master 73d v1.11.6 xuejian-global-02 NotReady master 73d v1.11.6 xuejian-slave-01 Ready node 10d v1.11.6 将文件移回 cp ~/10-flannel.conflist /etc/cni/net.d/10-flannel.conflist 此时节点竟然正常了，可以确定的是其它的什么也没有操作，它竟然恢复了 疑问问题其实还是没有解决的，不清楚具体原因是为啥，等明天再来继续排查吧，或者希望看到的大神能给个讲解","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"深入 Nginx 之配置篇","slug":"深入 Nginx 之配置篇","date":"2019-03-20T06:42:26.000Z","updated":"2021-01-27T02:25:26.052Z","comments":true,"path":"2019/03/20/深入 Nginx 之配置篇/","link":"","permalink":"http://example.com/2019/03/20/%E6%B7%B1%E5%85%A5%20Nginx%20%E4%B9%8B%E9%85%8D%E7%BD%AE%E7%AF%87/","excerpt":"","text":"注：原文链接：http://tenpercent.top/2019/02/15/learn-nginx-configuration/ 常用配置项在工作中，我们与 Nginx 打交道更多的是通过其配置文件来进行。那么掌握这些配置项各自的作用就很有必要了。 首先，nginx.conf 的内容通常是这样的： ... ... #核心摸块 events &#123; #事件模块 ... &#125; http &#123; # http 模块 server &#123; # server块 location [PATTERN] &#123; # location块 ... &#125; location [PATTERN] &#123; ... &#125; &#125; server &#123; ... &#125; &#125; mail &#123; # mail 模块 server &#123; # server块 ... &#125; &#125; 我们依次看一下每个模块一般有哪些配置项： 核心模块user admin; #配置用户或者组。 worker_processes 4; #允许生成的进程数，默认为1 pid /nginx/pid/nginx.pid; #指定 nginx 进程运行文件存放地址 error_log log/error.log debug; #错误日志路径，级别。 事件模块events &#123; accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off use epoll; #事件驱动模型select|poll|kqueue|epoll|resig worker_connections 1024; #最大连接数，默认为512 &#125; http 模块http &#123; include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型，默认为text/plain access_log off; #取消服务日志 sendfile on; #允许 sendfile 方式传输文件，默认为off，可以在http块，server块，location块。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 keepalive_timeout 65; #连接超时时间，默认为75s，可以在http，server，location块。 server &#123; keepalive_requests 120; #单连接请求上限次数。 listen 80; #监听端口 server_name 127.0.0.1; #监听地址 index index.html index.htm index.php; root your_path; #根目录 location ~ \\.php$ &#123; fastcgi_pass unix:/var/run/php/php7.1-fpm.sock; #fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi_params; &#125; &#125; &#125; 配置项解析 worker_processes worker_processes 用来设置 Nginx 服务的进程数。该值推荐使用 CPU 内核数。 worker_cpu_affinity worker_cpu_affinity 用来为每个进程分配CPU的工作内核，参数有多个二进制值表示，每一组代表一个进程，每组中的每一位代表该进程使用CPU的情况，1代表使用，0代表不使用。所以我们使用 worker_cpu_affinity 0001 0010 0100 1000;来让进程分别绑定不同的核上。默认情况下worker进程不绑定在任何一个CPU上。 worker_rlimit_nofile 设置毎个进程的最大文件打开数。如果不设的话上限就是系统的 ulimit –n的数字，一般为65535。 worker_connections 设置一个进程理论允许的最大连接数，理论上越大越好，但不可以超过 worker_rlimit_nofile 的值。 use epoll 设置事件驱动模型使用 epoll。epoll 是 Nginx 支持的高性能事件驱动库之一。是公认的非 常优秀的事件驱动模型。 accept_mutex off 关闭网络连接序列化，当其设置为开启的时候，将会对多个 Nginx 进程接受连接进行序列化，防止多个进程对连接的争抢。当服务器连接数不多时，开启这个参数会让负载有一定程度的降低。但是当服务器的吞吐量很大时，为了效率，请关闭这个参数；并且关闭这个参数的时候也可以让请求在多个 worker 间的分配更均衡。所以我们设置 accept_mutex off; multi_accept on 设置一个进程可同时接受多个网络连接 Sendfile on Sendfile是 Linux2.0 以后的推出的一个系统调用,它能简化网络传输过程中的步骤，提高服务器性能。 不用 sendfile的传统网络传输过程：硬盘 &gt;&gt; kernel buffer &gt;&gt; user buffer &gt;&gt; kernel socket buffer &gt;&gt; 协议栈 用 sendfile()来进行网络传输的过程： 硬盘 &gt;&gt; kernel buffer (快速拷贝到 kernelsocket buffer) &gt;&gt; 协议栈 tcp_nopush on 设置数据包会累积一下再一起传输，可以提高一些传输效率。 tcp_nopush 必须和 sendfile 搭配使用。 tcp_nodelay on 小的数据包不等待直接传输。默认为on。看上去是和 tcp_nopush 相反的功能，但是两边都为 on 时 nginx 也可以平衡这两个功能的使用。 keepalive_timeout HTTP 连接的持续时间。设的太长会使无用的线程变的太多。这个根据服务器访问数量、处理速度以及网络状况方面考虑。 send_timeout 设置 Nginx 服务器响应客户端的超时时间，这个超时时间只针对两个客户端和服务器建立连接后，某次活动之间的时间，如果这个时间后，客户端没有任何活动，Nginx服务器将关闭连接 gzip on 启用 gzip，对响应数据进行在线实时压缩,减少数据传输量。 gzip_disable “msie6” Nginx服务器在响应这些种类的客户端请求时，不使用 Gzip 功能缓存应用数据，gzip_disable “msie6”对IE6浏览器的数据不进行 GZIP 压缩。 常用的配置项大致这些，对于不同的业务场景，有的需要额外的其他配置项，这里不做展开。 其他http 配置里有 location 这一项，它是用来根据请求中的 uri 来为其匹配相应的处理规则。 location 查找规则location = / &#123; # 精确匹配 / ，主机名后面不能带任何字符串 [ config A ] &#125; location / &#123; # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求 # 但是正则和最长字符串会优先匹配 [ config B ] &#125; location /documents/ &#123; # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ config C ] &#125; location ~ /documents/Abc &#123; # 匹配任何以 /documents/Abc 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ config CC ] &#125; location ^~ /images/ &#123; # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。 [ config D ] &#125; location ~* \\.(gif|jpg|jpeg)$ &#123; # 匹配所有以 gif,jpg或jpeg 结尾的请求 # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则 [ config E ] &#125; location /images/ &#123; # 字符匹配到 /images/，继续往下，会发现 ^~ 存在 [ config F ] &#125; location /images/abc &#123; # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在 # F与G的放置顺序是没有关系的 [ config G ] &#125; location ~ /images/abc/ &#123; # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用 [ config H ] &#125; 正则查找优先级从高到低依次如下： “ = “ 开头表示精确匹配，如 A 中只匹配根目录结尾的请求，后面不能带任何字符串。 “ ^~ “ 开头表示uri以某个常规字符串开头，不是正则匹配 “ ~ “ 开头表示区分大小写的正则匹配; “ ~* “ 开头表示不区分大小写的正则匹配 “ / “ 通用匹配, 如果没有其它匹配,任何请求都会匹配到 负载均衡配置Nginx 的负载均衡需要用到 upstream 模块，可通过以下配置来实现： upstream test-upstream &#123; ip_hash; # 使用 ip_hash 算法分配 server 192.168.1.1; # 要分配的 ip server 192.168.1.2; &#125; server &#123; location / &#123; proxy_pass http://test-upstream; &#125; &#125; 上面的例子定义了一个 test-upstream 的负载均衡配置，通过 proxy_pass 反向代理指令将请求转发给该模块进行分配处理。 注：原文链接：http://tenpercent.top/2019/02/15/learn-nginx-configuration/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Nginx","slug":"Nginx","permalink":"http://example.com/tags/Nginx/"}]},{"title":"深入 Nginx 之架构篇","slug":"深入 Nginx 之架构篇","date":"2019-03-20T06:42:11.000Z","updated":"2021-01-27T02:25:26.051Z","comments":true,"path":"2019/03/20/深入 Nginx 之架构篇/","link":"","permalink":"http://example.com/2019/03/20/%E6%B7%B1%E5%85%A5%20Nginx%20%E4%B9%8B%E6%9E%B6%E6%9E%84%E7%AF%87/","excerpt":"","text":"注：原文链接：http://tenpercent.top/2019/01/18/depth-nginx/ 前言最近在读 Nginx 相关的书籍，做一下读书笔记。 Nginx 作为业界知名的高性能服务器，被广泛的应用。它的高性能正是由于其优秀的架构设计，其架构主要包括这几点：模块化设计、事件驱动架构、请求的多阶段异步处理、管理进程与多工作进程设计、内存池的设计，以下内容依次进行说明。 模块化设计高度模块化的设计是 Nginx 的架构基础。在 Nginx 中，除了少量的核心代码，其他一切皆为模块。 所有模块间是分层次、分类别的，Nginx 官方共有五大类型的模块：核心模块、配置模块、事件模块、HTTP 模块、mail 模块。它们之间的关系如下： 在这 5 种模块中，配置模块和核心模块是与 Nginx 框架密切相关的。而事件模块则是 HTTP 模块和 mail 模块的基础。HTTP 模块和 mail 模块的“地位”类似，它们都是更关注于应用层面。 事件驱动架构事件驱动架构，简单的说就是由一些事件发生源来产生事件，由事件收集器来收集、分发事件，然后由事件处理器来处理这些事件（事件处理器需要先在事件收集器里注册自己想处理的事件）。 对于 Nginx 服务器而言，一般由网卡、磁盘产生事件，Nginx 中的事件模块将负责事件的收集、分发操作；而所有的模块都可能是事件消费者，它们首先需要向事件模块注册感兴趣的事件类型，这样，在有事件产生时，事件模块会把事件分发到相应的模块中进行处理。 对于传统 web 服务器（如 Apache）而言，采用的所谓事件驱动往往局限在 TCP 连接建立、关闭事件上，一个连接建立以后，在其关闭之前的所有操作都不再是事件驱动，这时会退化成按顺序执行每个操作的批处理模式，这样每个请求在连接建立后都将始终占用着系统资源，直到关闭才会释放资源。这种请求占用着服务器资源等待处理的模式会造成服务器资源极大的浪费。如下图所示，传统 web 服务器往往把一个进程或线程作为时间消费者，当一个请求产生的事件被该进程处理时，直到这个请求处理结束时，进程资源都将被这一请求所占用。比较典型的例子如 Apache 同步阻塞的多进程模式就是这样的。 传统 web 服务器处理事件的简单模型（矩形代表进程）: Nginx 采用事件驱动架构处理业务的方式与传统的 web 服务器是不同的。它不使用进程或者线程来作为事件消费者，所谓的事件消费者只能是某个模块。只有事件收集、分发器才有资格占用进程资源，它们会在分发某个事件时调用事件消费模块使用当前占用的进程资源，如下图所示，该图中列出了 5 个不同的事件，在事件收集、分发者进程的一次处理过程中，这 5 个事件按照顺序被收集后，将开始使用当前进程分发事件，从而调用相应的事件消费者来处理事件。当然，这种分发、调用也是有序的。 Nginx 处理事件的简单模型： 由上图可以看出，处理请求事件时，Nginx 的事件消费者只是被事件分发者进程短期调用而已，这种设计使得网络性能、用户感知的请求时延都得到了提升，每个用户的请求所产生的事件会及时响应，整个服务器的网络吞吐量都会由于事件的及时响应而增大。当然，这也带来一定的要求，即每个事件消费者都不能有阻塞行为，否则将会由于长时间占用事件分发者进程而导致其他事件得不到及时响应，Nginx 的非阻塞特性就是由于它的模块都是满足这个要求的。 请求的多阶段异步处理多阶段异步处理请求与事件驱动架构是密切相关的，也就是说，请求的多阶段异步处理只能基于事件驱动架构实现。多阶段异步处理就是把一个请求的处理过程按照事件的触发方式划分为多个阶段，每个阶段都可以由事件收集、分发器来触发。 处理获取静态文件的 HTTP 请求时切分的阶段及各阶段的触发事件如下所示： 这个例子中，该请求大致分为 7 个阶段，这些阶段是可以重复发生的，因此，一个下载静态资源请求可能会由于请求数据过大，网速不稳定等因素而被分解为成百上千个上图所列出的阶段。 异步处理和多阶段是相辅相成的，只有把请求分为多个阶段，才有所谓的异步处理。当一个时间被分发到事件消费者中进行处理时，事件消费者处理完这个事件只相当于处理完 1 个请求的阶段。什么时候可以处理下一个阶段呢？这只能等待内核的通知，即当下一次事件出现时，epoll 等事件分发器将会获取到通知，然后去调用事件消费者进行处理。 管理进程、多工作进程设计Nginx 在启动后，会有一个 master 进程和多个 worker 进程。master 进程主要用来管理worker 进程，包括接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态以及启动 worker 进程。 worker 进程是用来处理来自客户端的请求事件。多个 worker 进程之间是对等的，它们同等竞争来自客户端的请求，各进程互相独立，一个请求只能在一个 worker 进程中处理。worker 进程的个数是可以设置的，一般会设置与机器 CPU 核数一致，这里面的原因与事件处理模型有关。Nginx 的进程模型，可由下图来表示： 在服务器上查看 Nginx 进程： 这种设计带来以下优点： 1） 利用多核系统的并发处理能力 现代操作系统已经支持多核 CPU 架构，这使得多个进程可以分别占用不同的 CPU 核心来工作。Nginx 中所有的 worker 工作进程都是完全平等的。这提高了网络性能、降低了请求的时延。 2） 负载均衡 多个 worker 工作进程通过进程间通信来实现负载均衡，即一个请求到来时更容易被分配到负载较轻的 worker 工作进程中处理。这也在一定程度上提高了网络性能、降低了请求的时延。 3） 管理进程会负责监控工作进程的状态，并负责管理其行为 管理进程不会占用多少系统资源，它只是用来启动、停止、监控或使用其他行为来控制工作进程。首先，这提高了系统的可靠性，当 worker 进程出现问题时，管理进程可以启动新的工作进程来避免系统性能的下降。其次，管理进程支持 Nginx 服务运行中的程序升级、配置项修改等操作，这种设计使得动态可扩展性、动态定制性较容易实现。 内存池的设计为了避免出现内存碎片，减少向操作系统申请内存的次数、降低各个模块的开发复杂度，Nginx 设计了简单的内存池，它的作用主要是把多次向系统申请内存的操作整合成一次，这大大减少了 CPU 资源的消耗，同时减少了内存碎片。 因此，通常每一个请求都有一个简易的独立内存池（如每个 TCP 连接都分配了一个内存池），而在请求结束时则会销毁整个内存池，把曾经分配的内存一次性归还给操作系统。这种设计大大提高了模块开发的简单些，因为在模块申请内存后不用关心它的释放问题；而且因为分配内存次数的减少使得请求执行的时延得到了降低。同时，通过减少内存碎片，提高了内存的有效利用率和系统可处理的并发连接数，从而增强了网络性能。 注：原文链接：http://tenpercent.top/2019/01/18/depth-nginx/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Nginx","slug":"Nginx","permalink":"http://example.com/tags/Nginx/"}]},{"title":"Linux 性能监控工具","slug":"Linux 性能监控工具","date":"2019-03-20T06:41:48.000Z","updated":"2021-01-27T02:25:26.040Z","comments":true,"path":"2019/03/20/Linux 性能监控工具/","link":"","permalink":"http://example.com/2019/03/20/Linux%20%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7/","excerpt":"","text":"注：原文连接：http://tenpercent.top/2018/07/10/Linux-performance-monitor/ Linux 性能指标 CPU CPU利用率、用户时间（表示CPU在用户进程上的时间百分比）、系统时间（表示CPU花在内核操作上的时间百分比）、空闲时间、平均负载、阻塞、上下文切换、中断等 内存 空闲内存、Swap利用率、缓冲和缓存、活动和非活动内存等 磁盘IO IO等待、平均队列长度、每秒传输（TPS）等 网络 接收和发送的包、每秒碰撞（各个网络接口所连接网络的所发生的冲突数量）、丢包、错误等 监测工具以下是对 Linux 的性能进行监控的常用工具： 工具 简介 top 查看进程活动状态以及一些系统状况 vmstat 查看系统状态、硬件和系统信息等 iostat 查看 CPU 负载、硬盘状况 sar 综合工具，查看系统状况 mpstat 查看多处理器状况 netstat 查看网络状况 iptraf 实时网络状态监测 tcpdump 抓取网络数据包，详细分析 tcptrace 网络包分析工具 netperf 网络带宽工具 dstat 综合了 vmstat、iostat、ifstat、netstat 等多个信息 top 工具top是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。 Tasks: 281 total, 1 running, 280 sleeping, 0 stopped, 0 zombie %Cpu(s): 8.5 us, 2.8 sy, 0.0 ni, 86.8 id, 1.4 wa, 0.0 hi, 0.4 si, 0.0 st KiB Mem : 16300820 total, 5955080 free, 2367488 used, 7978252 buff/cache KiB Swap: 0 total, 0 free, 0 used. 13466504 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 20373 root 20 0 1027288 930680 43260 S 24.0 5.7 1909:36 kube-apiserver 6439 root 20 0 475468 354316 33996 S 9.3 2.2 564:01.71 kube-controller 1852 root 20 0 10.2g 210320 44496 D 6.0 1.3 2724:13 etcd 22023 root 20 0 1456692 111800 36888 S 1.7 0.7 94:26.27 kubelet 3115 root 20 0 489188 24920 11632 S 0.7 0.2 230:39.64 flanneld 3823 root 20 0 47836 24852 12684 S 0.7 0.2 329:49.32 coredns 30021 root 20 0 84596 62604 15272 S 0.7 0.4 92:19.71 kube-scheduler 9 root 20 0 0 0 0 S 0.3 0.0 215:25.59 rcu_sched 1039 root 20 0 2701956 95516 17744 S 0.3 0.6 701:47.32 dockerd 1983 root 20 0 610316 13200 2376 S 0.3 0.1 331:44.18 barad_agent 3205 root 20 0 68952 36524 13144 S 0.3 0.2 222:29.18 nginx-ingress-c 3482 root 20 0 47300 22620 12664 S 0.3 0.1 116:26.12 coredns 23841 root 20 0 309956 66232 6120 S 0.3 0.4 11:08.46 ruby 26321 root 20 0 162104 2456 1584 R 0.3 0.0 0:00.04 top PID：进程的ID USER：进程所有者 PR：进程的优先级别，越小越优先被执行 NI：nice值 VIRT：进程占用的虚拟内存 RES：进程占用的物理内存 SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数 %CPU：进程占用CPU的使用率 %MEM：进程使用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND：进程启动命令名称 vmstat 工具vmstat 是个查看系统整体性能的小工具，小巧，即使在机器负载很高的情况下也运行良好，并且可以用时间间隔采集得到连续的性能数据。 [root@cloud-cn-master-1 ~]# vmstat 1 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 9 1 0 5953600 1868188 6112188 0 0 1 80 1 0 21 5 74 1 0 0 0 0 5953416 1868188 6112232 0 0 0 488 11335 17320 6 3 90 1 0 3 0 0 5952956 1868188 6112260 0 0 0 572 12337 18839 5 4 90 2 0 1 0 0 5951788 1868188 6112264 0 0 0 428 9709 14629 3 3 92 1 0 0 0 0 5952980 1868188 6112284 0 0 0 496 11227 16675 4 3 92 1 0 0 0 0 5952532 1868188 6112328 0 0 0 624 9619 14442 4 3 92 1 0 参数介绍： r，可运行队列的线程数，这些线程都是可运行状态，只不过 CPU 暂时不可用 b，被 blocked 的进程数，正在等待 IO 请求 in，被处理过的中断数 cs，系统上正在做上下文切换的数目 us，用户占用 CPU 的百分比 sys，内核和中断占用 CPU 的百分比 wa，所有可运行的线程被 blocked 以后都在等待 IO，这时候 CPU 空闲的百分比 id，CPU 完全空闲的百分比 举两个例子来分析一下： $ vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 4 0 140 2915476 341288 3951700 0 0 0 0 1057 523 89 21 0 0 0 4 0 140 2915724 341296 3951700 0 0 0 0 1048 546 79 11 0 0 0 4 0 140 2915848 341296 3951700 0 0 0 0 1044 514 88 22 0 0 0 4 0 140 2915848 341296 3951700 0 0 0 24 1044 564 80 20 0 0 0 4 0 140 2915848 341296 3951700 0 0 0 0 1060 546 78 12 0 0 0 从上面的数据可以看出几点： interrupts(in) 非常高，context switch(cs) 比较低，说明这个 CPU 一直在不停的请求资源 user time(us) 一直保持在 80% 以上，而且上下文切换较低 (cs)，说明某个进程可能一直霸占着 CPU run queue(r) 刚好在 4 个 $ vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 14 0 140 2904316 341912 3952308 0 0 0 460 1106 9593 36 64 1 0 0 17 0 140 2903492 341912 3951780 0 0 0 0 1037 9614 35 65 1 0 0 20 0 140 2902016 341912 3952000 0 0 0 0 1046 9739 35 64 1 0 0 17 0 140 2903904 341912 3951888 0 0 0 76 1044 9879 37 63 0 0 0 16 0 140 2904580 341912 3952108 0 0 0 0 1055 9808 34 65 1 0 0 从上面的数据可以看出几点： context switch(cs) 比 interrupts(in) 要高的多，说明内核不得不来回切换进程 进一步观察发现 system time(sy) 很高而 user time(us) 很低，而且加上高频度的上下文切换 (cs)，说明正在运行的应用程序调用了大量的系统调用 run queue(r) 在 14 个线程以上，而这个机器的硬件配置 (4 核），应该保持在 12 以内。 iostat 工具iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。iostat属于sysstat软件包。可以用yum install sysstat 直接安装。 [root@cloud-cn-master-1 ~]# iostat -mtx 2 Linux 3.10.0-862.11.6.el7.x86_64 (cloud-cn-master-1) 03/20/2019 _x86_64_ (4 CPU) 03/20/2019 10:54:37 PM avg-cpu: %user %nice %system %iowait %steal %idle 20.55 0.00 5.04 0.51 0.00 73.90 Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 6.06 0.01 33.68 0.00 0.25 15.07 0.05 1.57 5.97 1.57 0.63 2.12 vdb 0.03 4.57 0.09 5.33 0.00 0.06 24.15 0.02 3.95 1.68 3.99 0.52 0.28 vdc 0.00 0.00 0.00 0.00 0.00 0.00 31.86 0.00 0.52 0.52 0.00 0.42 0.00 cpu属性值说明： %user：CPU处在用户模式下的时间百分比。 %nice：CPU处在带NICE值的用户模式下的时间百分比。 %system：CPU处在系统模式下的时间百分比。 %iowait：CPU等待输入输出完成时间的百分比。 %steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。 %idle：CPU空闲时间百分比。 如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 disk属性值说明： rrqm/s: 每秒进行 merge 的读操作数目。即 rmerge/s wrqm/s: 每秒进行 merge 的写操作数目。即 wmerge/s r/s: 每秒完成的读 I/O 设备次数。即 rio/s w/s: 每秒完成的写 I/O 设备次数。即 wio/s rsec/s: 每秒读扇区数。即 rsect/s wsec/s: 每秒写扇区数。即 wsect/s rkB/s: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。 wkB/s: 每秒写K字节数。是 wsect/s 的一半。 avgrq-sz: 平均每次设备I/O操作的数据大小 (扇区)。 avgqu-sz: 平均I/O队列长度。 await: 平均每次设备I/O操作的等待时间 (毫秒)。 svctm: 平均每次设备I/O操作的服务时间 (毫秒)。 %util: 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。 如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间； 如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。 如果avgqu-sz比较大，也表示有大量io在等待。 sar 工具sar是System Activity Reporter（系统活动情况报告）的缩写。sar工具将对系统当前的状态进行取样，然后通过计算数据和比例来表达系统的当前运行状态。 查看 cpu使用率sar -u %user 用户模式下消耗的CPU时间的比例； %nice 通过nice改变了进程调度优先级的进程，在用户模式下消耗的CPU时间的比例 %system 系统模式下消耗的CPU时间的比例； %iowait CPU等待磁盘I/O导致空闲状态消耗的时间比例； %steal 利用Xen等操作系统虚拟化技术，等待其它虚拟CPU计算占用的时间比例； %idle CPU空闲时间比例； 查看平均负载sar -q runq-sz：运行队列的长度（等待运行的进程数） plist-sz：进程列表中进程（processes）和线程（threads）的数量 ldavg-1：最后1分钟的系统平均负载 ldavg-5：过去5分钟的系统平均负载 ldavg-15：过去15分钟的系统平均负载 查看内存使用状况sar -r kbmemfree：这个值和free命令中的free值基本一致,所以它不包括buffer和cache的空间. kbmemused：这个值和free命令中的used值基本一致,所以它包括buffer和cache的空间. %memused：物理内存使用率，这个值是kbmemused和内存总量(不包括swap)的一个百分比. kbbuffers和kbcached：这两个值就是free命令中的buffer和cache. kbcommit：保证当前系统所需要的内存,即为了确保不溢出而需要的内存(RAM+swap). %commit：这个值是kbcommit与内存总量(包括swap)的一个百分比. sar参数说明 -A 汇总所有的报告 -a 报告文件读写使用情况 -B 报告附加的缓存的使用情况 -b 报告缓存的使用情况 -c 报告系统调用的使用情况 -d 报告磁盘的使用情况 -g 报告串口的使用情况 -h 报告关于buffer使用的统计数据 -m 报告IPC消息队列和信号量的使用情况 -n 报告命名cache的使用情况 -p 报告调页活动的使用情况 -q 报告运行队列和交换队列的平均长 -R 报告进程的活动情况 -r 报告没有使用的内存页面和硬盘块 -u 报告CPU的利用率 -v 报告进程、i节点、文件和锁表状态 -w 报告系统交换活动状况 -y 报告TTY设备活动状况 注：原文连接：http://tenpercent.top/2018/07/10/Linux-performance-monitor/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"性能监控工具","slug":"性能监控工具","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7/"},{"name":"top","slug":"top","permalink":"http://example.com/tags/top/"},{"name":"vmstat","slug":"vmstat","permalink":"http://example.com/tags/vmstat/"},{"name":"iostat","slug":"iostat","permalink":"http://example.com/tags/iostat/"},{"name":"sar","slug":"sar","permalink":"http://example.com/tags/sar/"}]},{"title":"某云ace 2.3平台部署","slug":"某云ace 2.3平台部署","date":"2019-03-18T21:36:33.000Z","updated":"2021-01-27T02:25:26.053Z","comments":true,"path":"2019/03/19/某云ace 2.3平台部署/","link":"","permalink":"http://example.com/2019/03/19/%E6%9F%90%E4%BA%91ace%202.3%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2/","excerpt":"","text":"需要环境 一台数据库机器 一台ake机器，也就是部署机 三台master 三台global 三台es 注: 所有主机都需要单独挂载一块数据盘 环境准备 将所有主机启动 创建一个外网clb 在clb上进行绑定，32001-32006（挂载三台master)、3306（挂载mysql主机）端口 注：此文档安装docker为1.12.6，使用devicemapper存储 部署mysql 安装docker及docker-compose yum -y install docker docker-compose 编辑mysql yaml文件 vim mysql-master.yaml version: &#39;2&#39; services: mysql: network_mode: &quot;bridge&quot; ports: - &quot;3306:3306&quot; container_name: mysql-master image: mysql environment: MYSQL_ROOT_PASSWORD: &quot;xxxx&quot; ### 密码自行修改 image: index.alauda.cn/alaudaorg/mysql-master:latest ###### 注意此处images镜像的仓库地址需要更换，更换私有环境的镜像仓库IP privileged: true restart: always volumes: - /alauda/data/mysql/db:/var/lib/mysql 登录镜像仓库index.alauda.cn docker login index.alauda.cn 执行yaml文件，启动数据库 docker-compose -f mysql-master.yaml up -d 进入数据库修改内容 docker exec -it mysql-master bash mysql -u root -psgOnxrI9WN638Hj8; grant replication slave on *.* to &#39;master&#39;@&#39;%&#39; identified by &#39;sgOnxrI9WN638Hj8&#39;; flush privileges; show master status; +-------------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-------------------------+----------+--------------+------------------+-------------------+ | mysql-master-bin.000003 | 455 | | | | +-------------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 配置安装 注：建议单独挂一块大于100G的磁盘 下载安装包 scp alauda@xxxxxxx:/mnt/package/ALAUDA-release/ACE_2.3-1xxxxxx.tar.gz &lt;你的安装包目录&gt; 密码: xxxxx md5sum：xxxx 文件大小： xxx 所有主机配置hostname及hosts文件，并安装jq sshpass命令（建议使用ansible) 所有主机将单独的磁盘配置成lvm 进入数据目录/data/mnt/package/ace/ace_xxxxxx 编写server_list.json文件 修改所有主机内核参数： echo &quot;vm.max_map_count=262144&quot; &gt;&gt; /etc/sysctl.conf;sysctl -p 执行up-alaudaace.sh脚本开始安装 ./up-alaudaee.sh --network-interface=eth0 --kube_controlplane_endpoint clb-ip --alaudaee-domain-name clb-ip --db-info=&#39;DB_HOST=clb-ip;DB_PORT=3306;DB_USER=root;DB_PASSWORD=sgOnxrI9WN638Hj8;DB_ENGINE=mysql&#39; --debug &gt;注：命令中的用户名及密码可能需要修改，需要改成上边yaml里的 以下为相关参数 --network-interface 用来指定网卡 --alaudaee-domain-name global 的域名或者IP，具体解释请看白皮书 --kube_controlplane_endpoint k8s api lb 的地址，k8s api 的 vip 配置到这个 lb 上，具体解释请看白皮书 --db-info 数据库信息，其中DB_HOST、DB_PORT、DB_USER、DB_PASSWORD分别是数据库地址、端口、用户名和密码。DB_ENGINE是数据库类型，只有postgreql、mysql 两种 --master-is-node 可选参数，如果规划角色的时候，选择 k8s 集群的 master 也作为计算节点，就需要加上这个参数 --debug 可选参数，加上之后，ake 部署 k8s 集群的时候，会输出更多的信息，方便排除错误 --not-deploy-k8s 可选参数，不部署 k8s 集群，server_list.json 中记录的服务器，就是已经部署成功的 k8s 集群的所有服务器 访问web ace http://&lt;–alaudaee-domain-name 参数指定的地址&gt;:32005 账号/密码：alauda/alauda acp https://&lt;–kube_controlplane_endpoint 参数指定的地址&gt; 账号/密码：&#x61;&#100;&#109;&#105;&#110;&#x40;&#x61;&#108;&#x61;&#x75;&#x64;&#97;&#46;&#105;&#111;/password 常见问题 在快启动完成时会有两个Pod一直启动失败，需要手动删除 [root@xuejian-ace-update-m1 ~]# kubectl get deploy --all-namespaces | grep jaeger alauda-system jaeger-collector 1 1 1 0 21m alauda-system jaeger-query 1 1 1 0 21m 编辑deploy里的replicas参数，设置为0 [root@xuejian-ace-update-m1 ~]# kubectl edit deploy -n alauda-system jaeger-collector k8s安装完成后，在执行helm时会报错 在所有node上执行clear-all.sh脚本，重新运行脚本 upload chart 出错 找到chart-repo 容器挂载的卷，将权限改成777 即可，挂载的目录一般是安装目录下的ACP/chartmuseum ，执行 cat /alauda/.run_chart_repo.sh 可以看到具体的目录 若没有system项目，或未接入集群信息 在 jakiro 容器中手动运行： curl --request POST \\ --url http://127.0.0.1/v1/project-templates/&lt;org_name&gt;/ \\ --header &#39;authorization: Token &lt;token&gt;&#39; \\ --header &#39;content-type: application/json&#39; \\ --data &#39;&#123; &quot;name&quot;: &quot;empty-template&quot;, &quot;resource_actions&quot;: [ &quot;project_template:create&quot;, &quot;project_template:delete&quot;, &quot;project_template:view&quot; ], &quot;resources&quot;: [ ], &quot;roles&quot;: [ ] &#125; &#39; 注： &lt;org_name&gt;：根账号名称 &lt;token&gt;：根账号token，在用户中心页面查看 建议删掉jakiro 的数据库，重新给jakiro 做migrate 重新添加跟用户，重新对接ACE 集群。如果对上述操作不熟悉，也可以清空环境重新部署 获取集群secret kubectl get secret -n kube-system | awk &#39;/^clusterrole-aggregation-controller-token/&#123;print $1&#125;&#39; 获取集群token并记录 kubectl get secret -n kube-system 集群secret -o jsonpath=&#39;&#123;.data.token&#125;&#39; | base64 -d 删除数据库 DROP DATABASE jakirodb; 创建数据库 create database jakirodb; 数据库初始化，进入jakiro容器执行 python manage.py makemigrations python manage.py migrate 创建根用户 curl -X POST \\ -H &#39;Content-Type:application/json&#39; \\ -d &#39;&#123; &quot;username&quot;:&quot;&#39;alauda&#39;&quot;, &quot;password&quot;:&quot;alauda&quot;, &quot;realname&quot;:&quot;alauda&quot;, &quot;email&quot;:&quot;alauda@alauda.io&quot;, &quot;reference_code_id&quot;:&quot;cn&quot;, &quot;city&quot;:&quot;beijing&quot;, &quot;company&quot;:&quot;alauda&quot;, &quot;is_active&quot;: true, &quot;informed_way&quot;:&quot;others&quot;, &quot;industry&quot;:&quot;others&quot;, &quot;mobile&quot;:&quot;123123123123&quot;, &quot;position&quot;:&quot;beijing&quot;, &quot;currency&quot;:&quot;CNY&quot; &#125;&#39; http://api-server:32001/v1/auth/register/ 接入集群 点击平台的接入集群按钮 填写集群地址 （f5的地址或者alb的地址） 填写集群token （第二步获取的token）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"某云","slug":"某云","permalink":"http://example.com/tags/%E6%9F%90%E4%BA%91/"},{"name":"ace2.3","slug":"ace2-3","permalink":"http://example.com/tags/ace2-3/"}]},{"title":"使用Harbor管理Docker镜像","slug":"使用Harbor管理Docker镜像","date":"2019-03-14T22:50:09.000Z","updated":"2021-01-27T02:25:26.053Z","comments":true,"path":"2019/03/15/使用Harbor管理Docker镜像/","link":"","permalink":"http://example.com/2019/03/15/%E4%BD%BF%E7%94%A8Harbor%E7%AE%A1%E7%90%86Docker%E9%95%9C%E5%83%8F/","excerpt":"","text":"前言根据Harbor官方描述： Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。 部署 添加docker官方源 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安装docker yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.06.3.ce-3.el7 注：这里使用官方最新docker版本 安装docker-compose yum -y install docker-compose 修改docker配置文件/etc/docker/daemon.json （后续会用到），将harbor的IP添加到配置内。 &#123; &quot;insecure-registries&quot;: [&quot;10.0.0.34&quot;] &#125; 启动docker systemctl daemon-reload systemctl restart docker.service 安装Harbor，这里使用下载离线包的方式安装，请到github下载要使用的版本。 wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.4.tgz 解压并修改配置，这里要从公网访问web页面，所以配置文件里的hostname里写的是harbor的外网ip。 tar xvf harbor-offline-installer-v1.7.4.tgz cd harbor vim harbor.cfg hostname = 94.xxx.xxx.xxx 执行安装脚本 ./install.sh 等等完成后会启动10个容器 [root@VM_0_34_centos harbor]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 39b94959d4b6 goharbor/nginx-photon:v1.7.4 &quot;nginx -g &#39;daemon of…&quot; About an hour ago Up About an hour (healthy) 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginx d0ed8eefcc00 goharbor/harbor-portal:v1.7.4 &quot;nginx -g &#39;daemon of…&quot; About an hour ago Up 42 minutes (healthy) 80/tcp harbor-portal c47d91868179 goharbor/harbor-jobservice:v1.7.4 &quot;/harbor/start.sh&quot; About an hour ago Up 42 minutes harbor-jobservice 0b639dece741 goharbor/harbor-core:v1.7.4 &quot;/harbor/start.sh&quot; About an hour ago Up 42 minutes (healthy) harbor-core fee9d62ab1b8 goharbor/redis-photon:v1.7.4 &quot;docker-entrypoint.s…&quot; About an hour ago Up About an hour 6379/tcp redis 1aaec1227c62 goharbor/harbor-adminserver:v1.7.4 &quot;/harbor/start.sh&quot; About an hour ago Up About an hour (healthy) harbor-adminserver 931233aa6d8d goharbor/harbor-db:v1.7.4 &quot;/entrypoint.sh post…&quot; About an hour ago Up About an hour (healthy) 5432/tcp harbor-db 44aa216b1876 goharbor/registry-photon:v2.6.2-v1.7.4 &quot;/entrypoint.sh /etc…&quot; About an hour ago Up About an hour (healthy) 5000/tcp registry 6832dff60e6d goharbor/harbor-registryctl:v1.7.4 &quot;/harbor/start.sh&quot; About an hour ago Up About an hour (healthy) registryctl f8d7ae097c08 goharbor/harbor-log:v1.7.4 &quot;/bin/sh -c /usr/loc…&quot; About an hour ago Up About an hour (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-log root@VM_0_34_centos harbor]# 此时访web，即可进入harbor，页面默认管理用户及密码：admin/Harbor12345 复制管理 这是个很棒的功能，比如公司有很多个部署都有单独的镜像仓库用harbor管理（生产环境，测试环境等），当时测试环境的镜像测试成功后可以使用此功能将镜像复制到生产环境。 复制管理里添加规则，即可","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"registry","slug":"registry","permalink":"http://example.com/tags/registry/"},{"name":"Docker","slug":"Docker","permalink":"http://example.com/tags/Docker/"},{"name":"Harbor","slug":"Harbor","permalink":"http://example.com/tags/Harbor/"},{"name":"镜像","slug":"镜像","permalink":"http://example.com/tags/%E9%95%9C%E5%83%8F/"}]},{"title":"ES集群节点维护","slug":"ES集群节点维护","date":"2019-03-13T19:17:34.000Z","updated":"2021-01-27T02:25:26.036Z","comments":true,"path":"2019/03/14/ES集群节点维护/","link":"","permalink":"http://example.com/2019/03/14/ES%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E7%BB%B4%E6%8A%A4/","excerpt":"","text":"写在前边的话由于生产环境需要，现将ES集群节点进行精减维护，以下为具体操作步骤： 注：操作时请使用root权限执行 数据迁移ES集群运行中，每个节点数据都是平均的，下线维护一个节点，先把该节点数据迁移到其他节点，避免发生意外。以下使用Kibana进行操作。 例如，如果准备对172.31.1.34节点做维护 迁移命令如下： PUT _cluster/settings &#123; &quot;transient&quot;: &#123; &quot;cluster.routing.allocation.exclude._ip&quot;: &quot;172.31.1.34&quot; &#125; &#125; 迁移后进行查看： GET _cluster/settings 返回以下内容： &#123; &quot;persistent&quot;: &#123;&#125;, &quot;transient&quot;: &#123; &quot;cluster&quot;: &#123; &quot;routing&quot;: &#123; &quot;allocation&quot;: &#123; &quot;enable&quot;: &quot;all&quot;, &quot;exclude&quot;: &#123; &quot;_ip&quot;: &quot;172.31.1.34&quot; &#125; &#125; &#125; &#125; &#125; &#125; 这样172.31.1.34节点数据会逐步迁移到其他数据节点。最后该节点会保留一个searchguard副本，这个是正常的。根据数据量大小迁移完成时间也不一样，等待完成后可继续操作。 修改开机启动项 目的： 防止启动挂载错误的磁盘，执行错误的定时任务。 具体步骤： vim /etc/fstab crontab -e 将以上两个位置/es挂载点注释。 容器备份 目的： 防止出问题时候重建本机，保存2两条命令 具体步骤：执行history，把es镜像及启动历史命令记录下来。 docker pull index.alauda.cn/alaudaorg/elasticsearch-basic-http:v20181001.003257 docker run -d --name elasticsearch5 --restart=always --log-driver=json-file --net host -v /es/data:/esdata -e HOST_IP=172.31.1.32 -e NODE_MODEL=&quot;data&quot; -e ES_JAVA_OPTS=&quot;-Xms31g -Xmx31g&quot; -e ALAUDA_ES_TTL=7d -e ALAUDA_ES_CLUSTERS=&quot;172.31.1.40,172.31.1.41,172.31.1.42&quot; -e ALAUDA_ES_USERNAME=&quot;xxxxx&quot; -e ALAUDA_ES_PASSWORD=xxxx -e ALAUDA_ES_SHARDING=20 -e ALAUDA_ES_EXPECTED_NODE=14 -e ALAUDA_ES_MASTER_NODE=3 index.alauda.cn/alaudaorg/elasticsearch-basic-http:v20181001.003257 核对： 执行docker ps， 运行的ES版本镜像与历史命令对比，重点核对ES的版本一致即可。 停止容器 docker ps记录容器id docker stop 容器id 更换服务器规格登陆AWS管理界面。停止ES实例，修改服务器规格，把r4.4xlarge修改为r4.2xlarge。如果主机挂载了es额外硬盘，修改为新硬盘。 稳妥方式先创建新的数据磁盘挂载到/es目录，然后再删除原来数据磁盘。新建的磁盘需要格式化为ext4后挂载到/es。 重启后登陆服务器，首先停止ES容器（因为ES容器是开机自启动的）格式化新磁盘并进行挂载： mkfs.ext4 /dev/xvdg mount /dev/xvdg /es/ 最后启动ES实例。 配置ES上线启动ES容器后用docker logs查看日志，载kibana确认节点加入集群。登陆kibana管理端，查询： get _cluster/settings &#123; &quot;persistent&quot;: &#123;&#125;, &quot;transient&quot;: &#123; &quot;cluster&quot;: &#123; &quot;routing&quot;: &#123; &quot;allocation&quot;: &#123; &quot;enable&quot;: &quot;all&quot;, &quot;exclude&quot;: &#123; &quot;_ip&quot;: &quot;172.31.1.34&quot; &#125; &#125; &#125; &#125; &#125; &#125; 更新节点目前不分片分片数据。重新加入集群后，恢复分片数据，命令： PUT _cluster/settings &#123; &quot;transient&quot;: &#123; &quot;cluster.routing.allocation.exclude._ip&quot;: &quot;&quot; &#125; &#125; 以下返加True为正确 &#123; &quot;acknowledged&quot;: true, &quot;persistent&quot;: &#123;&#125;, &quot;transient&quot;: &#123; &quot;cluster&quot;: &#123; &quot;routing&quot;: &#123; &quot;allocation&quot;: &#123; &quot;exclude&quot;: &#123; &quot;_ip&quot;: &quot;&quot; &#125; &#125; &#125; &#125; &#125; &#125; 再次查询发现已经没有不可用的节点 GET _cluster/settings &#123; &quot;persistent&quot;: &#123;&#125;, &quot;transient&quot;: &#123; &quot;cluster&quot;: &#123; &quot;routing&quot;: &#123; &quot;allocation&quot;: &#123; &quot;enable&quot;: &quot;all&quot;, &quot;exclude&quot;: &#123; &quot;_ip&quot;: &quot;&quot; &#125; &#125; &#125; &#125; &#125; &#125; 执行完毕，触发ES的数据再平衡，索引数据会从其他节点重新分配给该节点。多观察。直到各节点shard数一致。 恢复开机启动项 vim /etc/fstab crontab -e &gt;将盘符修改为新盘符 至此一个节点从r4.4xlarge变更为r4.2xlarge。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"ES","slug":"ES","permalink":"http://example.com/tags/ES/"}]},{"title":"AWS常用cli命令","slug":"AWS常用cli命令","date":"2019-03-08T20:28:49.000Z","updated":"2021-01-27T02:25:26.034Z","comments":true,"path":"2019/03/09/AWS常用cli命令/","link":"","permalink":"http://example.com/2019/03/09/AWS%E5%B8%B8%E7%94%A8cli%E5%91%BD%E4%BB%A4/","excerpt":"","text":"写在前边的话对AWS服务操作可以通过管理控制台、各语言SDK、CLI以及API等方式。管理控制台最简单，可以直接通过Web界面操作，但是有些服务或者服务下的某些操作无法直接用控制台调用； API的方式最复杂，需要自己生成哈希值签署请求以及处理请求错误等低级别的操作，AWS 的大部分服务都提供REST的API以及类似于REST的查询API，API提供的服务操作是最新最全面的； SDK 的好处是封装好了请求签署与请求错误处理以及重试机制，用户只需要直接调用相关接口即可，但对于新的服务及操作的支持可能会滞后于API接口的发布。CLI 其实也可以看成SDK的一种，它是AWS 服务操作的一把瑞士军刀。 本文针对项目实践中所用到的 AWS 服务的常用 CLI 命令进行一个简单总结，方便以后查阅。 EC2 挂载 EBS linux 查看块设备： lsblk 格式化磁盘： sudo mkfs -t ext4 /dev/xvdb 挂载卷： sudo mount /dev/xvdb /mnt/mydir 卸载卷： sudo umount /dev/xvdb windows diskpart san policy=onlineall list disk disk yourdiskid attributes disk clear readonly online disk 实例操作 aws ec2 describe-instances aws ec2 describe-instances --instance-ids &quot;instanceid1&quot; &quot;instanceid2&quot; aws ec2 start-instances --instance-ids &quot;instanceid1&quot; &quot;instanceid2&quot; aws ec2 stop-intances --instance-ids &quot;instanceid1&quot; &quot;instanceid2&quot; aws ec2 run-instances --image-id ami-b6b62b8f --security-group-ids sg-xxxxxxxx --key-name mytestkey --block-device-mappings &quot;[&#123;\\&quot;DeviceName\\&quot;: \\&quot;/dev/sdh\\&quot;,\\&quot;Ebs\\&quot;:&#123;\\&quot;VolumeSize\\&quot;:100&#125;&#125;]&quot; --instance-type t2.medium --count 1 --subnet-id subnet-e8330c9c --associate-public-ip-address (Note: 若不指定subnet-id则会在默认vpc中去选，此时若指定了非默认vpc的安全组会出现请求错误。如无特殊要求，建议安全组和子网都不指定，就不会出现这种问题。) 查看region与AZ aws ec2 describe-region aws ec2 describe-availability-zones --region region-name 查看实例元数据和用户数据 curl http://169.254.169.254/latest/meta-data／ curl http://169.254.169.254/latest/user-data／ 查看ami aws ec2 describe-images key-pair aws ec2 create-key-pair --key-name mykeyname 安全组 aws ec2 create-security-group --group-name mygroupname --description mydescription --vpc-id vpc-id(若不指定vpc，则在默认vpc中创建安全组) aws ec2 authorize-security-group-ingress --group-id sg-xxxxyyyy --protocol tcp --port 22 --cidr 0.0.0.0/0 aws ec2 authorize-security-group-ingress --group-id sg-xxxxyyyy --protocol tcp --port 9999 --source-group sg-xxxxxxxx AutoScaling 列出AS组 aws autoscaling describe-auto-scaling-groups 列出AS实例 aws autoscaling describe-auto-scaling-instances --instance-ids [instance-id-1 instance-id-2 ...] 从组中分离实例 aws autoscaling detach-instances --auto-scaling-group-name myasgroup --instance-ids instanceid1 instanceid2 [--should-decrement-desired-capacity|--no-should-decrement-desired-capacity] 附加实例到组 aws autoscaling attach-instances --auto-scaling-group-name myasgroup --instance-ids instanceid1 instanceid2 挂起AS流程 aws autoscaling suspend-process --auto-scaling-group-name myasgroup --scaling-processes AZRebalance|AlarmNotification|... 删除AS组 aws autoscaling delete-auto-scaling-group --auto-scaling-group-name myasgroup S3 查看 aws s3 ls aws s3 ls s3://bucket aws s3 ls s3://bucket/prefix 拷贝 aws s3 cp /to/local/path s3://bucket/prefix aws s3 cp s3://bucket/prefix /to/local/path aws s3 cp s3://bucket1/prefix1 s3://bucket2/prefix2 同步 aws sync [--delete] /to/local/dir s3://bucket/prefixdir aws sync [--delete] s3://bucket/prefixdir /to/local/dir aws sync [--delete] s3://bucket1/prefixdir1 s3://bucket2/prefixdir2 手动分片上传 文件分片 split -b 40m myfile myfile-part- 创建分片上传任务 aws s3api create-multipart-upload --bucket bucketname --key prefix 记录返回值 &#123; &quot;Bucket&quot;: &quot;bucketname&quot;, &quot;UploadId&quot;: &quot;uploadeid&quot;, &quot;Key&quot;: &quot;prefix&quot; &#125; 上传分片 aws s3api upload-part --bucket bucketname --key prefix --part-number [分片上传编号(e.g. 1,2,3...)] --body myfile-[x] --upload-id uploadid 列出已上传分片，创建分片结构文件 aws s3api list-parts --bucket bucketname --key prefix --upload-id uploadid 将上命令结果中的parts部分保存为 temp 文件 &#123; &quot;Parts&quot;:[ &#123; &quot;PartNumber&quot;:1, &quot;ETag&quot;:&quot;&quot;xxxxxxx&quot;&quot; &#125;, &#123; &quot;PartNumber&quot;:2, &quot;ETag&quot;:&quot;&quot;xxxxxxxx&quot;&quot; &#125; ] &#125; 结束分片上传任务 aws s3api complete-multipart-upload --multipart-upload file://temp --bucket bucketname --key prefix --upload-id uploadid IAM Role 操作 aws iam create-role MY-ROLE-NAME --assum-role-policy-document file://path/to/trustpolicy.json aws iam put-role-policy --role-name MY-ROLE-NAME --policy-name MY-PERM-POLICY --policy-document file://path/to/permissionpolicy.json aws iam create-instance-profile --instance-profile-name MY-INSTANCE-PROFILE aws iam add-role-to-instance-profile --instance-profile-name MY-INSTANCE-PROFILE --role-name MY-ROLE-NAME AUTO-SCALING 查看信息 aws autoscaling describe-auto-scaling-groups aws autoscaling describe-auto-scaling-instances STS 代入ROLE的EC2实例的临时认证信息 curl http://169.254.169.254/latest/meta-data／iam/security-credentials/ROLE-NAME kinesis 创建流 aws kinesis create-stream –stream-name mystream –shard-count 列出流 aws kinesis list-streams 获取指定流的分片迭代器 aws kinesis get-shard-iterator –stream-name mystream –shard-id shard-1 –shard-iterator-type TRIM_HORIZON 发送数据到流 aws kinesis put-record –stream-name mystream –partition-key mykey –data test 获取流数据 aws kinesis get-records –shard-iterator myiterator","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"aws","slug":"aws","permalink":"http://example.com/tags/aws/"},{"name":"cli","slug":"cli","permalink":"http://example.com/tags/cli/"}]},{"title":"Kubernetes的Kube Proxy的转发规则分析","slug":"kubernetes的Kube-proxy的转发规则分析","date":"2019-03-08T20:10:56.000Z","updated":"2021-01-27T02:25:26.049Z","comments":true,"path":"2019/03/09/kubernetes的Kube-proxy的转发规则分析/","link":"","permalink":"http://example.com/2019/03/09/kubernetes%E7%9A%84Kube-proxy%E7%9A%84%E8%BD%AC%E5%8F%91%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90/","excerpt":"","text":"概要 filter表 nat表 Chain KUBE-SERVICES Chain KUBE-NODEPORTS (1 references) Chain KUBE-POSTROUTING (1 references) 报文处理流程图 发送到Node的报文的处理过程 Node发出的报文的处理过程概要 kube-proxy是kubernetes中设置转发规则的组件，通过iptables修改报文的流向。 以下是在一台kubernetes node节点上观察到的结果，kube-proxy是一个独立的组件，下面的观察结果适用于运行在其它地方的kube-proxy。 $kube-proxy --version kubernetes v1.5.2 通过iptables -L -t [iptables表名]可以看到，kube-proxy只修改了filter和nat表。五个检查点: INPUT OUPUT . | /_\\ +--------+ | _|_ +--------+ \\ / | &#39; Router --------|&gt; FORWARD . | | /_\\ +--------+ | | _|_ _|_ +---------+ \\ / \\ / | &#39; &#39; PKT ---&gt; PREROUTING POSTROUTING ---&gt; PKT filter表filter表中Chain: $iptables -t filter -L Chain INPUT (policy ACCEPT) target prot opt source destination KUBE-FIREWALL all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination DOCKER-ISOLATION all -- anywhere anywhere DOCKER all -- anywhere anywhere ACCEPT all -- anywhere anywhere ctstate RELATED,ESTABLISHED ACCEPT all -- anywhere anywhere ACCEPT all -- anywhere anywhere Chain OUTPUT (policy ACCEPT) target prot opt source destination KUBE-FIREWALL all -- anywhere anywhere KUBE-SERVICES all -- anywhere anywhere /* kubernetes service portals */ Chain DOCKER (1 references) target prot opt source destination Chain DOCKER-ISOLATION (1 references) target prot opt source destination RETURN all -- anywhere anywhere Chain KUBE-FIREWALL (2 references) target prot opt source destination DROP all -- anywhere anywhere /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000 Chain KUBE-SERVICES (1 references) target prot opt source destination REJECT tcp -- anywhere 10.254.153.61 /* first/webshell:http has no endpoints */ tcp dpt:http reject-with icmp-port-unreachable REJECT tcp -- anywhere 10.254.153.61 /* first/webshell:ssh has no endpoints */ tcp dpt:ssh reject-with icmp-port-unreachable 可以看到kube-proxy只设置了filter表中INPUT chain和OUTPUT chain，增加了KUBE-FIREWALL和KUBE-SERVICES两个规则链。 所有的出报文都要经过KUBE-SERVICES，如果一个Service没有对应的endpoint，则拒绝将报文发出: $./kubectl.sh get services -o wide -n first NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR webshell 10.254.153.61 &lt;none&gt; 80/TCP,22/TCP 3d name=webshell,type=pod 注意在KUBE-FIREWALL中，所有标记了0x8000的包都会被丢弃，标记动作可以发生在其它的表中。 nat表nat表中设置的规则比较多: (inbound)在PREROUTING阶段，将所有报文转发到KUBE-SERVICES (outbound)在OUTPUT阶段，将所有报文转发到KUBE-SERVICES (outbound)在POSTROUTING阶段，将所有报文转发到KUBE-POSTROUTING Chain KUBE-SERVICEStarget prot opt source destination KUBE-SVC-QMBTMOHBQS5DJKOG tcp -- anywhere 10.254.153.61 /* first/webshell:http cluster IP */ tcp dpt:http KUBE-SVC-TRP5S22NJPNCPLI2 tcp -- anywhere 10.254.153.61 /* first/webshell:ssh cluster IP */ tcp dpt:ssh KUBE-SVC-XGLOHA7QRQ3V22RZ tcp -- anywhere 172.16.60.36 /* kube-system/kubernetes-dashboard: cluster IP */ tcp dpt:http KUBE-SVC-NPX46M4PTMTKRN6Y tcp -- anywhere 10.254.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:https KUBE-NODEPORTS all -- anywhere anywhere /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL 可以看到，每个Service的每个服务端口都会在Chain KUBE-SERVICES中有一条对应的规则，发送到clusterIP的报文，将会转发到对应的Service的规则链，没有命中ClusterIP的，转发到KUBE-NODEPORTS。 Chain KUBE-SVC-XGLOHA7QRQ3V22RZ (2 references)target prot opt source destination KUBE-SEP-IIXSAVQWZXISB6RA all -- anywhere anywhere /* kube-system/kubernetes-dashboard: */ 而每一个SERVICE，又将报文提交到了各自的KUBE-SEP-XXX。 Chain KUBE-SEP-IIXSAVQWZXISB6RA (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 172.16.167.1 anywhere /* kube-system/kubernetes-dashboard: */ DNAT tcp -- anywhere anywhere /* kube-system/kubernetes-dashboard: */ tcp to:172.16.167.1:9090 最后在KUBE-SEP-XX中完整了最终的DNAT，将目的地址转换成了POD的IP和端口。 这里的KUBE-MARK-MASQ为报文打上了标记，表示这个报文是由kubernetes管理的，Kuberntes将会对它进行NAT转换。 Chain KUBE-MARK-MASQ (3 references) target prot opt source destination MARK all -- anywhere anywhere MARK or 0x4000 Chain KUBE-NODEPORTS (1 references)target prot opt source destination KUBE-MARK-MASQ tcp -- anywhere anywhere /* kube-system/kubernetes-dashboard: */ tcp dpt:31275 KUBE-SVC-XGLOHA7QRQ3V22RZ tcp -- anywhere anywhere /* kube-system/kubernetes-dashboard: */ tcp dpt:31275 可以看到，KUBE-NODEPORT中，根据目的端口，将报文转发到对应的Service的规则链，然后就如同在“Chain KUBE-SERVICES”中的过程，将报文转发到了对应的POD。 只有发送到被kubernetes占用的端口的报文才会进入KUBE-MARK-MASQ打上标记，并转发到对应的服务规则链。 例如这里分配给SERVICE的端口是31275，其它端口的包不由kuberentes管理. Chain KUBE-POSTROUTING (1 references)target prot opt source destination MASQUERADE all -- anywhere anywhere /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000 这里表示k8s管理的报文(也就是被标记了0x4000的报文)，在离开Node（物理机）的时候需要进行SNAT转换。 也就是POD发出的报文， 报文处理流程图下面的图中，没有画出KUBE-FIREWALL，KUBE-FIREWALL发生在filter表的INPUT和OUTPUT Chain中，下面的图中(FW)表示带有KUBE-FIREWALL。 (KUBE-SERVICES@nat): 表示nat表中的KUBE-SERVICES chain。 (KUBE-SERVICES@filter,nat): 表示在filter和nat中各有一个名为KUBE-SERVICES的chain。 发送到Node的报文的处理过程 报文先经过nat.prerouting，然后经过filter.input。 (KUBE-SVC@nat) +-&gt;SVC1 (KUBE-SERVICES@nat) | (KUBE-SEP@nat) +---&gt;命中ClusterIP --------------------+-&gt;SVC2 --&gt;SEP1,Mark0x0400,DNAT PREROUTING | ^ | | PKT --&gt;| | +-&gt;SVC3 | | | | +---&gt;未命中ClusterIP ---&gt;命中服务端口 | | | +-&gt;未命中服务端口 | | | v v +-----------------------------+ | INPUT(FW) |--&gt; END +-----------------------------+ Node发出的报文的处理过程 (KUBE-SVC@nat) (KUBE-SERVICES +-&gt;SVC1 OUTPUT(FW) @filter,nat) | (KUBE-SEP@nat) PKT -----&gt;命中ClusterIP ----------+-&gt;SVC2 --&gt;SEP1,Mark0x0400,DNAT | | | | +-&gt;SVC3 | | | | | | +-----------------+ | +------------&gt; | POSTROUTING | &lt;----------+ +--------+--------+ | v match 0x0400，SNAT@nat | v NIC","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"iptables","slug":"iptables","permalink":"http://example.com/tags/iptables/"}]},{"title":"使用 CLI 创建 Azure VM 的自定义映像","slug":"使用 CLI 创建 Azure VM 的自定义映像","date":"2019-03-07T16:03:02.000Z","updated":"2021-01-27T02:25:26.051Z","comments":true,"path":"2019/03/08/使用 CLI 创建 Azure VM 的自定义映像/","link":"","permalink":"http://example.com/2019/03/08/%E4%BD%BF%E7%94%A8%20CLI%20%E5%88%9B%E5%BB%BA%20Azure%20VM%20%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E6%98%A0%E5%83%8F/","excerpt":"","text":"自定义映像类似于应用商店映像，不同的是自定义映像的创建者是你自己。 自定义映像可用于启动配置，例如预加载应用程序、应用程序配置和其他 OS 配置。 在本教程中，你将创建自己的 Azure 虚拟机自定义映像。 你将学习如何执行以下操作： 取消预配和通用化 VM 创建自定义映像 从自定义映像创建 VM 列出订阅中的所有映像 删除映像 在 Azure 中国区使用 Azure CLI 2.0 之前，请先运行 az cloud set -n AzureChinaCloud 来改变云环境。如果想切回国际版 Azure，请再次运行 az cloud set -n AzureCloud。如果选择在本地安装并使用 CLI，本教程要求运行 Azure CLI 2.0.4 或更高版本。 运行 az --version 即可查找版本。 如果需要进行安装或升级，请参阅安装 Azure CLI 2.0。 开始之前下列步骤详细说明了如何将现有 VM 转换为可重用自定义映像，用于创建新的 VM 实例。若要完成本教程中的示例，必须现有一个虚拟机。 如果需要，此脚本示例可为你创建一个虚拟机。 按照教程进行操作时，请根据需要替换资源组和 VM 名称。 创建自定义映像若要创建虚拟机的映像，需通过以下方式准备 VM：取消源 VM 的预配，解除其分配，然后将其标记为通用化。 准备好 VM 后，可以创建映像。 取消预配 VM取消预配可通过删除特定于计算机的信息来通用化 VM。 实现此通用化后，即可从单个映像部署多个 VM。 在取消预配期间，主机名将重置为“localhost.localdomain”。 还会删除 SSH 主机密钥、名称服务器配置、根密码和缓存的 DHCP 租约。 若要取消预配 VM，请使用 Azure VM 代理 (waagent)。 Azure VM 代理安装在 VM 上，用于管理预配及其与 Azure 结构控制器的交互。 有关详细信息，请参阅 Azure Linux 代理用户指南。 使用 SSH 连接到 VM 并运行命令以取消预配 VM。 使用 +user 参数还会删除上次预配的用户帐户以及任何关联的数据。 将示例 IP 地址替换为 VM 的公共 IP 地址。 通过 SSH 连接到 VM。 ssh azureuser@52.174.34.95 取消预配 VM。 sudo waagent -deprovision+user -force 关闭 SSH 会话。 exit 解除分配 VM 并将其标记为通用化 若要创建映像，需要解除分配 VM。 使用 az vm deallocate 解除分配 VM。 az vm deallocate --resource-group myResourceGroup --name myVM 最后，使用 az vm generalize 将 VM 的状态设置为“通用化”，以便 Azure 平台知道 VM 已通用化。 只能从通用化 VM 创建映像。 az vm generalize --resource-group myResourceGroup --name myVM 创建映像现在，可使用 az image create 创建 VM 的映像。 以下示例从名为 myVM 的 VM 创建名为 myImage 的映像。 az image create \\ --resource-group myResourceGroup \\ --name myImage \\ --source myVM 从映像创建 VM现在，你已有了一个映像，可以使用 az vm create 从该映像创建一个或多个新 VM。 以下示例从名为 myImage 的映像创建名为 myVMfromImage 的 VM。 az vm create \\ --resource-group myResourceGroup \\ --name myVMfromImage \\ --image myImage \\ --admin-username azureuser \\ --generate-ssh-keys 映像管理下面是一些常见映像管理任务的示例，说明了如何使用 Azure CLI 完成这些任务。以表格格式按名称列出所有映像。 az image list \\ --resource-group myResourceGroup 删除映像。 此示例将从 myResourceGroup 中删除名为 myOldImage 的映像。 az image delete \\ --name myOldImage \\ --resource-group myResourceGroup 后续步骤在本教程中，你已创建了一个自定义 VM 映像。 你已了解如何： 取消预配和通用化 VM 创建自定义映像 从自定义映像创建 VM 列出订阅中的所有映像 删除映像","categories":[{"name":"Windows","slug":"Windows","permalink":"http://example.com/categories/Windows/"}],"tags":[{"name":"cli","slug":"cli","permalink":"http://example.com/tags/cli/"},{"name":"Azure","slug":"Azure","permalink":"http://example.com/tags/Azure/"}]},{"title":"发布Azure资源管理（ARM）模板","slug":"发布Azure资源管理（ARM）模板","date":"2019-03-07T15:47:01.000Z","updated":"2021-01-27T02:25:26.052Z","comments":true,"path":"2019/03/07/发布Azure资源管理（ARM）模板/","link":"","permalink":"http://example.com/2019/03/07/%E5%8F%91%E5%B8%83Azure%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%EF%BC%88ARM%EF%BC%89%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"以下为代码模板，根据自己情况修改&#123; &quot;$schema&quot;: &quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&quot;, &quot;contentVersion&quot;: &quot;1.0.0.0&quot;, &quot;parameters&quot;: &#123; &quot;virtualMachineName&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;虚拟机名称&quot; &#125; &#125;, &quot;virtualMachineSize&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;defaultValue&quot;:&quot;Standard_D4s_v3&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;虚拟机大小&quot; &#125; &#125;, &quot;adminUsername&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;用户名&quot; &#125; &#125;, &quot;virtualNetworkName&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;虚拟网络名&quot; &#125; &#125;, &quot;adminPassword&quot;: &#123; &quot;type&quot;: &quot;securestring&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;用户密码&quot; &#125; &#125;, &quot;diagnosticsStorageAccountName&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;存储账户名&quot; &#125; &#125;, &quot;subnetName&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;子网名称&quot; &#125; &#125;, &quot;publicIpAddressName&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;metadata&quot;: &#123; &quot;description&quot;: &quot;公网地址名称&quot; &#125; &#125; &#125;, &quot;variables&quot;: &#123; &quot;addressPrefix&quot;: &quot;10.0.0.0/16&quot;, &quot;subnetPrefix&quot;: &quot;10.0.0.0/24&quot;, &quot;networkInterfaceName&quot;: &quot;alauda-Nic0&quot;, &quot;networkSecurityGroupName&quot;: &quot;alauda-NSG0&quot;, &quot;diagnosticsStorageAccountType&quot;: &quot;Standard_LRS&quot;, &quot;vnetId&quot;: &quot;[resourceId(parameters(&#39;virtualMachineName&#39;),&#39;Microsoft.Network/virtualNetworks&#39;, parameters(&#39;virtualNetworkName&#39;))]&quot;, &quot;subnetRef&quot;: &quot;[resourceId(&#39;Microsoft.Network/virtualNetworks/subnets&#39;, parameters(&#39;virtualNetworkName&#39;), parameters(&#39;subnetName&#39;))]&quot; &#125;, &quot;resources&quot;: [ &#123; &quot;name&quot;: &quot;[parameters(&#39;virtualMachineName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines&quot;, &quot;apiVersion&quot;: &quot;2018-06-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Network/networkInterfaces/&#39;, variables(&#39;networkInterfaceName&#39;))]&quot;, &quot;[concat(&#39;Microsoft.Storage/storageAccounts/&#39;, parameters(&#39;diagnosticsStorageAccountName&#39;))]&quot; ], &quot;properties&quot;: &#123; &quot;osProfile&quot;: &#123; &quot;computerName&quot;: &quot;[parameters(&#39;virtualMachineName&#39;)]&quot;, &quot;adminUsername&quot;: &quot;[parameters(&#39;adminUsername&#39;)]&quot;, &quot;adminPassword&quot;: &quot;[parameters(&#39;adminPassword&#39;)]&quot; &#125;, &quot;hardwareProfile&quot;: &#123; &quot;vmSize&quot;: &quot;[parameters(&#39;virtualMachineSize&#39;)]&quot; &#125;, &quot;storageProfile&quot;: &#123; &quot;imageReference&quot;: &#123; &quot;publisher&quot;: &quot;xxxx&quot;, # 上传虚拟机镜像时填写的服务商ID &quot;offer&quot;: &quot;xxxx-xxxx-xxx&quot;, # 上传虚拟机镜像时填写的服务ID &quot;sku&quot;: &quot;standard&quot;, # 上传虚拟机镜像时填写的sku &quot;version&quot;: &quot;latest&quot; &#125;, &quot;osDisk&quot;: &#123; &quot;caching&quot;: &quot;ReadWrite&quot;, &quot;createOption&quot;: &quot;FromImage&quot;, &quot;managedDisk&quot;: &#123; &quot;storageAccountType&quot;: &quot;Standard_LRS&quot; &#125; &#125;, &quot;dataDisks&quot;: [ # 这里是给新创建的虚拟机添加一块50G的磁盘 &#123; &quot;name&quot;: &quot;alaudadatadisk&quot;, &quot;lun&quot;: 0, &quot;caching&quot;: &quot;ReadWrite&quot;, &quot;createOption&quot;: &quot;Empty&quot;, &quot;diskSizeGB&quot;: 50, &quot;managedDisk&quot;: &#123; &quot;storageAccountType&quot;: &quot;Premium_LRS&quot; &#125; &#125; ] &#125;, &quot;networkProfile&quot;: &#123; &quot;networkInterfaces&quot;: [ &#123; &quot;id&quot;: &quot;[resourceId(&#39;Microsoft.Network/networkInterfaces&#39;, variables(&#39;networkInterfaceName&#39;))]&quot; &#125; ] &#125;, &quot;diagnosticsProfile&quot;: &#123; &quot;bootDiagnostics&quot;: &#123; &quot;enabled&quot;: true, &quot;storageUri&quot;: &quot;[concat(&#39;https://&#39;, parameters(&#39;diagnosticsStorageAccountName&#39;), &#39;.blob.core.chinacloudapi.cn/&#39;)]&quot; &#125; &#125; &#125; &#125;, &#123; &quot;name&quot;: &quot;[parameters(&#39;diagnosticsStorageAccountName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts&quot;, &quot;apiVersion&quot;: &quot;2017-10-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;sku&quot;: &#123; &quot;name&quot;: &quot;[variables(&#39;diagnosticsStorageAccountType&#39;)]&quot; &#125;, &quot;properties&quot;: &#123;&#125; &#125;, &#123; &quot;name&quot;: &quot;[parameters(&#39;virtualNetworkName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Network/virtualNetworks&quot;, &quot;apiVersion&quot;: &quot;2018-08-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;properties&quot;: &#123; &quot;addressSpace&quot;: &#123; &quot;addressPrefixes&quot;: [ &quot;[variables(&#39;addressPrefix&#39;)]&quot; ] &#125;, &quot;subnets&quot;: [ &#123; &quot;name&quot;: &quot;[parameters(&#39;subnetName&#39;)]&quot;, &quot;properties&quot;: &#123; &quot;addressPrefix&quot;: &quot;[variables(&#39;subnetPrefix&#39;)]&quot;, &quot;networkSecurityGroup&quot;: &#123; &quot;id&quot;: &quot;[resourceId(&#39;Microsoft.Network/networkSecurityGroups&#39;, variables(&#39;networkSecurityGroupName&#39;))]&quot; &#125; &#125; &#125; ] &#125;, &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.Network/networkSecurityGroups&#39;, variables(&#39;networkSecurityGroupName&#39;))]&quot; ] &#125;, &#123; &quot;name&quot;: &quot;[variables(&#39;networkInterfaceName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Network/networkInterfaces&quot;, &quot;apiVersion&quot;: &quot;2018-08-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Network/virtualNetworks/&#39;, parameters(&#39;virtualNetworkName&#39;))]&quot;, &quot;[concat(&#39;Microsoft.Network/publicIpAddresses/&#39;, parameters(&#39;publicIpAddressName&#39;))]&quot;, &quot;[concat(&#39;Microsoft.Network/networkSecurityGroups/&#39;, variables(&#39;networkSecurityGroupName&#39;))]&quot; ], &quot;properties&quot;: &#123; &quot;ipConfigurations&quot;: [ &#123; &quot;name&quot;: &quot;ipconfig1&quot;, &quot;properties&quot;: &#123; &quot;subnet&quot;: &#123; &quot;id&quot;: &quot;[variables(&#39;subnetRef&#39;)]&quot; &#125;, &quot;privateIPAllocationMethod&quot;: &quot;Dynamic&quot;, &quot;publicIpAddress&quot;: &#123; &quot;id&quot;: &quot;[resourceId(&#39;Microsoft.Network/publicIpAddresses&#39;, parameters(&#39;publicIpAddressName&#39;))]&quot; &#125; &#125; &#125; ], &quot;networkSecurityGroup&quot;: &#123; &quot;id&quot;: &quot;[resourceId(&#39;Microsoft.Network/networkSecurityGroups&#39;, variables(&#39;networkSecurityGroupName&#39;))]&quot; &#125; &#125; &#125;, &#123; &quot;name&quot;: &quot;[parameters(&#39;publicIpAddressName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Network/publicIpAddresses&quot;, &quot;apiVersion&quot;: &quot;2018-02-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;properties&quot;: &#123; &quot;publicIpAllocationMethod&quot;: &quot;Static&quot; &#125; &#125;, &#123; &quot;name&quot;: &quot;[variables(&#39;networkSecurityGroupName&#39;)]&quot;, &quot;type&quot;: &quot;Microsoft.Network/networkSecurityGroups&quot;, &quot;apiVersion&quot;: &quot;2018-02-01&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;properties&quot;: &#123; &quot;securityRules&quot;: [ &#123; &quot;name&quot;: &quot;RDP&quot;, &quot;properties&quot;: &#123; &quot;priority&quot;: 360, &quot;protocol&quot;: &quot;Tcp&quot;, &quot;access&quot;: &quot;Allow&quot;, &quot;direction&quot;: &quot;Inbound&quot;, &quot;sourceApplicationSecurityGroups&quot;: [], &quot;destinationApplicationSecurityGroups&quot;: [], &quot;sourceAddressPrefix&quot;: &quot;*&quot;, &quot;sourcePortRange&quot;: &quot;*&quot;, &quot;destinationAddressPrefix&quot;: &quot;*&quot;, &quot;destinationPortRange&quot;: &quot;22&quot; &#125; &#125;, &#123; # 给新创建的虚拟机开放所需要的端口 &quot;name&quot;: &quot;Tcp-80&quot;, &quot;properties&quot;: &#123; &quot;priority&quot;: 370, # 此值为端口的优先级，每个端口的值不一样 &quot;protocol&quot;: &quot;Tcp&quot;, &quot;access&quot;: &quot;Allow&quot;, &quot;direction&quot;: &quot;Inbound&quot;, &quot;sourceApplicationSecurityGroups&quot;: [], &quot;destinationApplicationSecurityGroups&quot;: [], &quot;sourceAddressPrefix&quot;: &quot;*&quot;, &quot;sourcePortRange&quot;: &quot;*&quot;, &quot;destinationAddressPrefix&quot;: &quot;*&quot;, &quot;destinationPortRange&quot;: &quot;80&quot; &#125; &#125;, &#123; &quot;name&quot;: &quot;Tcp-443&quot;, &quot;properties&quot;: &#123; &quot;priority&quot;: 380, &quot;protocol&quot;: &quot;Tcp&quot;, &quot;access&quot;: &quot;Allow&quot;, &quot;direction&quot;: &quot;Inbound&quot;, &quot;sourceApplicationSecurityGroups&quot;: [], &quot;destinationApplicationSecurityGroups&quot;: [], &quot;sourceAddressPrefix&quot;: &quot;*&quot;, &quot;sourcePortRange&quot;: &quot;*&quot;, &quot;destinationAddressPrefix&quot;: &quot;*&quot;, &quot;destinationPortRange&quot;: &quot;443&quot; &#125; &#125; ] &#125; &#125; ], &quot;outputs&quot;: &#123; &quot;adminUsername&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;value&quot;: &quot;[parameters(&#39;adminUsername&#39;)]&quot; &#125; &#125; &#125;","categories":[{"name":"Windows","slug":"Windows","permalink":"http://example.com/categories/Windows/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"http://example.com/tags/Azure/"},{"name":"arm","slug":"arm","permalink":"http://example.com/tags/arm/"}]},{"title":"Kubernetes 滚动升级","slug":"kubernetes 滚动升级","date":"2019-03-01T17:47:56.000Z","updated":"2021-01-27T02:25:26.049Z","comments":true,"path":"2019/03/02/kubernetes 滚动升级/","link":"","permalink":"http://example.com/2019/03/02/kubernetes%20%E6%BB%9A%E5%8A%A8%E5%8D%87%E7%BA%A7/","excerpt":"","text":"run test deploy [root@k8s-master ~]# kubectl run --image=nginx --port=80 --replicas=2 test-nginx scale replica [root@k8s-master ~]# kubectl scale --replicas=1 deploy/test-nginx [root@k8s-master ~]# kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE busybox 2 2 2 2 39d busybox1 1 1 1 1 39d test-nginx 1 1 1 1 2h update image [root@k8s-master ~]# kubectl set image deploy/test-nginx test-nginx=nginx:alpine 查看升级历史 [root@k8s-master ~]# kubectl rollout history deploy/test-nginx deployments &quot;test-nginx&quot; REVISION CHANGE-CAUSE 1 &lt;none&gt; 2 &lt;none&gt; 回顾至上次版本 [root@k8s-master ~]# kubectl rollout undo deploy/test-nginx [root@k8s-master ~]# kubectl rollout history deploy/test-nginx deployments &quot;test-nginx&quot; REVISION CHANGE-CAUSE 2 &lt;none&gt; 3 &lt;none&gt; 回滚至指定版本 [root@k8s-master ~]# kubectl rolloutundo deployment/lykops-dpm --to-revision=2","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"K8s容器一直重启问题排查","slug":"k8s容器一直重启问题排查","date":"2019-02-27T19:21:10.000Z","updated":"2021-01-27T02:25:26.048Z","comments":true,"path":"2019/02/28/k8s容器一直重启问题排查/","link":"","permalink":"http://example.com/2019/02/28/k8s%E5%AE%B9%E5%99%A8%E4%B8%80%E7%9B%B4%E9%87%8D%E5%90%AF%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","excerpt":"","text":"问题昨天有个客户反馈服务不可访问，查看服务日志发现dns无法解析，通过kubectl查看kube-dns 这个pod一直在重启，客户只反馈了这一个问题，如图： 过了一会后发现kube-dns灾厄已经恢复，但是kube-flannel这个pod又开始一直重启。根据自己以往的经验还以为是路由出了什么问题，所以在排查路由方面浪费了一些时间， 排查一段时间路由后未发现异常，随后想到查看有问题机器的kubelet日志，如下 describe node的Events如下： 解决通过日志可以发现是磁盘满导致的原因，但在当时看到问题时只是根据经验以为是路由有问题（只能说是学艺不精），但是我还没来得及查看磁盘，就补客户给清理过，而且客户还顺手把机器给重启了，机器启动后一切就恢复正常 排查后来上网搜索相关错误字段，随后阅读相关文章以及查看k8s代码，发现k8s有一个Eviction Manager的机制，如下图： 如果在启动k8s时没有设置Eviction Thresholds（驱逐阈值)，k8s将按代码里（上图）默认的值执行 但是通过可看kubelet日志发现，前一天的时候就已经发现有相关报错，那时候报的是Eviction image 全是错误状态（没有驱逐成功），这一点也很好理解，因为当所有的镜像都在使用（都被容器占用）时，删除镜像是失败的 那为什么到后来就已经开始是pod被驱逐呢？ 后来又从k8s的代码里发现了两个概念 Soft Eviction Thresholds（软驱逐） 和 Hard Eviction Thresholds （硬驱逐）这两个概念可以查看 这里 总结简单一句话总结就是：当软驱逐失败时会强制执行硬驱逐 所以出现问题的时候就是，先软驱逐所有的image失败（因为从阈值看最先达到镜像的设置值），后开始驱逐相关pod，具体驱逐pod的机制在 这里 也有详细描述。 预防到这里问题就已经全部搞清楚，所以为了预防此类事故再次发生，建议将磁盘报警监控设置到80%以下，如果有报警需要及时处理。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"Eviction Thresholds","slug":"Eviction-Thresholds","permalink":"http://example.com/tags/Eviction-Thresholds/"}]},{"title":"Kubernetes Eviction Manager工作机制分析","slug":"Kubernetes Eviction Manager工作机制分析","date":"2019-02-27T06:59:43.000Z","updated":"2021-01-27T02:25:26.038Z","comments":true,"path":"2019/02/27/Kubernetes Eviction Manager工作机制分析/","link":"","permalink":"http://example.com/2019/02/27/Kubernetes%20Eviction%20Manager%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/","excerpt":"","text":"摘要为了极限的压榨资源，很多时候Kubernetes集群会运行一些Best-Effort Task，这样就会存在资源超配的情况，Kubernetes是如何控制Node上资源的使用，在压榨资源使用的同时又能保证Node的稳定性？本文就为你介绍其背后运行机制。我的下一篇博文，会对Kubelet Eviction Manager进行源码分析，感兴趣的同学可以关注。 研究过Kubernetes Resource QoS的同学，肯定会有一个疑问：QoS中会通过Pod QoS和OOM Killer进行资源的回收，当发生资源紧缺的时候。那为什么Kubernetes会再搞一个Kubelet Eviction机制，来做几乎同样的事呢？ 首先，我们来谈一下kubelet通过OOM Killer来回收资源的缺点： System OOM events本来就是对资源敏感的，它会stall这个Node直到完成了OOM Killing Process。 当OOM Killer干掉某些containers之后，kubernetes Scheduler可能很快又会调度一个新的Pod到该Node上或者container 直接在node上restart，马上又会触发该Node上的OOM Killer启动OOM Killing Process，事情可能会没完没了的进行，这可不妙啊。 我们再来看看Kubelet Eviction有何不同： Kubelet通过pro-actively监控并阻止Node上资源的耗尽，一旦触发Eviction Signals，就会直接Fail一个或者多个Pod以回收资源，而不是通过Linux OOM Killer这样本身耗资源的组件进行回收。 这样的Eviction Signals的可配置的，可以做到Pro-actively。 另外，被Evicted Pods会在其他Node上重新调度，而不会再次触发本Node上的再次Eviction。 下面，我们具体来研究一下Kubelet Eviction Policy的工作机制。 kubelet预先监控本节点的资源使用，并且阻止资源被耗尽，这样保证node的稳定性。 kubelet会预先Fail N(&gt;= 1)个Pod以回收出现紧缺的资源。 kubelet会Fail一个Node时，会将Pod内所有Containners都kill掉，并把PodPhase设为Failed。 kubelet通过事先人为设定Eviction Thresholds来触发Eviction动作以回收资源。Eviction Signals支持如下Eviction Signals: Eviction Signal Description memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet nodefs.available nodefs.available := node.stats.fs.available nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree imagefs.available imagefs.available := node.stats.runtime.imagefs.available imagefs.inodesFree imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree kubelet目前支持一下两种filesystem，其中imagefs为可选的。Kubelet通过cAdvisor来自动发现这些filesystem。 nodefs - Kubelet用来存储volume，logs等数据。 imagefs - 容器运行时(dockerd/rkt等)用来存放镜像和容器的Writable Layer。 Eviction Thresholds前面也提到，kubelet通过事先人为设定Eviction Thresholds来触发Eviction动作以回收资源。 Eviction Thresholds的形式为： quantity支持绝对值和相对百分比两种形式，比如： memory.available&lt;10% memory.available&lt;1Gi Soft Eviction ThresholdsSoft Eviction Thresholds是什么意思？它指的是，当Eviction Signal中值达到Soft Eviction Thresholds配置的值时，并不会马上触发Kubelet去Evict Pods，而是会等待一个用户配置的grace period之后，再触发。相关的配置有三个，如下： eviction-soft - (e.g. memory.available&lt;1.5Gi) 触发Soft Eviction的Evication Signal阈值。 eviction-soft-grace-period - (e.g. memory.available=1m30s) 当Eviction Signal的值达到配置eviction-soft值后，需要等待grace period，注意这期间，每10s会重新获取监控数据并维护Threshold的值。如果grace period最后一次监控数据仍然触发了阈值，才会再触发Evict Pods。这个参数就是配置这个grace period的。 eviction-max-pod-grace-period - (e.g. memory.available=30s) 这个是配置Evict Pods时，Pod Termination的Max Grace Period。如果待Evict的Pod指定了pod.Spec.TerminationGracePeriodSeconds，则取min(eviction-max-pod-grace-period, pod.Spec.TerminationGracePeriodSeconds)作为Pod Termination真正的Grace Period。 因此，从kubelet监控到的Eviction Signal达到指定的Soft Eviction Thresholds开始，到Pod真正被Kill，总共所需要的时间为：sum(eviction-soft-grace-period + min(eviction-max-pod-grace-period,pod.Spec.TerminationGracePeriodSeconds)) Hard Eviction Thresholds理解了Soft Eviction Thresholds,那么Hard Eviction Thresholds就很简单了，它是指：当Eviction Signal中值达到Hard Eviction Thresholds配置的值时，会立刻触发Kubelet去Evict Pods，并且也不会有Pod Termination Grace Period，而是立刻kill Pods，即使待Evict的Pod指定了pod.Spec.TerminationGracePeriodSeconds。 总之，Hard Eviction Thresholds就是来硬的，一旦触发，kubelet立刻马上kill相关的pods。 因此，kubelet关于Hard Eviction Thresholds的配置也只有一个： eviction-hard - (e.g. memory.available&lt;1Gi) 这个值，要设置的比eviction-soft更低才有意义。 Eviction Monitoring Intervalkubelet会通过监控Eviction Signal的值，当达到配置的阈值时，就会触发Evict Pods。 kubelet对应的监控周期，就通过cAdvisor的housekeeping-interval配置的，默认10s。 Node Conditions当Hard Eviction Thresholds或Soft Eviction Thresholds被触及后，Kubelet会将对应的Eviction Signals映射到对应的Node Conditions，其映射关系如下： Node Condition Eviction Signal Description MemoryPressure memory.available Available memory on the node has satisfied an eviction threshold DiskPressure nodefs.available, nodefs.inodesFree, imagefs.available, or imagefs.inodesFree Available disk space and inodes on either the node’s root filesystem or image filesystem has satisfied an eviction threshold kubelet映射了Node Condition之后，会继续按照–node-status-update-frequency(default 10s)配置的时间间隔，周期性的与kube-apiserver进行node status updates。 Oscillation of node conditions想象一下，如果一个Node上监控到的Soft Eviction Signals的值，一直在eviction-soft水平线上下波动，那么Kubelet就会将该Node对应的Node Condition在true和false频繁切换。这可不是什么好事，它可能会带来kube-scheduler做出错误的调度决定。kubelet是怎么处理这种情况的呢？ 很简单，Kubelet通过添加参数eviction-pressure-transition-period(default 5m0s)配置，使Kubelet在解除由Evicion Signal映射的Node Pressure之前，必须等待这么长的时间。 因此，逻辑就变成这样了： Soft Evction Singal高于Soft Eviction Thresholds时，Kubelet还是会立刻设置对应的MemoryPressure Or DiskPressure为True。 当MemoryPressure Or DiskPressure为True的前提下，发生了Soft Evction Singal低于Soft Eviction Thresholds的情况，则需要等待eviction-pressure-transition-period(default 5m0s)配置的这么长时间，才会将condition pressure切换回False。 一句话总结：Node Condition Pressure成为True容易，切换回False则要等eviction-pressure-transition-period。 Eviction of PodsKubelet的Eviction流程概括如下： 在每一个监控周期内，如果Eviction Thresholds被触及，则： 获取候选Pod Fail the Pod 等待该Pod被Terminated 如果该Pod由于种种原因没有被成功Terminated，Kubelet将会再选一个Pod进行Fail Operation。其中，Fail Pod的时候，Kubelet是通过调用容器运行时的KillPod接口，如果接口返回True，则认为Fail Pod成功，否则视为失败。Eviction Strategykubelet根据Pod的QoS Class实现了一套默认的Evication策略，内容见我的另外一篇博文Kubernetes Resource QoS机制解读中介绍的“如何根据不同的QoS回收Resource”,这里不再赘述。 下面给出Eviction Strategy的图解： Minimum eviction reclaim有些情况下，eviction pods可能只能回收一小部分的资源就能使得Evication Signal的值低于Thresholds。但是，可能随着资源使用的波动或者新的调度Pod使得在该Node上很快又会触发evict pods的动作，eviction毕竟是耗时的动作，所以应该尽量避免这种情况的发生。 Kubelet是通过–eviction-minimum-reclaim(e.g. memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi)来控制每次Evict Pods后，Node上对应的Resource不仅要比Eviction Thresholds低，还要保证最少比Eviction Thresholds低–eviction-minimum-reclaim中配置的数量。 Node OOM Behavior正常情况下，但Node上资源利用率很高时，Node的资源回收是会通过Kubelet Eviction触发完成。但是存在这么一种情况，Kubelet配置的Soft/Hard memory.available还没触发，却先触发了Node上linux kernel oom_killer，这时回收内存资源的请求就被kernel oom_killer处理了，而不会经过Kubelet Eviction去完成。 我的博文Kubernetes Resource QoS机制解读中介绍过，Kubelet根据Pod QoS给每个container都设置了oom_score_adj，整理如下： Quality of Service oom_score_adj Guaranteed -998 BestEffort 1000 Burstable min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) oom_killer根据container使用的内存占Node总内存的百分比计算得到该container的oom_score，然后再将该oom_sore和前面对应的oom_score_adj相加作为最终的oom_score，Node上所有containers的最终oom_score进行排名，将oom_score得分最高的container kill掉。通过如此方式进行资源回收。 oom_killer这样做的目标就是干掉QoS低的又消耗最多内存(相对request值)的容器首先被kill掉，如此回收内存。 不同于Kubelet Evict Pods的是，Node OOM Behavior存在一个缺点：如果Pod中某个容器被oom_killer干掉之后，会根据该容器的RestartPolicy决定是否restart这个容器。如果这是个有问题的容器，restart之后，可能又很快消耗大量内存进而触发了再次Node OOM Behavior，如此循环反复，该Node没有真正的回收到内存资源。 Scheduler前面提到，Kubelet会定期的将Node Condition传给kube-apiserver并存于etcd。kube-scheduler watch到Node Condition Pressure之后，会根据以下策略，阻止更多Pods Bind到该Node。 Node Condition Scheduler Behavior MemoryPressure No new BestEffort pods are scheduled to the node. DiskPressure No new pods are scheduled to the node. 总结 Kubelet通过Eviction Signal来记录监控到的Node节点使用情况。 Eviction Signal支持：memory.available, nodefs.available, nodefs.inodesFree, imagefs.available, imagefs.inodesFree。 通过设置Hard Eviction Thresholds和Soft Eviction Thresholds相关参数来触发Kubelet进行Evict Pods的操作。 Evict Pods的时候根据Pod QoS和资源使用情况挑选Pods进行Kill。 Kubelet通过eviction-pressure-transition-period防止Node Condition来回切换引起scheduler做出错误的调度决定。 Kubelet通过–eviction-minimum-reclaim来保证每次进行资源回收后，Node的最少可用资源，以避免频繁被触发Evict Pods操作。 当Node Condition为MemoryPressure时，Scheduler不会调度新的QoS Class为BestEffort的Pods到该Node。 当Node Condition为DiskPressure时，Scheduler不会调度任何新的Pods到该Node。 作者：WaltonWang来源：CSDN原文：https://blog.csdn.net/WaltonWang/article/details/55804309版权声明：本文为博主原创文章，转载请附上博文链接！","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"Kubernetes的使用技巧","slug":"Kubernetes的使用技巧","date":"2019-02-27T06:51:39.000Z","updated":"2021-01-27T02:25:26.039Z","comments":true,"path":"2019/02/27/Kubernetes的使用技巧/","link":"","permalink":"http://example.com/2019/02/27/Kubernetes%E7%9A%84%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","excerpt":"","text":"1. bash针对kubectl命令的自动补充这可能是在使用Kubernetes过程中最容易做的事，但它也是其中一个最有用的。要添加自动补充功能，如果使用bash，只需执行以下命令： echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc 它将添加自动补全命令到你的.bashrc文件。因此每个你打开的shell窗口都支持该功能。我发现自动补全对一些长的参数，比如–all-namespaces特别有用。 2. 给每个namespace添加默认的内存和CPU限额是人就会犯错。我们假定某人写了个应用，他每秒就会打开一个数据库连接，但是不会关闭。这样集群中就有了一个内存泄漏的应用。假定我们把该应用部署到了没有限额设置的集群，那么该应用就会crash掉一个节点。为了避免这种情况，Kubernetes允许为每个namespace设置默认的限额。要做到这很简单，我们只需创建一个limit range 的 yaml 并应用到特定namespace。以下是一个例子： apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container 将该内容创建一个yaml文件并将它应用到任何你想应用的namespace，例如namespace limit-example。使用了限额后，任何部署到该namespace的应用，假如没有主动设置限额，都将得到一个默认的512Mi的内存限额。 3. kebelet可以帮我清理掉Docker镜像吗这是kubelet默认已实现的功能。如果kubelet启动时没有设置flag，当/var/lib/docker目录到达90%的容量时，它就会自动进行垃圾回收。这是极好的，但是针对inode阈值它没有默认设置（Kubernetes 1.7之前）。你可能会遇到/var/lib/docker只使用了50%磁盘空间，但是inode全部用光的情况。这可能会引起工作节点各种各样的问题。如果你运行的kubelet版本在1.4到1.6之间，那你得给kubelet添加以下flag： --eviction-hard =memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% 如果kubelet版本是1.7或更高，它默认就有这个配置。1.6默认不会监控inode的使用率，所以得添加那个flag来解决这个问题。 4. minikube虽然是mini，但是本地使用功能强大minikube绝对是本地启动Kubernetes集群最容易的方式。你只需遵循这个[1]指南去下载所有东西。一旦所有组件安装完毕，你只需运行如下命令： minikube start 待命令执行完毕，你本地就有一个运行的Kubernetes集群了。当你想在本地构建一个应用并在本地运行时，有一个技巧。当你在本地构建一个Docker镜像时，如果不运行其它命令，你的镜像将被构建在你的本地计算机。为了使你构建的Docker镜像能够直接push到本地Kubernetes集群，你需要使用如下命令告知Docker机器： eval $(minikube docker-env) 这将使你能直接推送本地构建的应用到你的本地集群。 5. 不要将kubectl的权限开放给所有人这可能是一个明摆着的事，但是当多个团队部署应用到同一个集群时，而这种场景就是Kubernetes的目标，不要开放一个通用的kubectl给每个人。我的建议是基于namespace来隔离团队，然后使用RBAC策略来限制能且仅能访问那个namespace。 在权限被控制之后，你可能会变得疯狂，比如只能基于Pod来读取，创建和删除Pod。但是其中一个最需要做的事是只能访问管理员凭证，这样可以隔离谁能管理集群，而谁只能在集群上部署应用。 这个话题我期待着后续单独开一篇博客来进行更详细的分析。 6. Pod中断预算（Pod Disruption Budgets）是你的朋友在Kubernetes集群中我们如何确保应用零宕机？ PodDisruptionBudgetPodDisruptionBudgetPodDisruptionBudget 集群会更新。节点会打上drain标签且Pod会被移除，这没法避免。所以我们应该针对每个deployment都设置一个PDB，保证至少有一个实例。我们可以使用一个简单的yaml来创建一个PDB，应用到集群里，并使用标签选择器来确定这个PDB覆盖了哪些资源。 注意：PDB只对自愿中断的资源负责，某些如硬件失败这种错误，PDB无法起作用。 PDB例子如下： apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: app-a-pdb spec: minAvailable: 2 selector: matchLabels: app: app-a 两个最需要关注的字段是matchLabels和minAvailable。 matchLabels字段用来确定是否一个deployment可以关联到这个PDB。 例如，如果我有一个带标签app:app-a的deployment，和另一个带标签app:app-b的deployment，例子中的PDB将只对第一个deployment起作用。 minAvailable字段是Kubernetes在某些场景下，比如node被打上drain标签时，进行操作的依据。假设app-a运行在node1上，如果node1被打上了drain标签，那么kubernetes只会清除那些有至少2个实例的app-a。 这允许你在任何时候都可控制运行的实例数。 7. 你的APP还活着且可用吗Kubernetes允许我们定义探针，供kubelet确认我们的Pod和APP是否是健康的。 Kubernetes提供了两种类型的探针，Readiness探针和Liveness探针。 Readiness探针用来确认容器是否可接受流量。 Liveness探针用来确认容器是否是健康的，或者需要被重启。 这些配置可以很容易得追加到deployment的yaml，并且可以自定义超时时间，重试次数，延时时间等。需要更进一步得了解如何使用它们的，请阅读此文[2]。 8. 给所有事物都打上标签标签是Kubernetes的其中一个基石。它使得对象和对象之间保持松耦合，且允许我们根据标签来查询对象。你甚至可以使用go client根据标签来监控事件。 你几乎可以用标签做任何事，其中一个极佳的例子是同一集群中的多个环境。 我们假定你在dev和qa环境使用了相同的集群。这说明你将在dev和qa环境同时运行一个app-a应用。 为了达到这个目的，最简单的方式是使用service对象，其中一个选择带标签app:app-a和environment:dev的Pod，而另一个，则选择带标签app:app-a和environment:qa的Pod。 这样做的好处是，两个相同的APP，每一个有不同的endpoint，这样就支持同时测试。 9. 主动清理Kubernetes是一个非常非常强大的系统，但是和其它系统一样，它最终也会陷入混乱。kubelet必须进行任何你告诉它的校验，同时它也进行自己的校验。 当然，Kubernetes有一个服务无法连接了，系统是不会挂掉的，因为它支持扩缩容。但是一个服务一旦扩大到成千上万个endpoint，那么kubelet就会一下子陷入瘫痪。 简单的说，不论你因为什么理由需要删除一个deployment（或者其它东西），你都必须确保清理干净和它相关的一切东西。 10. 你热爱GO语言吗最后一点是我个人觉得最重要的：持续的学习GO语言。 Kubernetes是由GO编写的，它的所有插件也是用GO写的，他们甚至还编写了一个GO语言的客户端。client-go可用来做各种有趣的事。你可以用它根据自己的爱好扩展kubernetes。比如数据收集，部署引擎，或者一个简单的清理应用。 学习这个GO 客户端，并在Kubernetes中使用它，这是我给每个使用Kubernetes的用户的最大建议。 原文链接： https://kubernetes.io/docs/tasks/tools/install-minikube/ https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes 原文链接：https://hackernoon.com/top-10-kubernetes-tips-and-tricks-27528c2d0222","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"K8S的 QoS策略","slug":"K8S的 QoS策略","date":"2019-02-27T05:44:12.000Z","updated":"2021-01-27T02:25:26.037Z","comments":true,"path":"2019/02/27/K8S的 QoS策略/","link":"","permalink":"http://example.com/2019/02/27/K8S%E7%9A%84%20QoS%E7%AD%96%E7%95%A5/","excerpt":"","text":"QoSk8s中对容器的资源分配有三种策略： Guaranteed 。该策略下，pod.spec.containers[].resources中会存在cpu或memory的request和limit。顾名思义是该容器对资源的最低要求和最高使用量限制。如果我们配置了limit，没有配置request，默认会以limit的值来定义request。具体的配置可以参考以前的这篇笔记。 BestEffort。当pod的描述文件中没有resource.limit、resource.request相关的配置时，意味着这个容器想跑多少资源就跑多少资源，其资源使用上限实际上即所在node的capacity。 Burstable。当resource.limit和resource.request以上述两种方式以外的形式配置的时候，就会采用本模式。QoS目前只用cpu和memory来描述，其中cpu可压缩资源，当一个容器的cpu使用率超过limit时会被进行流控，而当内存超过limit时则会被oom_kill。这里kubelet是通过自己计算容器的oom_score，确认相应的linux进程的oom_adj，oom_adj最高的进程最先被oom_kill。Guaranteed模式的容器oom_score最小：-998，对应的oom_adj为0或1，BestEffort模式则是1000，Burstable模式的oom_score随着其内存使用状况浮动，但会处在2-1000之间。 因此我们可以看出，当某个node内存被严重消耗时，BestEffort策略的pod会最先被kubelet杀死，其次Burstable（该策略的pods如有多个，也是按照内存使用率来由高到低地终止），再其次Guaranteed。 kubelet的eviction机制完全依赖于oom_kill并不是一个很好的方案，一来对于cpu要求高的容器没有作用，二来单纯将pod杀死，并不能根本上解决困局，比如pod占用node绝大部分内存，加入pod被kill后再次调度到这个node上，oom的情况还会复现。所以kubelet增加了一套驱逐机制。eviction机制适用于： memory.available nodefs.available nodefs.inodesFree imagefs.available imagefs.inodesFree 分别对应于node目前可用内存、node上用于kubelet运行日志、容器挂载磁盘所使用的的文件系统的余量和inode余量、node上用于存放容器镜像和读写层的文件系统的余量、inode余量。 eviction中要设置触发驱逐的阈值Eviction Thresholds，这个阈值的配置可以是一个定值或一个百分比。如： memory.available&lt;10% memory.available&lt;1Gi Soft Eviction Thresholds软驱逐机制表示，当node的内存/磁盘空间达到一定的阈值后，我要观察一段时间，如果改善到低于阈值就不进行驱逐，若这段时间一直高于阈值就进行驱逐。 这里阈值通过参数–eviction-soft配置，样例如上；观察时间通过参数–eviction-soft-grace-period进行配置，如1m30s。 另外还有一个参数eviction-max-pod-grace-period，该参数会影响到要被驱逐的pod的termination time，即终止该pod的容器要花费的时间。 Hard Eviction Thresholds强制驱逐机制则简单的多，一旦达到阈值，立刻把pod从本地kill，驱逐eviction-hard参数配置，样例亦如上。 pod eviction当资源使用情况触发了驱逐条件时，kubelet会启动一个任务去轮流停止运行中的pod，直到资源使用状况恢复到阈值以下。以硬驱逐为例，整体流程是： 每隔一段时间从cadvisor中获取资源使用情况，发现触发了阈值； 从运行中的pod里找到QoS策略最开放的一个，比如策略为bestEffort的一个pod（即便这个pod没有吃多少内存，大部分内存是另一个策略为burstable，但内存使用率也很高的pod），kubelet停止该pod对应的所有容器，然后将pod状态更新为Failed。如果该pod长时间没有被成功kill掉，kubelet会再找一个pod进行驱逐。 检查内存用量是否恢复到阈值以下，如果没有，则重复第二步（这里就要干掉那个罪魁祸首了）。一直到内存使用情况恢复到阈值以下为止。 有几个要注意的点是： kubelet挑选pod进行驱逐的策略，就是按照QoS的策略开放度排序，而同一个QoS的多个pod中，kubelet会优先驱逐使用触发指标资源最多的一个。 磁盘的使用不像memory有通过request和limit进行配置，磁盘用量可以认为是一种QoS策略为BestEffort的资源。当触发磁盘资源不足时，kubelet会做一些额外的工作，比如清理已经dead的pod的容器日志，清理没有被使用的容器镜像，当然kubelet也会挑磁盘使用量（包括挂载本地volume空间+容器log大小,若是imagefs指标超额，此处还要加上容器运行时读写层的文件大小）最大的一个pod进行驱逐。 node condition Node Condition Eviction Signal Description MemoryPressure memory.available Available memory on the node has satisfied an eviction threshold DiskPressure nodefs.available, nodefs.inodesFree, imagefs.available, or imagefs.inodesFree Available disk space and inodes on either the node’s root filesystem or image filesystem has satisfied an eviction thresh 如上图，当软驱逐或者硬驱逐触发时，kubelet会尝试干掉一个pod，并且会将自身的状态从驱逐的指标信息中映射过来，比如内存使用超标触发驱逐，node的condtion就会变成memoryPressure，这个condition伴随的kubelet定时的心跳报文上传到master，记录在etcd中。在调度器进行调度时，会以这些condition作为调度条件的参考。比如，处于diskPressure的node，调度器就不会再将任何pod调度上去。否则一旦磁盘空间用满，node上的容器可能会严重崩溃。 但如果node的内存在阈值上下波动，condition被反复更新为pressure或正常，那么pod被误调度到node上也会很耽误事，所以用eviction-pressure-transition-period参数指定触发eviction后condition更新一次后要保留改状态的最小时长。在这个时长范围内即便资源使用下降到阈值以下，condition也不会恢复。 其他Minimum eviction reclaim 我们担心node可能驱逐了一个小pod后，指标就只是稍低于阈值，那么一旦其他pod的指标稍一上来，该node就又要进行eviction。所以用这个参数：–eviction-minimum-reclaim(值如”memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi”)进行限定，一旦发生了eviction，必须要保证node的某指标用量低于（该指标阈值-本参数指定的该指标值）才认为node恢复正常，否则还要接着驱逐pod。简单的说，该参数表示的是node进行驱逐工作后要达到的效果是低于阈值多少。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"K8s的一次路由故障","slug":"k8s的一次路由故障","date":"2019-02-26T22:21:47.000Z","updated":"2021-01-27T02:25:26.048Z","comments":true,"path":"2019/02/27/k8s的一次路由故障/","link":"","permalink":"http://example.com/2019/02/27/k8s%E7%9A%84%E4%B8%80%E6%AC%A1%E8%B7%AF%E7%94%B1%E6%95%85%E9%9A%9C/","excerpt":"","text":"问题今天有个客户三台master上有两台的kube-dns一直起不来，如图： 排查 查看pod的日志： 看到报错后首先想到的就是rount有问题，然后通过查看路由发现，有问题的两台机器多了一条路由： 通过以下命令查看本机应该对应的flannel的路由： [root@node-dev-3 ~]# cat /run/flannel/subnet.env FLANNEL_NETWORK=10.5.0.0/16 FLANNEL_SUBNET=10.5.2.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true 解决方法 删除多余的路由规则： [root@node-dev-3 ~]# route del -net 10.5.2.0 gw 172.26.7.12 netmask 255.255.255.0 删除之前的旧dns pod [root@node-dev-3 ~]# kubectl delete pod -n kube-system kube-dns-xxx 最后查看dns pod 已经正常运行","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"route","slug":"route","permalink":"http://example.com/tags/route/"}]},{"title":"Hung_task_timeout_secs参数","slug":"hung_task_timeout_secs参数","date":"2019-02-26T17:11:36.000Z","updated":"2021-01-27T02:25:26.045Z","comments":true,"path":"2019/02/27/hung_task_timeout_secs参数/","link":"","permalink":"http://example.com/2019/02/27/hung_task_timeout_secs%E5%8F%82%E6%95%B0/","excerpt":"","text":"问题客户有一台服务器，安装了VMW软件做了虚拟化，在其上搭建了一台readhat虚拟机，起初给的内存为16G，在添加了12G的内存后，将虚拟机的内存调整到了20G调整完后主机这边就一直报错： Nov 5 13:05:41 RedHat5 kernel: INFO: task oracle:22439 blocked for more than 120 seconds. Nov 5 13:05:41 RedHat5 kernel: “echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs” disables this message. 解决方法从以上的报错信息也给出了简单的解决方案，就是禁止该120秒的超时： echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs 原因查询了资料后对于该参数的了解为后台对进行的任务由于超时而挂起 随后询问了主机工程师：给出方案是按照告警里的提示将该提醒disable后续询问后给出如下解释： This is a know bug. By default Linux uses up to 40% of the available memory for file system caching.After this mark has been reached the file system flushes all outstanding data to disk causing all following IOs going synchronous.For flushing out this data to disk this there is a time limit of 120 seconds by default.In the case here the IO subsystem is not fast enough to flush the data withing 120 seconds.This especially happens on systems with a lof of memory.The problem is solved in later kernels and there is not “fix” from Oracle.I fixed this by lowering the mark for flushing the cache from 40% to 10% by setting “vm.dirty_ratio=10″ in /etc/sysctl.conf.This setting does not influence overall database performance since you hopefully use Direct IO and bypass the file system cache completely. 告知是linux会设置40%的可用内存用来做系统cache，当flush数据时这40%内存中的数据由于和IO同步问题导致超时（120s），所将40%减小到10%，避免超时。 报错截图以下为报错页面","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"hung_task_timeout_secs","slug":"hung-task-timeout-secs","permalink":"http://example.com/tags/hung-task-timeout-secs/"}]},{"title":"K8S的pod内获取变量的值错误","slug":"Get the value of a variable error within the pod K8S","date":"2019-02-26T03:43:02.000Z","updated":"2021-01-27T02:25:26.036Z","comments":true,"path":"2019/02/26/Get the value of a variable error within the pod K8S/","link":"","permalink":"http://example.com/2019/02/26/Get%20the%20value%20of%20a%20variable%20error%20within%20the%20pod%20K8S/","excerpt":"","text":"问题今天遇到一个问题，就是用shell脚本从k8s的pod内获取变量，然后再进行赋值或字符拼接时，结果发现总是不对，但是通过自定义的变量没有问题以下为问题复现： [root@cloud-cn-master-1 ~]# cc=`kubectl exec -it -n nginx-system nginx-df4b995c5-98w66 -c nginx-01 env | grep SYS_ADMIN_USERNAME | awk -F&quot;=&quot; &#39;&#123;print $2&#125;&#39;` [root@cloud-cn-master-1 ~]# echo $cc sysadmin [root@cloud-cn-master-1 ~]# echo $cc&quot;abcde&quot; :abcdein [root@cloud-cn-master-1 ~]# [root@cloud-cn-master-1 ~]# dd=test [root@cloud-cn-master-1 ~]# echo $dd test [root@cloud-cn-master-1 ~]# echo $dd&quot;:abcde&quot; test:abcde [root@cloud-cn-master-1 ~]# 经过反复排查，结果发现取出的值的文件编码为doc，通过vim 文件即可发现： [root@cloud-cn-master-1 ~]# kubectl exec -it -n nginx-system nginx-df4b995c5-98w66 -c nginx-01 env | grep SYS_ADMIN_USERNAME | awk -F&quot;=&quot; &#39;&#123;print $2&#125;&#39;&gt;cc.txt [root@cloud-cn-master-1 ~]# vim cc.txt 解决方法使用字符转换命令（dos2unix）对此文件进行转换即可： [root@cloud-cn-master-1 ~]# dos2unix cc.txt dos2unix: converting file cc.txt to Unix format ... [root@cloud-cn-master-1 ~]# cc=`cat cc.txt` [root@cloud-cn-master-1 ~]# echo $cc&quot;:abcde&quot; sysadmin:abcde [root@cloud-cn-master-1 ~]#","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"}]},{"title":"DNS介绍","slug":"DNS介绍","date":"2019-02-25T05:24:22.000Z","updated":"2021-01-27T02:25:26.035Z","comments":true,"path":"2019/02/25/DNS介绍/","link":"","permalink":"http://example.com/2019/02/25/DNS%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"DNS解析全过程 客户机提出域名解析请求,并将该请求发送给本地的域名服务器。 当本地的域名服务器收到请求后,就先查询本地的缓存,如果有该纪录项,则本地的域名服务器就直接把查询的结果返回。 如果本地的缓存中没有该纪录,则本地域名服务器就直接把请求发给根域名服务器,然后根域名服务器再返回给本地域名服务器一个所查询域(根的子域)的主域名服务器的地址。 本地服务器再向上一步返回的域名服务器发送请求,然后接受请求的服务器查询自己的缓存,如果没有该纪录,则返回相关的下级的域名服务器的地址。 重复第四步,直到找到正确的纪录。 本地域名服务器把返回的结果保存到缓存,以备下一次使用,同时还将结果返回给客户机。 1、什么是域名解析？域名解析就是国际域名或者国内域名以及中文域名等域名申请后做的到IP地址的转换过程。IP地址是网路上标识您站点的数字地址，为了简单好记，采用域名来代替ip地址标识站点地址。域名的解析工作由DNS服务器完成。 2、什么是A记录？A (Address) 记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。 3、什么是MX记录？邮件路由记录，用户可以将该域名下的邮件服务器指向到自己的mail server上，然后即可自行操控所有的邮箱设置。您只需在线填写您服务器的IP地址，即可将您域名下的邮件全部转到您自己设定相应的邮件服务器上。 4、什么是CNAME记录？即：别名记录。这种记录允许您将多个名字映射到同一台计算机。 通常用于同时提供WWW和MAIL服务的计算机。 例如，有一台计算机名为“host.mydomain.com”（A记录）。 它同时提供WWW和MAIL服务，为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。 这两个别名的全称就http://www.mydomain.com/和“mail.mydomain.com”。实际上他们都指向“host.mydomain.com”。 5、什么是TTL值？TTL值全称是“生存时间（Time To Live)”，简单的说它表示DNS记录在DNS服务器上缓存时间。要理解TTL值，请先看下面的一个例子： 假设，有这样一个域名myhost.abc.com（其实，这就是一条DNS记录，通常表示在abc.com域中有一台名为myhost的主机）对应IP地址为1.1.1.1，它的TTL为10分钟。这个域名或称这条记录存储在一台名为dns.abc.com的DNS服务器上。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"DNS","slug":"DNS","permalink":"http://example.com/tags/DNS/"}]},{"title":"Linux:配置iptables使内网虚拟机访问公网","slug":"配置iptables使内网虚拟机访问公网","date":"2019-02-25T05:00:28.000Z","updated":"2021-01-27T02:25:26.054Z","comments":true,"path":"2019/02/25/配置iptables使内网虚拟机访问公网/","link":"","permalink":"http://example.com/2019/02/25/%E9%85%8D%E7%BD%AEiptables%E4%BD%BF%E5%86%85%E7%BD%91%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%AE%BF%E9%97%AE%E5%85%AC%E7%BD%91/","excerpt":"","text":"问题当公网ip地址不够用的时候，或者出于安全目的，虚拟机使用纯内网ip，然后通过nat转发出去，是一个典型的虚拟化网络环境。 解决方法 打开宿主机的内核转发： echo 1 &gt; /proc/sys/net/ipv4/ip_forward 配置nat iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o br0 -j SNAT --to 外网ip 然后虚拟机网关配置宿主机内网ip，就可以上网了。 注意点： 1 宿主机虚拟机之间的流量是走的forward的链，宿主机的forward链必须全部打开。 2 如果希望外面能访问虚拟机上的应用，可以配置端口映射 比如将宿主机的10080映射到虚拟机192.168.1.8的80端口，命令如下： iptables -t nat -A PREROUTING -d 外网ip -p tcp --10080 -j DNAT --to 192.168.1.8:80","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"iptables","slug":"iptables","permalink":"http://example.com/tags/iptables/"}]},{"title":"明朝皇帝列表","slug":"明朝皇帝列表","date":"2019-02-24T06:59:20.000Z","updated":"2021-01-27T02:25:26.055Z","comments":true,"path":"2019/02/24/明朝皇帝列表/","link":"","permalink":"http://example.com/2019/02/24/%E6%98%8E%E6%9C%9D%E7%9A%87%E5%B8%9D%E5%88%97%E8%A1%A8/","excerpt":"","text":"明朝皇帝列表太祖朱元璋(1328-1398) 年号“洪武”1368年称帝，在位31年 太祖朱元璋，安徽凤阳人，公元1368年8月，由朱元璋领导的起义军在大将徐达、常遇春等人的指挥下一举攻陷元大都–北京，宣告了元朝的灭亡。同年，朱元璋在建康(今南京)称帝，改元洪武，建立了明王朝，朱元璋是为明太祖。他死于1398年，时年71岁。 惠帝朱允炆(1377-1402) 年号“建文”1398年即位，在位5年 惠帝，太祖孙，皇太子朱标的次子，朱元璋在世时，大封王室，20多个儿子均封为亲王，分驻各地，惠帝登基后，进行削藩，以统一军事，惹恼诸王，燕王朱棣其兵北京，1402年攻入南京，惠帝一说被烧死，一说外逃。 成祖朱棣(1360-1424) 年号“永乐”1402年即位，在位23年 成祖朱棣，朱元璋的第四子，朱棣以入京除奸为名，发动了“靖难之役“。经过四年的战争，在1420年打败惠帝统治集团，夺取了明朝政权，建元永乐，是为明成祖。 1421年迁都北京，并下令编纂了《永乐大典》。死于1424年，时年65岁。 仁宗朱高炽(1378-1425) 年号“洪熙”1424年即位，在位1年仁宗 明成祖长子，仁宗政治比较清明，采取一些缓和社会矛盾的措施。在位1年，死于1425年，时年48岁。 宣宗朱瞻基(1398-1435) 年号“宣德”1425年即位，在位11年 宣宗，仁宗长子，他和其父一样，比较能倾听臣下的意见与仁宗并称“仁宣之治”，宣宗时君臣关系融洽，经济也稳步发展。宣宗死于1435年，时年38岁。 英宗朱祁镇(1427-1464) 年号“正统”“天顺”(1435-1449；1457-1464)在位，在位23年 英宗，宣宗长子，即位时才9岁，被宦官王振专权，1449年，瓦喇大举南侵，王振惬英宗亲征，英宗被俘，史称“土木之变”，1450年，被放回，直到1457年，才又即位。死于1464年，时年38岁。 景帝朱祁钰(1428-1457) 年号“景泰”1449年即位，在位9年 宣宗次子，英宗被俘后被拥立为帝，即位后，用于谦为兵部尚书，粉碎了瓦喇对北京的进攻，迫使瓦喇放回英宗，英宗放回后，景帝将其软禁，直到1457年，景帝病危时，英宗才又被拥为帝。景帝死于1457年，时年30岁。 宪宗朱见深(1447-1487) 年号“成化”1464年即位，在位24年 宪宗，英宗长子，宪宗好方术，溺于女色，致使宦官“奸欺国政”。1487年，宪宗死，时年41岁。 孝宗朱佑樘(1470-1505) 年号“弘治”1487年即位，在位19年 孝宗，宪宗三子。孝宗“更新庶政，言路大开”，使英宗朝以来奸佞当道的局面，得以改观。被誉为“中兴之令主”。死于1505年，时年36岁。 武宗朱厚照(1491-1521) 年号“正德”1505年即位，在位17年 武宗，孝宗长子。好逸乐，贪女色，是明朝有名的荒唐皇帝，因荒淫过度，死于1521年，时年31岁。 世宗朱厚璁(1507-1566) 年号“嘉靖”1521年即位，在位46年 世宗，宪宗孙，父兴献王。世宗前期颇有一些作为，诛杀宦官，节用宽民，但后期荒淫无度，政治腐败，多次爆发农民起义世宗因服丹药中毒死，时年60岁。 穆宗朱载垕(1537-1572) 年号“隆庆”1566年即位，在位7年 穆宗，世宗第三子。在位7年，死于1572年，时年36岁。 神宗朱翊钧(1563-1620) 年号“万历”1572年即位，在位48年 神宗，穆宗第三子。即位时才10岁，由皇太后陈氏及李贵妃主持政务。神宗亲政后，深居宫中，荒淫享乐，政治腐败，神宗时，北方努尔哈赤建立后金，窥视中原。神宗死于1620年，时年58岁。 光宗朱常洛(1582-1620) 年号“泰昌”1620年即位，在位1月 光宗，神宗长子。是一个贪财好色的皇帝，由于淫欲过度，即位当天就病倒了，后因服用丹砂过度而亡。时年39岁。 熹宗朱由校(1605-1627) 年号“天启”1620年即位，在位8年 熹宗，光宗长子。在位时任用宦官魏忠贤，致使政治腐败。努尔哈赤乘机攻占沈阳。熹宗死于1627年，时年23岁。 思宗朱由检(1610-1644) 年号“崇祯”1627年即位，在位17年 思宗，光宗第五子。即位后，诛杀魏忠贤，颇为勤政，勉力振作，无奈积重难反，各地农民起义不断爆发，北方皇太极又不断骚扰入侵，又崇祯性多疑，刚愎自用。终于在1644年，李自成攻入北京，崇祯帝在景山自缢身亡。时年35岁。死前于兰色袍服上大书“勿伤百姓一人”。","categories":[{"name":"Other","slug":"Other","permalink":"http://example.com/categories/Other/"}],"tags":[{"name":"明朝皇帝列表","slug":"明朝皇帝列表","permalink":"http://example.com/tags/%E6%98%8E%E6%9C%9D%E7%9A%87%E5%B8%9D%E5%88%97%E8%A1%A8/"}]},{"title":"Ansible常用模块详解","slug":"ansible常用模块详解","date":"2019-02-24T06:41:20.000Z","updated":"2021-01-26T02:34:53.714Z","comments":true,"path":"2019/02/24/ansible常用模块详解/","link":"","permalink":"http://example.com/2019/02/24/ansible%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"写在前边的根据zs官方的分类，将模块按功能分类为：云模块、命令模块、数据库模块、文件模块、资产模块、消息模块、监控模块、网络模块、通知模块、包管理模块、源码控制模块、系统模块、单元模块、web设施模块、windows模块 ，具体可以参看官方页面。这里从官方分类的模块里选择最常用的一些模块进行介绍。 一、ping模块测试主机是否是通的，用法很简单，不涉及参数： ansible test -m ping 二、setup模块setup模块，主要用于获取主机信息，在playbooks里经常会用到的一个参数gather_facts就与该模块相关。setup模块下经常使用的一个参数是filter参数，具体使用示例如下： ansible 10.212.52.252 -m setup -a &#39;filter=ansible_*_mb&#39; //查看主机内存信息 ansible 10.212.52.252 -m setup -a &#39;filter=ansible_eth[0-2]&#39; //查看地接口为eth0-2的网卡信息 ansible all -m setup --tree /tmp/facts //将所有主机的信息输入到/tmp/facts目录下，每台主机的信息输入到主机名文件中（/etc/ansible/hosts里的主机名） 三、file模块file模块主要用于远程主机上的文件操作，file模块包含如下选项： 常用参数 force：需要在两种情况下强制创建软链接，一种是源文件不存在但之后会建立的情况下；另一种是目标软链接已存在,需要先取消之前的软链，然后创建新的软链，有两个选项：yes|no group：定义文件/目录的属组 mode：定义文件/目录的权限 owner：定义文件/目录的属主 path：必选项，定义文件/目录的路径, required recurse：递归的设置文件的属性，只对目录有效 src：要被链接的源文件的路径，只应用于state=link的情况 dest：被链接到的路径，只应用于state=link的情况 state： directory：如果目录不存在，创建目录 file：即使文件不存在，也不会被创建 link：创建软链接 hard：创建硬链接 touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间 absent：删除目录、文件或者取消链接文件 使用示例： * ansible test -m file -a “src=/etc/fstab dest=/tmp/fstab state=link” * ansible test -m file -a “path=/tmp/fstab state=absent” * ansible test -m file -a “path=/tmp/test state=touch” 四、copy模块复制文件到远程主机 常用参数 backup：在覆盖之前将原文件备份，备份文件包含时间信息。有两个选项：yes|no content：用于替代”src”,可以直接设定指定文件的值 dest：必选项。要将源文件复制到的远程主机的绝对路径，如果源文件是一个目录，那么该路径也必须是个目录, required. directory_mode：递归的设定目录的权限，默认为系统默认权限 force：如果目标主机包含该文件，但内容不同，如果设置为yes，则强制覆盖，如果为no，则只有当目标主机的目标位置不存在该文件时，才复制。默认为yes others：所有的file模块里的选项都可以在这里使用 src：要复制到远程主机的文件在本地的地址，可以是绝对路径，也可以是相对路径。如果路径是一个目录，它将递归复制。在这种情况下，如果路径使用”/“来结尾，则只复制目录里的内容，如果没有使用”/“来结尾，则包含目录在内的整个内容全部复制，类似于rsync。 validate ：The validation command to run before copying into place. The path to the file to validate is passed in via ‘%s’ which must be present as in the visudo example below. 示例如下 * ansible test -m copy -a “src=/srv/myfiles/foo.conf dest=/etc/foo.conf owner=foo group=foo mode=0644” * ansible test -m copy -a “src=/mine/ntp.conf dest=/etc/ntp.conf owner=root group=root mode=644 backup=yes” * ansible test -m copy -a “src=/mine/sudoers dest=/etc/sudoers validate=’visudo -cf %s’” 五、service模块用于管理服务 常用参数 arguments：给命令行提供一些选项 enabled：是否开机启动 yes|no name：必选项，服务名称 pattern：定义一个模式，如果通过status指令来查看服务的状态时，没有响应，就会通过ps指令在进程中根据该模式进行查找，如果匹配到，则认为该服务依然在运行 runlevel：运行级别 sleep：如果执行了restarted，在则stop和start之间沉睡几秒钟 state：对当前服务执行启动，停止、重启、重新加载等操作（started, stopped, restarted, reloaded） 使用示例 * ansible test -m service -a “name=httpd state=started enabled=yes” * asnible test -m service -a “name=foo pattern=/usr/bin/foo state=started” * ansible test -m service -a “name=network state=restarted args=eth0” 六、cron模块用于管理计划任务 常用参数 backup：对远程主机上的原任务计划内容修改之前做备份 cron_file：如果指定该选项，则用该文件替换远程主机上的cron.d目录下的用户的任务计划 day：日（1-31，，/2,……） hour：小时（0-23，，/2，……） minute：分钟（0-59，，/2，……） month：月（1-12，，/2，……） weekday：周（0-7，*，……） job：要执行的任务，依赖于state=present name：该任务的描述 special_time：指定什么时候执行，参数：reboot,yearly,annually,monthly,weekly,daily,hourly state：确认该任务计划是创建还是删除 user：以哪个用户的身份执行 使用示例 * ansible test -m cron -a ‘name=”a job for reboot” special_time=reboot job=”/some/job.sh”‘ * ansible test -m cron -a ‘name=”yum autoupdate” weekday=”2” minute=0 hour=12 user=”root * ansible test -m cron -a ‘backup=”True” name=”test” minute=”0” hour=”5,2” job=”ls -alh &gt; /dev/null”‘ * ansilbe test -m cron -a ‘cron_file=ansible_yum-autoupdate state=absent’ 七、yum模块使用yum包管理器来管理软件包 常用参数 config_file：yum的配置文件 disable_gpg_check：关闭gpg_check disablerepo：不启用某个源 enablerepo：启用某个源 name：要进行操作的软件包的名字，也可以传递一个url或者一个本地的rpm包的路径 state：状态（present，absent，latest） 使用示例 * ansible test -m yum -a ‘name=httpd state=latest’ * ansible test -m yum -a ‘name=”@Development tools” state=present’ * ansible test -m yum -a ‘name=http://nginx.org/packages/centos/6/noarch/RPMS/nginx-release-centos-6-0.el6.ngx.noarch.rpm state=present’ 八、user模块用户管理 常用参数 home：指定用户的家目录，需要与createhome配合使用 groups：指定用户的属组 uid：指定用的uid password：指定用户的密码 name：指定用户名 createhome：是否创建家目录 yes|no system：是否为系统用户 remove：当state=absent时，remove=yes则表示连同家目录一起删除，等价于userdel -r state：是创建还是删除 shell：指定用户的shell环境 使用示例： * user: name=johnd comment=”John Doe” uid=1040 group=admin * user: name=james shell=/bin/bash groups=admins,developers append=yes user: name=johnd state=absent remove=yes * user: name=james18 shell=/bin/zsh groups=developers expires=1422403387 * user: name=test generate_ssh_key=yes ssh_key_bits=2048 ssh_key_file=.ssh/id_rsa #生成密钥时，只会生成公钥文件和私钥文件，和直接使用ssh-keygen指令效果相同，不会生成authorized_keys文件。 注：指定password参数时，不能使用明文密码，因为后面这一串密码会被直接传送到被管理主机的/etc/shadow文件中，所以需要先将密码字符串进行加密处理。然后将得到的字符串放到password中即可。 echo &quot;123456&quot; | openssl passwd -1 -salt $(&lt; /dev/urandom tr -dc &#39;[:alnum:]&#39; | head -c 32) -stdin $1$4P4PlFuE$ur9ObJiT5iHNrb9QnjaIB0 #使用上面的密码创建用户 ansible all -m user -a &#39;name=foo password=&quot;$1$4P4PlFuE$ur9ObJiT5iHNrb9QnjaIB0&quot;&#39; 不同的发行版默认使用的加密方式可能会有区别，具体可以查看/etc/login.defs文件确认，centos 6.5版本使用的是SHA512加密算法。 九、group模块组管理模块 常用参数 gid ：指定gid name : 指定组名称 state ：操作状态，present,absent system : 是否为系统组 使用示例 ansible all -m group -a ‘name=somegroup state=present’ 十、synchronize模块使用rsync同步文件 常用参数 archive: 归档，相当于同时开启recursive(递归)、links、perms、times、owner、group、-D选项都为yes ，默认该项为开启 checksum: 跳过检测sum值，默认关闭 compress:是否开启压缩 copy_links：复制链接文件，默认为no ，注意后面还有一个links参数 delete: 删除不存在的文件，默认no dest：目录路径 dest_port：默认目录主机上的端口 ，默认是22，走的ssh协议 dirs：传速目录不进行递归，默认为no，即进行目录递归 rsync_opts：rsync参数部分 set_remote_user：主要用于/etc/ansible/hosts中定义或默认使用的用户与rsync使用的用户不同的情况 mode: push或pull 模块，push模的话，一般用于从本机向远程主机上传文件，pull 模式用于从远程主机上取文件 使用示例 * src=some/relative/path dest=/some/absolute/path rsync_path=”sudo rsync” * src=some/relative/path dest=/some/absolute/path archive=no links=yes * src=some/relative/path dest=/some/absolute/path checksum=yes times=no * src=/tmp/helloworld dest=/var/www/helloword rsync_opts=—no-motd,—exclude=.git mode=pull 十一、filesystem模块在块设备上创建文件系统 常用参数 dev：目标块设备 force：在一个已有文件系统 的设备上强制创建 fstype：文件系统的类型 opts：传递给mkfs命令的选项 使用示例 * ansible test -m filesystem -a ‘fstype=ext2 dev=/dev/sdb1 force=yes’ * ansible test -m filesystem -a ‘fstype=ext4 dev=/dev/sdb1 opts=”-cc”‘ 十二、mount模块配置挂载点 常用参数 fstype：必选项，挂载文件的类型 name：必选项，挂载点 opts：传递给mount命令的参数 src：必选项，要挂载的文件 state：必选项 present：只处理fstab中的配置 absent：删除挂载点 mounted：自动创建挂载点并挂载之 umounted：卸载 使用示例： name=/mnt/dvd src=/dev/sr0 fstype=iso9660 opts=ro state=present name=/srv/disk src=&#39;LABEL=SOME_LABEL&#39; state=present name=/home src=&#39;UUID=b3e48f45-f933-4c8e-a700-22a159ec9077&#39; opts=noatime state=present ansible test -a &#39;dd if=/dev/zero of=/disk.img bs=4k count=1024&#39; ansible test -a &#39;losetup /dev/loop0 /disk.img&#39; ansible test -m filesystem &#39;fstype=ext4 force=yes opts=-F dev=/dev/loop0&#39; ansible test -m mount &#39;name=/mnt src=/dev/loop0 fstype=ext4 state=mounted opts=rw&#39; 十三、get_url 模块该模块主要用于从http、ftp、https服务器上下载文件（类似于wget） 常用参数 sha256sum：下载完成后进行sha256 check； timeout：下载超时时间，默认10s url：下载的URL url_password、url_username：主要用于需要用户名密码进行验证的情况 use_proxy：是事使用代理，代理需事先在环境变更中定义 使用示例： get_url: url=http://example.com/path/file.conf dest=/etc/foo.conf mode=0440 get_url: url=http://example.com/path/file.conf dest=/etc/foo.conf sha256sum=b5bb9d8014a0f9b1d61e21e796d78dccdf1352f23cd32812f4850b878ae4944c 十四、unarchive模块用于解压文件，模块包含如下选项： 常用参数 copy：在解压文件之前，是否先将文件复制到远程主机，默认为yes。若为no，则要求目标主机上压缩包必须存在。 creates：指定一个文件名，当该文件存在时，则解压指令不执行 dest：远程主机上的一个路径，即文件解压的路径 grop：解压后的目录或文件的属组 list_files：如果为yes，则会列出压缩包里的文件，默认为no，2.0版本新增的选项 mode：解决后文件的权限 src：如果copy为yes，则需要指定压缩文件的源路径 owner：解压后文件或目录的属主 使用示例 unarchive: src=foo.tgz dest=/var/lib/foo unarchive: src=/tmp/foo.zip dest=/usr/local/bin copy=no unarchive: src=https://example.com/example.zip dest=/usr/local/bin copy=no","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Ansible","slug":"Ansible","permalink":"http://example.com/tags/Ansible/"}]},{"title":"Commonly used Git command list","slug":"Commonly used Git command list","date":"2019-02-24T06:33:56.000Z","updated":"2021-01-27T02:25:26.034Z","comments":true,"path":"2019/02/24/Commonly used Git command list/","link":"","permalink":"http://example.com/2019/02/24/Commonly%20used%20Git%20command%20list/","excerpt":"","text":"说在前边的话一般来说，日常使用只要记住下图6个命令，就可以了。但是熟练使用，恐怕要记住60～100个命令。 几个代码库* Workspace：工作区 * Index / Stage：暂存区 * Repository：仓库区（或本地仓库） * Remote：远程仓库 一、新建代码库# 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 二、配置Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot; 三、增加/删除文件# 添加指定文件到暂存区 $ git add [file1][file2]... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1][file2]... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original][file-renamed] 四、代码提交# 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1][file2]...-m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1][file2]... 五、分支# 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch][commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch][remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch][remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 六、标签# 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag][commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote][tag] # 提交所有tag $ git push [remote]--tags # 新建一个分支，指向某个tag $ git checkout -b [branch][tag] 七、查看信息# 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file]$ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5--pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 八、远程同步# 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname][url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote][branch] # 上传本地指定分支到远程仓库 $ git push [remote][branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote]--force # 推送所有分支到远程仓库 $ git push [remote]--all 九、撤销# 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit][file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 十、其他# 生成一个可供发布的压缩包 $ git archive","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"}]},{"title":"Git common commands","slug":"Git common commands","date":"2019-02-24T06:23:58.000Z","updated":"2021-01-27T02:25:26.036Z","comments":true,"path":"2019/02/24/Git common commands/","link":"","permalink":"http://example.com/2019/02/24/Git%20common%20commands/","excerpt":"","text":"常用的几条git add . git commit -m &quot;xxxxxx&quot; git push git pull 基本操作查看、添加、提交、删除、找回，重置修改文件git help &lt;command&gt; # 显示command的help git show # 显示某次提交的内容 git show $id git co -- &lt;file&gt; # 抛弃工作区修改 git co . # 抛弃工作区修改 git add &lt;file&gt; # 将工作文件修改提交到本地暂存区 git add . # 将所有修改过的工作文件提交暂存区 git rm &lt;file&gt; # 从版本库中删除文件 git rm &lt;file&gt; --cached # 从版本库中删除文件，但不删除文件 git reset &lt;file&gt; # 从暂存区恢复到工作文件 git reset -- . # 从暂存区恢复到工作文件 git reset --hard # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改 git ci &lt;file&gt; git ci . git ci -a # 将git add, git rm和git ci等操作都合并在一起做 git ci -am &quot;some comments&quot; git ci --amend # 修改最后一次提交记录 git revert &lt;$id&gt; # 恢复某次提交的状态，恢复动作本身也创建次提交对象 git revert HEAD # 恢复最后一次提交的状态 查看文件diffgit diff &lt;file&gt; # 比较当前文件和暂存区文件差异 git diff git diff &lt;id1&gt;&lt;id2&gt; # 比较两次提交之间的差异 git diff &lt;branch1&gt;..&lt;branch2&gt; # 在两个分支之间比较 git diff --staged # 比较暂存区和版本库差异 git diff --cached # 比较暂存区和版本库差异 git diff --stat # 仅仅比较统计信息 查看提交记录git log git log &lt;file&gt; # 查看该文件每次提交记录 git log -p &lt;file&gt; # 查看每次详细修改内容的diff git log -p -2 # 查看最近两次详细修改内容的diff git log --stat #查看提交统计信息 tigMac上可以使用tig代替diff和log， 安装：brew install tig Git 本地分支管理查看、切换、创建和删除分支git br -r # 查看远程分支 git br &lt;new_branch&gt; # 创建新的分支 git br -v # 查看各个分支最后提交信息 git br --merged # 查看已经被合并到当前分支的分支 git br --no-merged # 查看尚未被合并到当前分支的分支 git co &lt;branch&gt; # 切换到某个分支 git co -b &lt;new_branch&gt; # 创建新的分支，并且切换过去 git co -b &lt;new_branch&gt; &lt;branch&gt; # 基于branch创建新的new_branch git co $id # 把某次历史提交记录checkout出来，但无分支信息，切换到其他分支会自动删除 git co $id -b &lt;new_branch&gt; # 把某次历史提交记录checkout出来，创建成一个分支 git br -d &lt;branch&gt; # 删除某个分支 git br -D &lt;branch&gt; # 强制删除某个分支 (未被合并的分支被删除的时候需要强制) 分支合并和rebasegit merge &lt;branch&gt; # 将branch分支合并到当前分支 git merge origin/master --no-ff # 不要Fast-Foward合并，这样可以生成merge提交 git rebase master &lt;branch&gt; # 将master rebase到branch，相当于： git co &lt;branch&gt; &amp;&amp; git rebase master &amp;&amp; git co master &amp;&amp; git merge &lt;branch&gt; Git补丁管理(方便在多台机器上开发同步时用)git diff &gt; ../sync.patch # 生成补丁 git apply ../sync.patch # 打补丁 git apply --check ../sync.patch #测试补丁能否成功 Git暂存管理git stash # 暂存 git stash list # 列所有stash git stash apply # 恢复暂存的内容 git stash drop # 删除暂存区 Git远程分支管理git pull # 抓取远程仓库所有分支更新并合并到本地 git pull --no-ff # 抓取远程仓库所有分支更新并合并到本地，不要快进合并 git fetch origin # 抓取远程仓库更新 git merge origin/master # 将远程主分支合并到本地当前分支 git co --track origin/branch # 跟踪某个远程分支创建相应的本地分支 git co -b &lt;local_branch&gt; origin/&lt;remote_branch&gt; # 基于远程分支创建本地分支，功能同上 git push # push所有分支 git push origin master # 将本地主分支推到远程主分支 git push -u origin master # 将本地主分支推到远程(如无远程主分支则创建，用于初始化远程仓库) git push origin &lt;local_branch&gt; # 创建远程分支， origin是远程仓库名 git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; # 创建远程分支 git push origin :&lt;remote_branch&gt; #先删除本地分支(git br -d &lt;branch&gt;)，然后再push删除远程分支 Git远程仓库管理GitHubgit remote -v # 查看远程服务器地址和仓库名称 git remote show origin # 查看远程服务器仓库状态 git remote add origin git@ github:robbin/robbin_site.git # 添加远程仓库地址 git remote set-url origin git@ github.com:robbin/robbin_site.git # 设置远程仓库地址(用于修改远程仓库地址) git remote rm &lt;repository&gt; # 删除远程仓库 创建远程仓库git clone --bare robbin_site robbin_site.git # 用带版本的项目创建纯版本仓库 scp -r my_project.git git@ git.csdn.net:~ # 将纯仓库上传到服务器上 mkdir robbin_site.git &amp;&amp; cd robbin_site.git &amp;&amp; git --bare init # 在服务器创建纯仓库 git remote add origin git@ github.com:robbin/robbin_site.git # 设置远程仓库地址 git push -u origin master # 客户端首次提交 git push -u origin develop # 首次将本地develop分支提交到远程develop分支，并且track git remote set-head origin master # 设置远程仓库的HEAD指向master分支 也可以命令设置跟踪远程库和本地库git branch --set-upstream master origin/master git branch --set-upstream develop origin/develop","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"}]},{"title":"GitHub + Hugo to Build a Personal Blog","slug":"GitHub + Hugo To build a personal blog","date":"2019-02-23T06:22:31.000Z","updated":"2021-01-27T02:25:26.037Z","comments":true,"path":"2019/02/23/GitHub + Hugo To build a personal blog/","link":"","permalink":"http://example.com/2019/02/23/GitHub%20+%20Hugo%20To%20build%20a%20personal%20blog/","excerpt":"","text":"- 自己搭博客的N个理由 你的笔记非常值钱，丢了就等于丢了N年工作经验。 大多数博客网站都满足不了你： 倒闭/停止服务/被删库时，你多半没备份。 格式互不兼容，导入导出困难，复制粘贴会乱。 富文本编辑器极其难用。 可配置的地方不够，你讨厌的功能一堆。 可以使用 Markdown 写博文： 任何一个文本编辑器都能写，随时随地。 足够沉浸，让你忘记格式，专注内容。 备份容易： GitHub, GitBook, 印象笔记，CSDN，博客园，简书等都支持，到处复制粘贴也能得到大致相同的格式。 纯文本本身也有足够可读性，网站/软件不支持也无所谓。 GitHub 提供免费空间和 CDN，站点放 GitHub 仓库也方便做版本控制。 用来当印象笔记的一个备份网站，写作的同时加深印象 方便自己，方便他人 可以作为学习新技术的试验田。 注册 GitHub先注册一个 GitHub 账号。 用户名会成为网址的一部分，以下为建议 最好全英文小写 不需要空格 不需要太长 建议使用自己的名字，方便好记。 例：用户名 adolf，对应网址 adolf.github.io 安装GitMac brew install git PC下载安装 Git ，全用默认设置，然后打开 Git 终端。 默认工作目录是 Git 安装目录下的 mingw64 文件夹。 路径分隔符使用 / 或 \\ 设置 Git 默认用户信息git config --global user.name &quot;adolf&quot; git config --global user.email &quot;adolf@qq.com&quot; # 使用 GitHub 注册的用户名和邮箱 这台机器上的每个项目提交时默认都会用这些信息。 创建 GitHub Pages在 GitHub 上新建仓库(repository)： 名字必须为 username.github.io ，如：adolf.github.io 只要往里面提交一个 index.html 文件，就能通过 https://adolf.github.io 访问了。 不过除了前端，没几个人愿意碰 HTML 和 CSS，这么建站实在太2000年代了，生成器用起来。 使用静态网站生成器搭建站点个人网站/博客通常只需要简单的静态页面，这类工具 正好可以把 Markdown 文档转换成网页，非常方便。 这里选择 Hugo，特点是生成速度非常快（构建5000篇文章不用10秒），安装配置也不算复杂，适合不想折腾的人。 它最实用的就是实时预览功能，不需要发布到 Github 就能看到改动。 安装Mac brew install hugo # 查看版本 hugo version # 查看帮助 PC下载后解压，把 hugo.exe 的路径加进环境变量 Path 就行。 新建站点假设站点文件想放在当前目录的 hugo-blog/ 目录下： hugo new site -f yaml hugo-blog # -f, --format &#123;yaml|json|toml&#125; 指定 config 和 front matter 的格式，默认 toml。 # 推荐 yaml 格式。 生成的目录结构如下： $ cd hugo-blog $ tree . ├── archetypes # 博文模板 │ └── default.md # 默认模板文件 ├── config.yaml # 主配置文件 ├── content # 存放博文，里面的子目录叫做 section。 ├── data # 生成网站时的配置 ├── layouts # 网页框架，放在这里的文件会比主题里的同名文件优先，可以在不动主题源码的情况下覆盖部分设置。 ├── static # 静态资源，这目录下的文件会原封不动的拷到站点根目录下。 └── themes # 主题 https://gohugo.io/commands/hugo_new_site/ https://gohugo.io/getting-started/directory-structure/ 下载主题默认不带主题，从 这里 挑喜欢的下载。 我用的是 Even ： git clone https://github.com/olOwOlo/hugo-theme-even themes/even 修改配置文件编辑 config.yaml，默认值如下： baseURL: http://example.org/ # 替换为你的网址 languageCode: en-us # 中文博客建议改为 zh-cn。如果文章内容不是这里指定的编码，Chrome 会弹出要不要翻译页面的提示。 title: My New Hugo Site # 替换为你的博客标题 theme: even #替换为刚才的主题 或者也可以使用even主题自带的配置，默认在 even github 的页面已经给出，大家可自行操作： Important: Take a look inside the exampleSite folder of this theme. You&#39;ll find a file called config.toml. To use it, copy the config.toml in the root folder of your Hugo site. Feel free to change it. 编辑默认模板在 archetype/ 下新建 .md 文件，如果文件名跟新建文章时的 SECTIONNAME 一致，则该 section 下的文章都会套用这模板。 找不到匹配的 section 才会套用 default.md。 default.md 的默认 front matter： --- title: &#39;&#123;&#123; replace .Name \"-\" \" \" | title &#125;&#125;&#39; date: &#123;&#123; .Date &#125;&#125; draft: true --- 添加常用 front matter（加在分隔符之间）： slug: &#123;&#123; .TranslationBaseName &#125;&#125; # URL 路径的最后一部分 # .TranslationBaseName 为不带语言标识符的文件名。比如文件名为 foo.en.md，得到 foo tags: [] categories: [] 另外 Even 主题还配置了一些特有的参数，参见它的 默认模板。 https://gohugo.io/content-management/archetypes/ https://gohugo.io/variables/files/ 调整样式如果对主题某些地方不满意，又不想直接改主题源文件，可以去 themes/&lt;主题名&gt;/layouts/ 看看都有什么文件，把要改的拷到 layouts/ 下。 文件查找顺序大致如下，同名文件在 layouts/ 下的比主题里的优先，匹配到 section 名的比 _default 优先。 layouts/&lt;section&gt;/... themes/&lt;主题名&gt;/layouts/&lt;section&gt;/... layouts/_default/... themes/&lt;主题名&gt;/layouts/_default/... https://gohugo.io/templates/lookup-order/#examples-layout-lookup-for-regular-pages 新建博客文章hugo new post/2016-07-19-first.md # 格式：&lt;SECTIONNAME&gt;/&lt;FILENAME&gt;.&lt;FORMAT&gt; 可以看到在 content/ 下生成了 post/ 目录，post/ 下有 2016-07-19-first.md 文件。 推荐文件名加上日期前缀，因为通常所有博文都放在同一个目录下（这样配置最简单），带日期一眼就能区分新旧文章。 这操作经常用，建议写成脚本：（例如叫做 new） 自动加上当天日期做前缀section 名默认叫 post因为上面在配置文件里已经设置了把日期前缀提取到 URL 路径，front matter 里的 slug 去掉前缀。 #!/bin/bash [[ -z &quot;$1&quot; ]] &amp;&amp; echo &quot;Usage: $(basename &quot;$0&quot;) FILENAME(without suffix) [SECTIONNAME]&quot; &amp;&amp; exit 1 section_name=$&#123;2:-post&#125; date_prefix=$(date +%Y-%m-%d-) file_path=&quot;$&#123;section_name&#125;/$&#123;date_prefix&#125;$1.md&quot; hugo new &quot;$&#123;file_path&#125;&quot; sed -i &#39;.bak&#39; &quot;s/slug: $&#123;date_prefix&#125;/slug: /&quot; &quot;content/$&#123;file_path&#125;&quot; rm -f &quot;content/$&#123;file_path&#125;.bak&quot; # Mac 的 sed -i 的第1个参数必须为备份文件后缀名 编辑博文用文本编辑器打开刚才生成的文件，可以看见类似如下内容（YAML格式）： --- title: &quot;2016 07 19 First&quot; date: 2018-02-10T23:16:02+08:00 draft: true --- （或默认的 TOML 格式）： +++ date = &quot;2016-07-19T00:12:34+08:00&quot; draft = true title = &quot;2016 07 19 first&quot; +++ 像这样的出现在每篇文章前的元数据叫 front matter，— 之间包着的内容会解析为 YAML，+++ 之间包着的内容会解析为 TOML。 在下面的空白处用 Markdown 格式写正文，如： ## Hello Hugo 坚持写博客的好处： - 记录心得 - 整理思路 - 分享交流 - 求职展示 完整文件见 这里 https://gohugo.io/content-management/front-matter/ 资源文件放到 static/ 下，这目录下的所有文件和目录都会原封不动的拷到站点根目录下。 我习惯用的目录结构： . ├── CNAME ├── css # CSS 文件目录 ├── googleXXX.html # Google Analytics 的验证网页 ├── img # 图片目录 │ ├── reward # 特定的主题专门建目录 │ │ ├── alipay.jpg # 支付宝收款二维码 │ │ └── wechat.png # 微信赞赏二维码 │ └── post # 一般的博文配图按年月日建目录 │ └── 2018 │ └── 02 │ └── 28 ├── js # JS 脚本目录 └── robots.txt 【注意】 博文里引用图片时要写完整 URL，前面是实际的域名，后面是相对 static/ 目录的路径。 例： Hugo 不会帮你压缩图片，也不提供缩略图，建议提交前用工具处理图片。 不重要的图可以压成 70% jpg，宽度缩到 600px。颜色少可以试试 8色 png。 命令行工具有 imagemagick 用来裁剪和压缩图片，转格式等，exiftool 用来去掉 EXIF 信息。 本地预览运行 hugo server，就会启动一个很简单的 HTTP 服务器。浏览器打开 localhost:1313 就能看到生成的网页，样式跟发布到线上完全相同。 之后文件有任何改动都会自动重新构建和刷新浏览器页面。 【注意】 运行 hugo server 时，当前工作目录必须为站点根目录（包含配置文件），否则提示 Error: Unable to locate Config file.Hugo 每次生成站点时不会删旧文件，因此推荐把预览和发布目录分开，并且每次生成前把旧目录删除。这操作经常用，建议写成脚本：（例如叫做 dev_preview） #!/bin/bash [[ -d dev ]] &amp;&amp; rm -rf dev hugo server --buildDrafts --destination dev --disableFastRender # -D, --buildDrafts[=false] 文章的默认状态是草稿，草稿默认不会构建，必须加上这参数才会生成页面。 # -w, --watch 文件有改动时自动重新构建并刷新浏览器页面。（默认就带这个，不传也行） # -d, --destination DIR 输出目录。不传这参数的话构建出来的文件只会放在内存里。 # --disableFastRender 有改动时触发完整构建。反正 Hugo 非常快，几百毫秒根本感觉不到。 ctrl + c 结束 hugo server 。 生成发布页面首先把要发布的文章的 draft 属性改为 false，运行 hugo。 执行后会生成 public/ 目录，可以看到之前的 Markdown 文件转换成了文件夹 + HTML 文件。 这操作经常用，建议写成脚本：（例如叫做 release） #!/bin/bash if [[ -d public ]]; then GLOBIGNORE=*.git rm -rf -v public/* fi hugo if [[ -n &quot;$1&quot; ]]; then cd public git add -A git commit -m &quot;$1&quot; git push else echo echo &quot;[WARN] Files will NOT be uploaded to Github without adding comments.&quot; echo &quot;Usage: $(basename &quot;$0&quot;) COMMIT_COMMENTS&quot; fi 再用 hugo server 检查一下有没有 draft 忘了改，确定文章能看到就可以发布了。 这步也可以写成脚本：（例如叫做 preview） #!/bin/bash hugo server --disableFastRender 本地图片由于还没上传，肯定全是叉。介意的话可以先把图传上去，验证过路径都写对了才发布网页。 发布到 GitHub可以装 GitHub Desktop 或 SourceTree 等客户端，或者直接命令行： cd public git init git remote add origin &quot;git@github.com:keithmork/keithmork.github.io.git&quot; # 替换成你的 GitHub Pages 仓库地址 git add -A git commit -m &quot;first commit&quot; git pull git push -u origin master # 注意：要用 SSH 的仓库地址才能用公钥，如果用了 HTTPS 的仓库地址，必须每次输用户名密码。 之后会提示输入在 GitHub 注册的邮箱和密码。 提交成功后，浏览器打开https://adolf.github.io，就能看到刚才的页面了。 第一次麻烦点，之后每次提交都很简单：（已经写在上面的 release 脚本里，参数传 true 就会提交和发布） git add -A git commit -m &quot;XXX&quot; git push 使用 SSH 密钥登录 GitHub每次发博文都输用户名密码太麻烦，用密钥代替密码就方便多了。 前提是机器只有你一个人用。 生成SSH密钥对 提示 Enter file in which to save the key 时直接回车，使用默认设置。 文件名不用改，GitHub 连接时只认 id_rsa 提示 Enter passphrase 和 Enter same passphrase again 时，可以直接回车，不使用口令（否则每次提交时会要求输入口令）。 https://help.github.com/articles/generating-an-ssh-key/ 添加 SSH 公钥到 GitHub复制公钥文件（默认 ~/.ssh/id_rsa.pub）的内容。登上 GitHub，在个人设置里找到 SSH and GPG keys，新建 SSH key，粘贴进去。取个容易识别的名字，如 Mac-Home PC-Work 等，保存。测试是否成功： ssh -T git@github.com # 用户名就是git，不用改。 # 如果报错，这样看详细信息： ssh -vT git@github.com # 看到以下讯息就是成功了：（虽然命令返回 1） # You&#39;ve successfully authenticated, but GitHub does not provide shell access. 看到 Are you sure you want to continue connecting 时输 yes 回车，然后就可以了。 如果依然每次都问用户名密码，可能是当初加仓库地址时用了 HTTPS 格式，改为 SSH 格式的地址就好了： git remote set-url origin git@github.com:keithmork/keithmork.github.io.git 发布 Hugo 工程源文件到 GitHub源文件比生成的发布文件重要得多，必须备份到 GitHub。发布文件丢了随时重新生成，源文件丢了就没了。 在 GitHub 新建仓库，例如叫 hugo-blog，不要勾选创建 README.md 。在工程根目录下创建 .gitignore 文件，写上不需要备份的目录和文件，例： public themes dev .git .DS_Store *.bak *_bak *.old *.log 如果想写项目简介，创建 README.md 文件。和之前类似的操作： git init git remote add origin &quot;git@github.com:adolf/hugo-blog.git&quot; # 替换成你的 GitHub 仓库地址 git add -A git commit -m &quot;first commit&quot; git pull git push -u origin master 如果忘了写 .gitignore，把 public 和 themes 也提交了上去，会发现它们被认为是 submodule （因为里面有 .git 目录）。即使之后设置忽略它们，每次里面的文件有更新时都会出现烦人的子模块状态变更的记录。 解决方法：先把那2个目录复制到别的地方，把它们删掉，写好 .gitignore，在 .git/config 里删掉 [submodule] 相关内容，提交，再把它们搬回来。 遇到的问题 文章时间一直不对，修改config.toml文件，将以下参数修改成： enableGitInfo = false dateFormatToUse = &quot;&quot; 打赏处的二维码一直不显示 [params.reward] # 文章打赏 enable = true wechat = &quot;https://emor.me/img/reward/wechat.jpg&quot; # 微信二维码 alipay = &quot;https://emor.me/img/reward/alipay.jpg&quot; # 支付宝二维码 结束语到这里，一个属于自己的静态博客基本成型了。 可能样式、功能或别的细节不能完全令人满意，但那些基本不影响写文章，可以先专注于输出内容，其他留到以后慢慢优化。 本文源链接是Haunted Hovel - 闹鬼小屋","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"GitHub","slug":"GitHub","permalink":"http://example.com/tags/GitHub/"},{"name":"Hugo","slug":"Hugo","permalink":"http://example.com/tags/Hugo/"}]},{"title":"Create_lvm","slug":"Create Lvm","date":"2019-02-22T17:20:58.000Z","updated":"2021-01-27T02:25:26.035Z","comments":true,"path":"2019/02/23/Create Lvm/","link":"","permalink":"http://example.com/2019/02/23/Create%20Lvm/","excerpt":"","text":"创建 pvpvcreate /dev/sdb 创建 vgvgcreate docker /dev/sdb 检查 vgvgdisplay docker 创建 lvlvcreate --wipesignatures y -n thinpool docker -l 90%VG 注： 在名字叫 docker 的 vg 上创建名字叫 thinpool 的 lv，并指定这个 lv 使用的空间是 vg 的90%，–wipesignatures y 参数的意思是擦除signatures # 创建 lvlvcreate --wipesignatures y -n thinpoolmeta docker -l 5%VG 注：在名字叫 docker 的 vg 上创建名字叫 thinpoolmeta 的 lv，并指定这个 lv 使用的空间是 vg 的5%，–wipesignatures y 参数的意思是擦除signatures# 将pool转换为thinpoollvconvert -y --zero n -c 512K --thinpool docker/thinpool --poolmetadata docker/thinpoolmeta 注：将pool转换为thinpool， -c 参数指定块大小 –zero n 参数清理控制块 ， –thinpool 参数指定精简池的名字是 docker/thinpool ，–poolmetadata 参数指定了池的元数据的 lv 的名字是 docker/thinpoolmeta# 查看lv lvscan 创建挂载目录mkdir /data 格式化mkfs -t ext4 /dev/docker/thinpool 挂载mount /dev/docker/thinpool /data/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"lvm","slug":"lvm","permalink":"http://example.com/tags/lvm/"}]},{"title":"Delete private registry mirror","slug":"Delete private registry mirror","date":"2019-02-22T17:16:20.000Z","updated":"2021-01-27T02:25:26.035Z","comments":true,"path":"2019/02/23/Delete private registry mirror/","link":"","permalink":"http://example.com/2019/02/23/Delete%20private%20registry%20mirror/","excerpt":"","text":"首先，在默认情况下，docker registry 是不允许删除镜像的，需要在配置config.yml中启用，在reigstry容器内： delete: enabled: true 然后，使用 API GET /v2/&lt;镜像名&gt;/manifests/ 来取得要删除的镜像:Tag所对应的 digest。Registry 2.3 以后，必须加入头 Accept: application/vnd.docker.distribution.manifest.v2+json，否则取到的 digest 是错误的，这是为了防止误删除。比如，要删除 myimage:latest 镜像，那么取得 digest 的命令是： $ curl --header &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; \\ -I -X HEAD http://192.168.99.100:5000/v2/myimage/manifests/latest \\ | grep Digest Docker-Content-Digest: sha256:3a07b4e06c73b2e3924008270c7f3c3c6e3f70d4dbb814ad8bff2697123ca33c 然后调用 API DELETE /v2/&lt;镜像名&gt;/manifests/ 来删除镜像。比如： curl -X DELETE http://192.168.99.100:5000/v2/myimage/manifests/sha256:3a07b4e06c73b2e3924008270c7f3c3c6e3f70d4dbb814ad8bff2697123ca33c 至此，镜像已从 registry 中标记删除，外界访问 pull 不到了。但是 registry 的本地空间并未释放，需要等待垃圾收集才会释放。而垃圾收集不可以在线进行，必须停止 registry，然后执行。比如，假设 registry 是用 Compose 运行的，那么下面命令用来垃圾收集： docker-compose stop docker run -it --name gc --rm --volumes-from registry_registry_1 registry:2 garbage-collect /etc/registry/config.yml docker-compose start 其中 registry_registry_1 可以替换为实际的 registry 的容器名，而 /etc/registry/config.yml 则替换为实际的 registry 配置文件路径。 参考官网文档： https://docs.docker.com/registry/configuration/#/delete https://docs.docker.com/registry/spec/api/#/deleting-an-image","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"registry","slug":"registry","permalink":"http://example.com/tags/registry/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}]},{"title":"Journalctl执行报错","slug":"journalctl","date":"2019-02-21T22:36:03.000Z","updated":"2021-01-26T14:39:54.515Z","comments":true,"path":"2019/02/22/journalctl/","link":"","permalink":"http://example.com/2019/02/22/journalctl/","excerpt":"","text":"原因： 日志文件损坏，删除之前的日志 [root@172 admin]# journalctl Error was encountered while opening journal files: Input/output error 解决方法：rm -rf /var/log/journal/* 然后可以通过重启systemd-journald服务重新生成： systemctl restart systemd-journald%","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"system","slug":"system","permalink":"http://example.com/tags/system/"},{"name":"log","slug":"log","permalink":"http://example.com/tags/log/"}]},{"title":"K8s pod内访问svc IP服务不通","slug":"k8sdepodbunengfangwensvc","date":"2019-02-20T22:32:45.000Z","updated":"2021-01-27T02:25:26.046Z","comments":true,"path":"2019/02/21/k8sdepodbunengfangwensvc/","link":"","permalink":"http://example.com/2019/02/21/k8sdepodbunengfangwensvc/","excerpt":"","text":"在pod内访问服务名不通的问题?Flannel网络,打开网卡的混合模式: ip link set flannel.1 promisc on ip link set cin0 promisc on 同一台主机上的pod内访问同一台Node上svc的ip不通？解决方法： [root@k8s-int-node4 ~]# cat /proc/sys/net/bridge/bridge-nf-call-iptables 0 [root@k8s-int-node4 ~]# echo &#39;1&#39; &gt; /proc/sys/net/bridge/bridge-nf-call-iptables [root@k8s-int-node4 ~]# cat /proc/sys/net/bridge/bridge-nf-call-iptables 1 [root@k8s-int-node4 ~]#","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[]},{"title":"Linux 下各文件夹的结构说明及用途介绍","slug":"Linux 下各文件夹的结构说明及用途介绍","date":"2018-03-09T06:16:39.000Z","updated":"2021-01-27T02:25:26.041Z","comments":true,"path":"2018/03/09/Linux 下各文件夹的结构说明及用途介绍/","link":"","permalink":"http://example.com/2018/03/09/Linux%20%E4%B8%8B%E5%90%84%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E7%BB%93%E6%9E%84%E8%AF%B4%E6%98%8E%E5%8F%8A%E7%94%A8%E9%80%94%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"目录简介/bin：二进制可执行命令。 /dev：设备特殊文件。 /etc：系统管理和配置文件。 /etc/rc.d：启动的配 置文件和脚本。 /home：用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示。 /lib：标准程序设计库，又 叫动态链接共享库，作用类似windows里的.dll文件。 /sbin：系统管理命令，这 里存放的是系统管理员使用的管理程序。 /tmp：公用的临时文件存储 点。 /root：系统管理员的主目 录。 /mnt：系统提供这个目录是 让用户临时挂载其他的文件系统。 /lost+found：这个 目录平时是空的，系统非正常关机而留下“无家可归”的文件就在这里。 /proc：虚拟的目录，是系 统内存的映射。可直接访问这个目录来获取系统信息。 /var：某些大文件的溢出 区，比方说各种服务的日志文件。 /usr：最庞大的目录，要用 到的应用程序和文件几乎都在这个目录。其中包含： /usr/x11r6：存放x window的目录。 /usr/bin：众多的应用程序。 /usr/sbin：超级用户的一些管理程序。 /usr/doc：linux文档。 /usr/include：linux下开发和编译应用程序所需要的头文件。 /usr/lib：常用的动态链接库和软件包的配置文件。 /usr/man：帮助文档。 /usr/src：源代码，linux内核的源代码就放在/usr/src/linux 里。 /usr/local/bin：本地增加的命令。 /usr/local/lib：本地增加的库根文件系统。 通常情况下，根文件系统所占空间一般应该比较小，因为其中的绝大部分文件都不需要经常改动，而且包括严格的文件和一个小的 不经常改变的文件系统不容易损坏。除了可能的一个叫/vmlinuz标准的系统引导映像之外，根目录一般不含任何文 件。所有其他文件在根文件系统的子目录中。 /bin目录 /bin目录包含了引导启动所需的命令或普通用户可能用的命令(可能在引导启动后)。这些命 令都是二进制文件的可执行程序(bin是binary的简称)，多是系统中重要的系统文件。 /sbin目录 /sbin目录类似/bin ，也用于存储二进制文件。因为其中的大部分文件多是系统管理员使用的基本的系统程序，所以虽然普通用户必要且允许时可以使用，但一般不给普通用户使 用。 /etc目录 /etc目录存放着各种系统配置文件，其中包括了用户信息文件/etc/passwd， 系统初始化文件/etc/rc等。linux正是靠这些文件才得以正常地运行。 /root目录 /root目录是超级用户的目录。 /lib目录 /lib目录是根文件系统上的程序所需的共享库，存放了根文件系统程序运行所需的共享文件。 这些文件包含了可被许多程序共享的代码，以避免每个程序都包含有相同的子程序的副本，故可以使得可执行文件变得更小，节省空间。 /lib/modules目录 /lib/modules目录包含系统核心可加载各种模块，尤其是那些在恢复损坏的系统时重 新引导系统所需的模块(例如网络和文件系统驱动)。 /dev目录 /dev目录存放了设备文件，即设备驱动程序，用户通过这些文件访问外部设备。比如，用户可 以通过访问/dev/mouse来访问鼠标的输入，就像访问其他文件一样。 /tmp目录 /tmp目录存放程序在运行时产生的信息和数据。但在引导启动后，运行的程序最好使用/var/tmp来 代替/tmp，因为前者可能拥有一个更大的磁盘空间。 /boot目录 /boot目录存放引导加载器(bootstrap loader)使用的文件，如lilo，核心映像也经常放在这里，而不是放在根目录中。但是如果有许多核心映像，这个目录就可能变得很大，这时使用单独的 文件系统会更好一些。还有一点要注意的是，要确保核心映像必须在ide硬盘的前1024柱面内。 /mnt目录 /mnt目录是系统管理员临时安装(mount)文件系统的安装点。程序并不自动支持安装到/mnt 。/mnt下面可以分为许多子目录，例如/mnt/dosa可能是使用 msdos文件系统的软驱，而/mnt/exta可能是使用ext2文件系统的软驱，/mnt/cdrom光 驱等等。 /proc, /usr, /var, /home目录 其他文件系统的安装点。 目录树可以分为小的部分，每个部分可以在自己的磁盘或分区上。主要部分是根、/usr 、/var 和 /home 文件系统。每个部分有不同的目的。 每台机器都有根文件系统，它包含系统引导和使其他文件系统得以mount所必要的文件，根文件系统应该有单用户状态所必须的足够的内容。还应该包括修复损坏 系统、恢复备份等的工具。 /usr 文件系统包含所有命令、库、man页和其他一般操作中所需的不改变的文件。 /usr 不应该有 一般使用中要修改的文件。这样允许此文件系统中的文件通过网络共享，这样可以更有效，因为这样节省了磁盘空间(/usr 很容易是数百兆)，且易于管理 (当升级应用时，只有主/usr 需要改变，而无须改变每台机器) 即使此文件系统在本地盘上，也可以只读mount，以减少系统崩溃时文件系统的损 坏。 /var 文件系统包含会改变的文件，比如spool目录(mail、news、打印机等用的)， log文件、 formatted manual pages和暂存文件。传统上/var 的所有东西曾在 /usr 下的某个地方，但这样/usr 就不可能只读安装 了。 /home 文件系统包含用户家目录，即系统上的所有实际数据。一个大的/home 可能要分为若干文件系统，需要在 /home 下加一级名字，如/home/students 、/home/staff 等。 详细介绍：/etc文件系统/etc目录包含各种系统配置文件，下面说明其中的一些。其他的你应该知道它们属于哪个程序， 并阅读该程序的man页。许多网络配置文件也在/etc中。 /etc/rc或/etc/rc.d或/etc/rc?.d：启动、或改变运行级时运 行的脚本或脚本的目录。 /etc/passwd：用户数据库，其中的域给出了用户名、真实姓名、用户起始目 录、加密口令和用户的其他信息。 /etc/fdprm：软盘参数表，用以说明不同的软盘格式。可用setfdprm进 行设置。更多的信息见setfdprm的帮助页。 /etc/fstab：指定启动时需要自动安装的文件系统列表。也包括用swapon -a启用的swap区的信息。 /etc/group：类似/etc/passwd ，但说明的不是用户信息而是组的信息。包括组的各种数据。 /etc/inittab：init 的配置文件。 /etc/issue：包括用户在登录提示符前的输出信息。通常包括系统的一段短说明 或欢迎信息。具体内容由系统管理员确定。 /etc/magic：“file”的配置文件。包含不同文件格式的说 明，“file”基于它猜测文件类型。 /etc/motd：motd是message of the day的缩写，用户成功登录后自动输出。内容由系统管理员确定。常用于通告信息，如计划关机时间的警告等。 /etc/mtab：当前安装的文件系统列表。由脚本(scritp)初始化，并由 mount命令自动更新。当需要一个当前安装的文件系统的列表时使用(例如df命令)。 /etc/shadow：在安装了影子(shadow)口令软件的系统上的影子口令 文件。影子口令文件将/etc/passwd文件中的加密口令移动到/etc/shadow中，而后者只对超级用户(root)可读。这使破译口令更困 难，以此增加系统的安全性。 /etc/login.defs：login命令的配置文件。 /etc/printcap：类似/etc/termcap ，但针对打印机。语法不同。 /etc/profile 、/etc/csh.login、/etc/csh.cshrc：登 录或启动时bourne或cshells执行的文件。这允许系统管理员为所有用户建立全局缺省环境。 /etc/securetty：确认安全终端，即哪个终端允许超级用户(root) 登录。一般只列出虚拟控制台，这样就不可能(至少很困难)通过调制解调器(modem)或网络闯入系统并得到超级用户特权。 /etc/shells：列出可以使用的shell。chsh命令允许用户在本文件 指定范围内改变登录的shell。提供一台机器ftp服务的服务进程ftpd检查用户shell是否列在/etc/shells文件 中，如果不是，将不允许该用户登录。 /etc/termcap：终端性能数据库。说明不同的终端用什么“转义序列”控 制。写程序时不直接输出转义序列(这样只能工作于特定品牌的终端)，而是从/etc/termcap中查找要做的工作的 正确序列。这样，多数的程序可以在多数终端上运行。 /dev文件系统/dev目录包括所有设备的设备文件。设备文件用特定的约定命名，这在设备列表中说明。设备文件在安装时由系 统产生，以后可以用/dev/makedev描述。/dev/makedev.local 是系统管理员为本地设备文件(或连接)写的描述文稿(即如一些非标准设备驱动不是标准makedev 的一部分)。下面简要介绍/dev下 一些常用文件。 /dev/console：系统控制台，也就是直接和系统连接的监视器。 /dev/hd：ide硬盘驱动程序接口。如：/dev/hda指的是第一个硬 盘，had1则是指/dev/hda的第一个分区。如系统中有其他的硬盘，则依次为/dev /hdb、/dev/hdc、. . . . . .；如有多个分区则依次为hda1、hda2 . . . . . . /dev/sd：scsi磁盘驱动程序接口。如系统有scsi硬盘，就不会访问/dev/had， 而会访问/dev/sda。 /dev/fd：软驱设备驱动程序。如：/dev/fd0指 系统的第一个软盘，也就是通常所说的a盘，/dev/fd1指第二个软盘，. . . . . .而/dev/fd1 h1440则表示访问驱动器1中的4.5高密盘。 /dev/st：scsi磁带驱动器驱动程序。 /dev/tty：提供虚拟控制台支持。如：/dev/tty1指 的是系统的第一个虚拟控制台，/dev/tty2则是系统的第二个虚拟控制台。 /dev/pty：提供远程登陆伪终端支持。在进行telnet登录时就要用到/dev/pty设 备。 /dev/ttys：计算机串行接口，对于dos来说就是“com1”口。 /dev/cua：计算机串行接口，与调制解调器一起使用的设备。 /dev/null：“黑洞”，所有写入该设备的信息都将消失。例如：当想要将屏幕 上的输出信息隐藏起来时，只要将输出信息输入到/dev/null中即可。 /usr文件系统/usr是个很重要的目录，通常这一文件系统很大，因为所有程序安装在这里。/usr里 的所有文件一般来自linux发行版；本地安装的程序和其他东西在/usr/local下，因为这样可以在升级新版系 统或新发行版时无须重新安装全部程序。/usr目录下的许多内容是可选的，但这些功能会使用户使用系统更加有效。/usr可容纳许多大型的软件包和它们的 配置文件。下面列出一些重要的目录(一些不太重要的目录被省略了)。 /usr/x11r6：包含x window系统的所有可执行程序、配置文件和支持文件。为简化x的开发和安装，x的文件没有集成到系统中。x window系统是一个功能强大的图形环境，提供了大量的图形工具程序。用户如果对microsoft windows比较熟悉的话，就不会对x window系统感到束手无策了。 /usr/x386：类似/usr/x11r6 ，但是是专门给x 11 release 5的。 /usr/bin：集中了几乎所有用户命令，是系统的软件库。另有些命令在/bin或/usr/local/bin中。 /usr/sbin：包括了根文件系统不必要的系统管理命令，例如多数服务程序。 /usr/man、/usr/info、/usr/doc：这些目录包含所有手册页、 gnu信息文档和各种其他文档文件。每个联机手册的“节”都有两个子目录。例如：/usr/man/man1中包含联机手册第一节的源码(没有格式化的原 始文件)，/usr/man/cat1包含第一节已格式化的内容。联机手册分为以下九节：内部命令、系统调用、库函数、设备、文件格式、游戏、宏软件包、 系统管理和核心程序。 /usr/include：包含了c语言的头文件，这些文件多以.h结尾，用来描述c 语言程序中用到的数据结构、子过程和常量。为了保持一致性，这实际上应该放在/usr/lib下，但习惯上一直沿用了这 个名字。 /usr/lib：包含了程序或子系统的不变的数据文件，包括一些site – wide配置文件。名字lib来源于库(library); 编程的原始库也存在/usr/lib 里。当编译程序时，程序便会和其中的库进行连接。也有许多程序把配置文件存入其中。 /usr/local：本地安装的软件和其他文件放在这里。这与/usr很相似。用户 可能会在这发现一些比较大的软件包，如tex、emacs等。 /var文件系统/var包含系统一般运行时要改变的数据。通常这些数据所在的目录的大小是要经常变化或扩充 的。原来/var目录中有些内容是在/usr中的，但为了保持/usr目录的相对稳定，就把那些需要经常改变的目录放到/var中了。每个系统是特定的， 即不通过网络与其他计算机共享。下面列出一些重要的目录(一些不太重要的目录省略了)。 /var/catman：包括了格式化过的帮助(man)页。帮助页的源文件一般存在 /usr/man/catman中；有些man页可能有预格式化的版本，存在/usr/man/cat中。而其他的man页在第一次看时都需要格式化，格 式化完的版本存在/var/man中，这样其他人再看相同的页时就无须等待格式化了。(/var/catman经常被 清除，就像清除临时目录一样。) /var/lib：存放系统正常运行时要改变的文件。 /var/local：存放/usr/local中 安装的程序的可变数据(即系统管理员安装的程序)。注意，如果必要，即使本地安装的程序也会使用其他/var目录，例如/var/lock 。 /var/lock：锁定文件。许多程序遵循在/var/lock中 产生一个锁定文件的约定，以用来支持他们正在使用某个特定的设备或文件。其他程序注意到这个锁定文件时，就不会再使用这个设备或文件。 /var/log：各种程序的日志(log)文件，尤其是login (/var/log/wtmplog纪 录所有到系统的登录和注销) 和syslog (/var/log/messages 纪录存储所有核心和系统程序信息)。/var/log 里的文件经常不确定地增长，应该定期清除。 /var/run：保存在下一次系统引导前有效的关于系统的信息文件。例如，/var/run/utmp包 含当前登录的用户的信息。 /var/spool：放置“假脱机(spool)”程序的目录，如mail、 news、打印队列和其他队列工作的目录。每个不同的spool在/var/spool下有自己的子目录，例如，用户的邮箱就存放在/var/spool/mail 中。 /var/tmp：比/tmp允许更大的或需要存在较长时间的临时文件。注意系统管理 员可能不允许/var/tmp有很旧的文件。 /proc文件系统/proc文件系统是一个伪的文件系统，就是说它是一个实际上不存在的目录，因而这是一个非 常特殊的目录。它并不存在于某个磁盘上，而是由核心在内存中产生。这个目录用于提供关于系统的信息。下面说明一些最重要的文件和目录(/proc文件系统 在proc man页中有更详细的说明)。 /proc/x：关于进程x的信息目录，这x是这一进程的标识号。每个进程在 /proc下有一个名为自己进程号的目录。 /proc/cpuinfo：存放处理器(cpu)的信息，如cpu的类型、制造商、 型号和性能等。 /proc/devices：当前运行的核心配置的设备驱动的列表。 /proc/dma：显示当前使用的dma通道。 /proc/filesystems：核心配置的文件系统信息。 /proc/interrupts：显示被占用的中断信息和占用者的信息，以及被占用 的数量。 /proc/ioports：当前使用的i/o端口。 /proc/kcore：系统物理内存映像。与物理内存大小完全一样，然而实际上没有 占用这么多内存；它仅仅是在程序访问它时才被创建。(注意：除非你把它拷贝到什么地方，否则/proc下没有任何东西占用任何磁盘空间。) /proc/kmsg：核心输出的消息。也会被送到syslog。 /proc/ksyms：核心符号表。 /proc/loadavg：系统“平均负载”；3个没有意义的指示器指出系统当前 的工作量。 /proc/meminfo：各种存储器使用信息，包括物理内存和交换分区 (swap)。 /proc/modules：存放当前加载了哪些核心模块信息。 /proc/net：网络协议状态信息。 /proc/self：存放到查看/proc的 程序的进程目录的符号连接。当2个进程查看/proc时，这将会是不同的连接。这主要便于程序得到它自己的进程目录。 /proc/stat：系统的不同状态，例如，系统启动后页面发生错误的次数。 /proc/uptime：系统启动的时间长度。 /proc/version：核心版本。 /usr/local一般是你安装软件的目录，这个目录就相当于在windows下的programefiles这个目录 /opt这个目录是一些大型软件的安装目录，或者是一些服务程序的安装目录 举个例子：刚才装的测试版firefox，就可以装到/opt/firefox_beta目录下，/opt/firefox_beta目录下面就包含了运 行firefox所需要的所有文件、库、数据等等。要删除firefox的时候，你只需删除/opt/firefox_beta目录即可，非常简单。 /usr/local这里主要存放那些手动安装的软件，即 不是通过“新立得”或apt-get安装的软件 。 它和/usr目录具有相类似的目录结构 。让软件包管理器来管理/usr目录，而把自定义的脚本(scripts)放到/usr/local目录下面，我想这应该是个不错的主意。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"目录","slug":"目录","permalink":"http://example.com/tags/%E7%9B%AE%E5%BD%95/"}]},{"title":"故障处理的惯用思路","slug":"故障处理的惯用思路","date":"2018-02-28T06:06:02.000Z","updated":"2021-01-27T02:25:26.056Z","comments":true,"path":"2018/02/28/故障处理的惯用思路/","link":"","permalink":"http://example.com/2018/02/28/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%9A%84%E6%83%AF%E7%94%A8%E6%80%9D%E8%B7%AF/","excerpt":"","text":"【报障来源及描述】 【明确之故障现象】 【定位分析并验证】 【处理方法与结果】 【经验教训和精华】 以上5段虽是我们故障报告的格式，但其实是总结了故障处理的惯用思路： 【报障来源及描述】了解这些，是为了做形势分析。———这也是《5个优势思维》里边的第一条，对重要性和紧急性有个初步判断。 比如，老板报障的？付费客户报障的？客服收到大量投诉报障的？监控告警主动发现的？ 或是个别用户个别同事反映的？ 【明确之故障现象】对故障现象的本身明确清楚，是为了聚焦排障范围，也避免兜转一圈回来才发现方向查错（比如人家其实不是乐视主站挂了，而是百度QQ全都打不开）。 明确故障现象的方法就是问4个W： What ——— 具体现象是什么？有报错提示内容？同类对比正常吗？能提供操作方法或URL或截图或抓包？ Who ——— 是谁发现类似现象？ 还有其他人吗？ 哪位方便我们联系抓包或测试？ Where ——— 位置是在公司还是哪里？ 哪个型号终端？ 哪个版本？ 没绑定hosts吧？ When ——— 什么时候发现的？ 一直这样吗还是突然变这样的？ 【定位分析并验证】首先需要定位：这只是个案？还是局部故障？还是全局故障？ 分析方法1：手里有条件就立马快速上线，查看各个日志等 分析方法2：查看带宽、QPS、负载 的曲线图，通过环比+同比、观察变化的趋势和影响的幅度。如果带宽/请求数/连接数/负载出现陡升或陡降那很可能就是发生了故障，否则可能是个案或局部。 分析方法3：对比事发时间点的前后日志，按error_log 和access_log 中各字段归类排序取top10，找出规律共性（比如增长的请求或错误的请求被发现集中在某接口、某IP、某UA？） 分析方法4：缩小范围到某设备或某进程后，使用抓包工具分析，比如lsof、strace、tcpdump、ping、traceroute/tcptraceroute/mtr 有了分析推测后，再做相应调整。验证效果，观察各曲线变化及各日志的反馈。 【处理方法与结果】故障的处理方法分短期和长期。 短期的调整需要使得当前故障快速缓解并恢复，不能等待影响扩散蔓延，必要情况就电话上级争取资源协调支持。 长期的优化方案可能涉及架构调整、扩容建设、功能开发改造等，需要投入较大的设备资源和人力资源。 【经验教训和精华】唯有思维质量才是我们的核心竞争力，不是所谓的“经验”。 而善于总结经验教训，才能提升我们的思维质量。 否则，同类的坑你今天掉完，往后还会再掉进去的。","categories":[{"name":"Other","slug":"Other","permalink":"http://example.com/categories/Other/"}],"tags":[{"name":"思路","slug":"思路","permalink":"http://example.com/tags/%E6%80%9D%E8%B7%AF/"}]},{"title":"曾国藩：一勤天下无难事","slug":"曾国藩：一勤天下无难事","date":"2018-02-28T05:35:33.000Z","updated":"2021-01-27T02:25:26.056Z","comments":true,"path":"2018/02/28/曾国藩：一勤天下无难事/","link":"","permalink":"http://example.com/2018/02/28/%E6%9B%BE%E5%9B%BD%E8%97%A9%EF%BC%9A%E4%B8%80%E5%8B%A4%E5%A4%A9%E4%B8%8B%E6%97%A0%E9%9A%BE%E4%BA%8B/","excerpt":"","text":"曾国藩：一勤天下无难事 曾国藩说为官者当有五勤： 一曰身勤：险远之路，身往验之；艰苦之境，身亲尝之。 二曰眼勤：遇一人，必详细察看；接一文，必反复审阅。 三曰手勤：易弃之物，随手收拾；易忘之事，随笔记载。 四曰口勤：待同僚，则互相规劝；待下属，则再三训导。 五曰心勤：精诚所至，金石亦开；苦思所积，鬼神迹通。 曾国藩的“五勤”之道虽是为官之道，同时也是为人处世之道： 一曰身勤曾国藩所说的“身勤”就是身体力行、以身作则。 曾国藩曾说“余谓天子或可不亲细事，为大臣者则断不可不亲”。 曾国藩是这么说的，也是这么做的。曾国藩在军中要求自己早起，不论是什么样的天气，不论是什么样的环境，他一定“闻鸡起舞”，练兵督训，办理各项事务。曾国藩对军中将士说：“练兵之道，必须官弁昼夜从事，乃可渐几于熟。如鸡孵卵，如炉炼丹，未可须臾稍离。” 《论语》有曰：“其身正，不令而行；其身不正，虽令不从。” 言传不如身教，曾国藩就是这样影响手下的幕僚、将领的。不管是个人修行还是管理团队，这一点至关重要，要给周围的人和下属做一个好榜样。 二曰眼勤曾国藩所说的“眼勤”是从细微之处识人。 曾国藩指派李鸿章训练淮军时，李鸿章带了三个人求见，请曾国藩分配职务给他们。不巧曾刚好饭后出外散步，李命三人在室外等候，自己则进入室内。等到曾散步回来，李请曾传见三人。 曾说不用再召见了，并对李说：“站在右边的是个忠厚可靠的人，可委派后勤补给工作；站在中间的是个阳奉险违之人，只能给他无足轻重的工作；站在左边的人是个上上之材，应予重用。” 李惊问道：“您是如何看出来的呢？” 曾笑道：“刚才我散步回来，走过三人的面前时，右边那人垂首不敢仰视，可见他恭谨厚重，故可委派补给工作。中间那人表面上必恭必敬，但我一走过，立刻左顾右盼，可见他阳奉阴违，故不可用。左边那人始终挺直站立，双目正视，不亢不卑，乃大将之材。” 曾国藩所指左边那位“大将之材”，就是后来担任台湾巡抚鼎鼎有名的刘铭传。 曾国藩从细微之处识人，练就了他的一双慧眼，曾府幕僚鼎盛一时，幕僚在曾国藩平定太平军的过程中出谋划策，立下了赫赫功勋。 三曰手勤曾国藩所说的“手勤”其实就是要养成一个好习惯。 曾国藩一生养成了三个好习惯： 一是反省的习惯：曾国藩每一天都写日记，曾国藩说：“吾人只有进德、修业两事靠得住。进德，则孝弟仁义是也；修业，则诗文作字是也。此二者由我作主，得尺则我之尺也，得寸则我之寸也。今日进一分德，便算积了一升谷；明日修一分业，又算馀了一文钱；德业并增，则家私日起。至于功名富贵，悉由命走，丝毫不能自主。”曾国藩通过写日记进行修身，反思自己在为人处世等方面存在的不足，通过这样的反省，不断修炼自己。 第二个好习惯就是读书习惯，他规定自己每一天必须坚持看历史不下十页，饭后写字不下半小时。曾国藩说“人之气质，由于天生，很难改变，唯读书则可以变其气质。古之精于相法者，并言读书可以变换骨相。”通过坚持读书，曾国藩不仅改变了气质，更磨练了他持之以恒的精神，同时也增长了他的才干，懂得不少为人处世的道理，也让他成了一代大儒。 第三个好习惯就是写家书，据说曾国藩仅在1861年就写了不下253封家书，通过写家书不断训导教育弟弟和子女，在曾国藩的言传身教之下，曾家后人人才辈出。 正所谓习惯决定性格，性格决定命运。曾国藩养成很好的习惯，不仅成就了曾国藩自己，也影响了曾家后人。 四曰口勤曾国藩的“口勤”就是他与人的相处之道。 曾国藩认为同僚相处“两虎相斗，胜者也哀”。 据说曾国藩开始同湖南巡抚骆秉章的关系并不好，咸丰三年，曾国藩在长沙初办团练时，骆秉章压根儿就没把曾国藩放在眼里，对曾国藩的工作也不是十分支持。当绿营与团练闹矛盾时，他总是把偏向着绿营。 让曾国藩特别愤愤不平的是，在靖港兵败，湘军退驻长沙城郊的水陆洲时，骆秉章来到离曾国藩座船仅数十米之遥的码头送客，曾国藩以为他是特意来看望和安慰自己的，内心正十分感激，谁知他送完客人之后竟然转身便走，就当没有看到曾国藩！并且还同长沙官员一起对曾国藩的兵败百般讥讽。 尽管如此，曾国藩并没有逞口舌之争，而是采取曲意忍让的态度，在他为父守孝后第二次出山之时，他特意拜访了骆秉章，态度十分谦恭又十分热情，之前的那点事就当没发生一样。这让骆秉章大感意外，当场表态，以后湘军有什么困难，我们湖南当倾力相助。 “己预立而立人，己欲达而达人”，曾国藩口勤不仅仅是对同僚和上级，对下属也会耐心地训导，曾国藩秉持的这种为人处世之道，不仅让他成就了自己，也成就了如李鸿章、左宗棠、张之洞、刘铭传、胡林翼等名臣，实现了清末短暂的中兴。 五曰心勤曾国藩所说的“心勤”其实就是坚定的意志品质。 曾国藩不管是从科考还是在平定太平军时“屡败屡战”，都有一种精诚所至的信念在支撑他。从各方面下足工夫，功到自然成。 曾国藩说“天下古今之庸人，皆以一“惰”字致败。”以勤治惰，以勤治庸，不管是修身自律，还是为人处世，一勤天下无难事。","categories":[{"name":"Other","slug":"Other","permalink":"http://example.com/categories/Other/"}],"tags":[{"name":"曾国藩","slug":"曾国藩","permalink":"http://example.com/tags/%E6%9B%BE%E5%9B%BD%E8%97%A9/"}]},{"title":"Pwgen随机生成密码","slug":"pwgen随机生成密码","date":"2018-02-25T20:22:13.000Z","updated":"2021-01-27T02:25:26.051Z","comments":true,"path":"2018/02/26/pwgen随机生成密码/","link":"","permalink":"http://example.com/2018/02/26/pwgen%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E5%AF%86%E7%A0%81/","excerpt":"","text":"安装 Centos 7.5的系统可以使用yum安装： yum install -y pwgen Ubuntu 系统安装： apt-get install -y pwgen 常用组合pwgen - generate pronounceable passwords常用命令可指定为： pwgen -sy -1 18 参数详解-0, --no-numerals 不要在生成的密码中包含数字。 Don&#39;t include numbers in the generated passwords. -1 打印生成的密码每行一个。 Print the generated passwords one per line. -A, --no-capitalize 不包含任何大写字母。 Don&#39;t bother to include any capital letters in the generated passwords. -a, --alt-phonics 此选项没有做任何特殊的，它只存在向后兼容性。 This option doesn&#39;t do anything special; it is present only for backwards compatibility. -B, --ambiguous 不要使用打印时可能被用户混淆的字符，如“L”和“1”或“0”或“O”.。这大大减少了可能的密码的数量，并因此降低了质量的通行证—话.对于视力不好的用户可能是有用的，但一般不推荐使用此选项.。 Don&#39;t use characters that could be confused by the user when printed, such as &#39;l&#39; and &#39;1&#39;, or &#39;0&#39; or &#39;O&#39;.This reduces the number of possible passwords significantly, and as such reduces the quality of the pass-words. It may be useful for users who have bad vision, but in general use of this option is not recommended. -c, --capitalize 在密码中至少包含一个大写字母。这是默认如果标准输出是一个tty设备。 Include at least one capital letter in the password. This is the default if the standard output is a tty device. -C 在列中打印生成的密码。这是默认如果标准输出是一个tty设备。 Print the generated passwords in columns. This is the default if the standard output is a tty device. -N, --num-passwords=num 生成数字密码。这个默认是全屏如果密码打印的列，和一个密码。 Generate num passwords. This defaults to a screenful if passwords are printed by columns, and one password. -n, --numerals 在密码中至少包含一个号码。这是默认如果标准输出是一个tty设备。 Include at least one number in the password. This is the default if the standard output is a tty device. -H, --sha1=/path/to/file[#seed] 将使用SHA1的散列特定文件和可选的种子创建密码。它会让你计算出相同的密码后，如果你记得文件，种子，和pwgen的选项。即：pwgen H ~ / your_favorite。MP3 # your@email.com给您的POP3帐户可能的密码列表，你可以问这个列表一次又一次。 Will use the sha1&#39;s hash of given file and the optional seed to create password. It will allow you to compute the same password later, if you remember the file, seed, and pwgen&#39;s options used. ie: pwgen -H ~/your_favorite.mp3#your@email.com gives a list of possibles passwords for your pop3 account, and you can ask this list again and again. 警告：使用此选项生成的密码不是非常随机的。如果使用此选项，请确保攻击者无法获取文件的副本.。另外，值得注意的是，该文件的名称可以很容易地从~ /可用。历史或~ / bash_history文件。 WARNING: The passwords generated using this option are not very random. If you use this option, make sure the attacker can not obtain a copy of the file. Also, note that the name of the file may be easily available from the ~/.history or ~/.bash_history file. -h, --help 查看帮助消息。 Print a help message. -s, --secure 生成完全随机，难以记忆的密码。这些应该只用于机器密码，否则，它几乎可以保证，用户将只需写密码的一张纸条上的显示器… Generate completely random, hard-to-memorize passwords. These should only be used for machine passwords, since otherwise it&#39;s almost guaranteed that users will simply write the password on a piece of paper taped to the monitor... -v, --no-vowels 生成随机密码，不包含元音或数字，可能被误认为元音。它提供了更安全的密码支持允许系统管理员不必担心随机密码不小心包含进攻的子串。 Generate random passwords that do not contain vowels or numbers that might be mistaken for vowels. It pro- vides less secure passwords to allow system administrators to not have to worry with random passwords accidentally contain offensive substrings. -y, --symbols 在密码中至少包含一个特殊字符。 Include at least one special character in the password.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"命令","slug":"命令","permalink":"http://example.com/tags/%E5%91%BD%E4%BB%A4/"},{"name":"pwgen","slug":"pwgen","permalink":"http://example.com/tags/pwgen/"}]},{"title":"LVS负载均衡","slug":"LVS负载均衡","date":"2018-02-25T01:14:02.000Z","updated":"2021-01-27T02:25:26.039Z","comments":true,"path":"2018/02/25/LVS负载均衡/","link":"","permalink":"http://example.com/2018/02/25/LVS%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","excerpt":"","text":"阅读目录 基础知识介绍 二、基础知识及一些要点 三、LVS 性能调优 1 通过NAT实现虚拟服务器（VS/NAT） 2 通过IP隧道实现虚拟服务器（VS/TUN） 3 通过直接路由实现虚拟服务器（VS/DR） Load Balancer(负载均衡器)：Load Balancer是整个集群系统的前端，负责把客户请求转发到Real Server上。Load Balancer通过Ldirectord监测各Real Server的健康状况。在Real Server不可用时把它从群中剔除，恢复时重新加入。Backup是备份Load Balancer，当Load Balancer不可用时接替它，成为实际的Load Balancer。 Server Array(服务器群)：Server Array是 一组运行实际应用服务的机器，比如WEB, Mail, FTP, DNS, Media等等。在实际应用中，Load Balancer和Backup也可以兼任Real Server的角色。以下的测试就是一台服务器既担任了LVSserver,同时也是realserver节点. Shared Storage(共享存储)： Shared Storage为所有Real Server提供共享存储空间和一致的数据内容。 Director: 前端负载均衡器即运行LVS服务可以针对web、ftp、cache、mms甚至mysql等服务做load balances。 RealServer: 后端需要负载均衡的服务器，可以为各类系统，Linux、Solaris、Aix、BSD、Windows都可，甚至Director本身也可以作为 RealServer使用. LVS( Linux Virtual Server),Linux下的负载均衡器，支持LVS-NAT、 LVS-DR、LVS-TUNL三种不同的方式: nat用的不是很多，主要用的是DR、TUNL方式。 DR方式适合所有的RealServer同一网段下，即接在同一个交换机上. TUNL方式就对于RealServer的位置可以任意了，完全可以跨地域、空间，只要系统支持Tunnel就可以,方便以后扩充的话直接Tunl方式即可 一、基础知识介绍1、LVS基础及介绍LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩（目前就职于阿里）博士成立，是中国国内最早出现的自由软件项目之一。目前有三种IP负载均衡技术（VS/NAT、VS/TUN和VS/DR）；十种调度算法（rrr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq）【参考资料:】1)官方中文参考资料: http://www.linuxvirtualserver.org/zh/index.html 2)LinuxTone 相关LVS技术档汇总: http://bbs.linuxtone.org/thread-1191-1-1.html 2、 LVS 三种IP负载均衡技术对比三种IP负载均衡技术的优缺点归纳在下表中： null VS/NAT VS/TUN VS/DR Server any Tunneling Non-arp device server network private LAN/WAN LAN server number low (10~20) High (100) High (100) server gateway load balancer own router Own router 【注】 以上三种方法所能支持最大服务器数目的估计是假设调度器使用100M网卡，调度器的硬件配置与后端服务器的硬件配置相同，而且是对一般Web服务。使用更 高的硬件配置（如千兆网卡和更快的处理器）作为调度器，调度器所能调度的服务器数量会相应增加。当应用不同时，服务器的数目也会相应地改变。所以，以上数 据估计主要是为三种方法的伸缩性进行量化比较。 3、LVS目前实现的几种调度算法IPVS 在内核中的负载均衡调度是以连接为粒度的。在HTTP协议（非持久）中，每个对象从WEB服务器上获取都需要建立一个TCP连接，同一用户的不同请求会被 调度到不同的服务器上，所以这种细粒度的调度在一定程度上可以避免单个用户访问的突发性引起服务器间的负载不平衡。在内核中的连接调度算法上，IPVS已实现了以下十种调度算法： 轮叫调度（Round-Robin Scheduling） 加权轮叫调度（Weighted Round-Robin Scheduling） 最小连接调度（Least-Connection Scheduling） 加权最小连接调度（Weighted Least-Connection Scheduling） 基于局部性的最少链接（Locality-Based Least Connections Scheduling） 带复制的基于局部性最少链接（Locality-Based Least Connections with Replication Scheduling） 目标地址散列调度（Destination Hashing Scheduling） 源地址散列调度（Source Hashing Scheduling） 最短预期延时调度（Shortest Expected Delay Scheduling） 不排队调度（Never Queue Scheduling） 对应: rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq, Ldirecotrd配置选项及ipvsadm使用参数. ldirectord配置选项 ipvsadm使用的参数 ipvsadm -L的输出 LVS转发方法 gate -g Route LVS-DR ipip -i Tunnel LVS-TUN masq -m Masq LVS-NAT 4、集群架构时我们应该采用什么样的调度算法?在一般的网络服务（如HTTP和Mail Service等）调度中，我会使用加权最小连接调度wlc或者加权轮叫调度wrr算法。 基于局部性的最少链接LBLC和带复制的基于局部性最少链接LBLCR主要适用于Web Cache集群。 目标地址散列调度和源地址散列调度是用静态映射方法，可能主要适合防火墙调度。 最短预期延时调度SED和不排队调度NQ主要是对处理时间相对比较长的网络服务。 其实，它们的适用范围不限于这些。我想最好参考内核中的连接调度算法的实现原理，看看那种调度方法适合你的应用。 5、LVS的ARP问题2.4.x kernels: Hidden Patch arptable iptables 2.6.x kernels: （关闭arp查询响应请求） net.ipv4.conf.eth0.arp_ignore = 1 net.ipv4.conf.eth0.arp_announce = 2 net.ipv4.conf.all.arp_ignore = 1 net.ipv4.conf.all.arp_announce = 2 arping tools 二、基础知识及一些要点1、InActConn并不代表错误连接，它是指不活跃连接(Inactive Connections)，我们将处于TCP ESTABLISH状态以外的连接都称为不活跃连接，例如处于SYN_RECV状态的连接，处于TIME_WAIT状态的连接等。 2、用四个参数来关闭arp查询响应请求：echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce 3、ipvsadm -L -n –statsProt LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes连接数 输入包 输出包 输入流量 输出流量 4、注意事项: 1）在LVS方案中，虚拟ip地址与普通网络接口大大不同，这点需要特别注意。虚拟ip地址的广播地址是它本身，子网掩码是255.255.255.255。 为什么要这样呢？因为有若干机器要使用同一个ip地址，用本身做广播地址和把子网掩码设成4个255就不会造成ip地址冲突了,否则lvs将不能正常转发访问请求。 2） 假如两台VS之间使用的互备关系，那么当一台VS接管LVS服务时，可能会网络不通，这时因为路由器的MAC缓存表里关于vip这个地址的MAC地 址还是被替换的VS的MAC，有两种解决方法，一种是修改新VS的MAC地址，另一种是使用send_arp 命令（piranha软件包里带的一个小工具） 格式如下： send_arp: send_arp [-i dev] src_ip_addr src_hw_addr targ_ip_addr tar_hw_addr这个命令不一定非要在VS上执行，只+要在同一VLAN即可。 /sbin/arping -f -q -c 5 -w 5 -I eth0 -s WEBVIP−UGW 5、Virtual Server via Direct Routing（VS/DR）VS/DR 通过改写请求报文的MAC地址，将请求发送到真实服务器，而真实服务器将响应直接返回给客户。 同VS/TUN技术一样，VS/DR技术可极大地提高集群系 统的伸缩性。这种方法没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器与真实服务器都有一块网卡连在同一物理 网段上。 6. LVS 经验:1). LVS调度的最小单位是“连接”。 2). 当apache的KeepAlive被设置成Off时，“连接”才能被较均衡的调度。 3). 在不指定-p参数时，LVS才真正以“连接”为单位按“权值”调度流量。 4). 在指定了-p参数时，则一个client在一定时间内，将会被调度到同一台RS。 5). 可以通过”ipvsadm ?set tcp tcpfin udp”来调整TCP和UDP的超时，让连接淘汰得快一些。 6). 在NAT模式时，RS的PORT参数才有意义。 7). DR和TUN模式时，InActConn 是没有意义的(Thus the count in the InActConn column for LVS-DR, LVS-Tun isinferred rather than real.) /sbin/arping -f -q -c 5 -w 5 -I eth0 -s WEBVIP−UGW 三、LVS 性能调优Least services in System or Compile kernel. Performace Tuning base LVS:LVS self tuning( ipvsadm Timeout (tcp tcpfin udp)). ipvsadm -Ln --timeout Timeout (tcp tcpfin udp): 900 120 300 ipvsadm --set tcp tcpfin udp Improving TCP/IP performance net.ipv4.tcp_tw_recyle=1 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_max_syn_backlog=8192 net.ipv4.tcp_keepalive_time=1800 net.ipv4.tcp_fin_timeout=30 net.core.rmem_max=16777216 net.core.wmem_max=16777216 net.ipv4.tcp_rmem=4096 87380 16777216 net.ipv4.tcp_wmem=4096 65536 16777216 net.core.netdev_max_backlog=3000 1 通过NAT实现虚拟服务器（VS/NAT）NAT的工作原理是报文头（目标地址、源地址和端口等）被正确改写后，客户相信它们连接一个IP地址，而不同IP地址的服务器组也认为它们是与客户直接相连的。 可以用NAT方法将不同IP地址的并行网络服务变成在一个IP地址上的一个虚拟服务。 客户通过Virtual IP Address（虚拟服务的IP地址）访问网络服务时，请求报文到达调度器，调度器根据连接调度算法从一组真实服务器中选出一台服务器，将报文的目标地址 Virtual IP Address改写成选定服务器的地址，报文的目标端口改写成选定服务器的相应端口，最后将修改后的报文发送给选出的服务器。同时，调度器在连接Hash 表中记录这个连接，当这个连接的下一个报文到达时，从连接Hash表中可以得到原选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务 器。当来自真实服务器的响应报文经过调度器时，调度器将报文的源地址和源端口改为Virtual IP Address和相应的端口，再把报文发给用户。在连接上引入一个状态机，不同的报文会使得连接处于不同的状态，不同的状态有不同的超时值。在TCP连接中，根据标准的TCP有限状态机进行状态迁移； 在UDP中，我们只设置一个UDP状态。不同状态的超时值是可以设置的，在缺省情况下，SYN状态的超时为1分钟，ESTABLISHED状态的超时为 15分钟，FIN状态的超时为1分钟；UDP状态的超时为5分钟。当连接终止或超时，调度器将这个连接从连接Hash表中删除。 2 通过IP隧道实现虚拟服务器（VS/TUN）大多数Internet服务都有这样的特点：请求报文较短而响应报文往往包含大量的数据。如果能将请求和响应分开处理，即在负载调度器中只负责调度请求而响应直接返回给客户，将极大地提高整个集群系统的吞吐量。调度器根据各个服务器的负载情况，动态地选择一台服务器，将请求报文封装在另一个IP报文中，再将封装后的IP报文转发给选出的服务器；服务器收到报文 后，先将报文解封获得原来目标地址为VIP的报文，服务器发现VIP地址被配置在本地的IP隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直 接返回给客户。响应报文根据服务器的路由表直接返回给客户，而不经过负载调度器，所以负载调度器只处于从客户到服务器的半连接中 3 通过直接路由实现虚拟服务器（VS/DR）VS/DR利用大多数Internet服务的非对称特点，负载调度器中只负责调度请求，而服务器直接将响应返回给客户，可以极大地提高整个集群系统的吞吐量在VS/DR中，调度器根据各个服务器的负载情况，动态地选择一台服务器，不修改也不封装IP报文，而是将数据帧的MAC地址改为选出服务器的MAC地址， 再将修改后的数据帧在与服务器组的局域网上发送。因为数据帧的MAC地址是选出的服务器，所以服务器肯定可以收到这个数据帧，从中可以获得该IP报文。当 服务器发现报文的目标地址VIP是在本地的网络设备上，服务器处理这个报文，然后根据路由表将响应报文直接返回给客户。 xx VS/NAT VS/TUN VS/DR Server any Tuneling Non-arp device server number low10-20 high100 high100 server gateway loadbalancer own router own router server network private lan/wan lan NAT 当服务器数目变多时，调度器成为瓶颈 IP Tuneling 术对服务器有要求，即所有的服务器必须支持“IP Tunneling”者“IPEncapsulation ”协议 DR 跟VS/TUN相比，这种方法没有IP隧道的开销，但是要求负载调度器与实际服务器都有一块网卡连在同一物理网段上，服务器网络设备（或者设备别名）不作ARP响应，或者能将报文重定向（Redirect）到本地的Socket端口上。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"LVS","slug":"LVS","permalink":"http://example.com/tags/LVS/"}]},{"title":"Puppet基本流程","slug":"puppet基本流程","date":"2018-02-24T21:30:50.000Z","updated":"2021-01-27T02:25:26.050Z","comments":true,"path":"2018/02/25/puppet基本流程/","link":"","permalink":"http://example.com/2018/02/25/puppet%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/","excerpt":"","text":"1. 概述puppet是一个开源的软件自动化配置和部署工具，它使用简单且功能强大，正得到了越来越多地关注，现在很多大型IT公司均在使用puppet对集群中的软件进行管理和部署，如google利用puppet管理超过6000台地mac桌面电脑（2007年数据）。本文主要介绍puppet安装方法，设计架构及使用方法。 2. 设计架构puppet是基于c/s架构的。服务器端保存着所有对客户端服务器的配置代码，在puppet里面叫做manifest. 客户端下载manifest之后，可以根据manifest对服务器进行配置，例如软件包管理，用户管理和文件管理等等。 如上图所示，puppet的工作流程如下： （1）客户端puppetd调用facter，facter探测出主机的一些变量，例如主机名，内存大小，ip地址等。pupppetd 把这些信息通过ssl连接发送到服务器端； （2）服务器端的puppetmaster 检测客户端的主机名，然后找到manifest里面对应的node配置， 并对该部分内容进行解析，facter送过来的信息可以作为变量处理，node牵涉到的代码才解析，其他没牵涉的代码不解析。解析分为几个阶段，语法检查，如果语法错误就报错。如果语法没错，就继续解析，解析的结果生成一个中间的“伪代码”，然后把伪代码发给客户端； （3）客户端接收到“伪代码”，并且执行，客户端把执行结果发送给服务器； （4）服务器端把客户端的执行结果写入日志。 puppet工作过程中有两点值得注意，第一，为了保证安全，client和master之间是基于ssl和证书的，只有经master证书认证的client可以与master通信；第二，puppet会让系统保持在你所期望的某种状态并一直维持下去，如检测某个文件并保证其一直存在，保证ssh服务始终开启，如果文件被删除了或者ssh服务被关闭了，puppet下次执行时（默认30分钟），会重新创建该文件或者启动ssh服务。 3. 软件安装不推荐使用apt-get命令进行安装，因为该命令下载的puppet存在bug。可直接从源代码进行安装，需要安装的软件有ruby，facter和puppet。 3.1 安装步骤编辑/etc/host以修改主机名，因为puppet是基于证书的，证书中包含主机名；在master和slave上依次安装ruby、facter和puppet，安装facter和puppet时，要使用ruby install.rb。 3.2 安装后的目录结构(1) 安装目录安装目录默认存为/etc/puppet，该目录下的manifests存放manifest文件。其他可执行文件在/user/sbin下，主要有： puppet： 用于执行用户所写独立的mainfests文件，如： puppet -l /tmp/manifest.log manifest.pp puppetd： 运行在被管理主机上的客户端程序，如： puppet –server servername –waitforcert 60 puppetmasterd：运行在管理机上的服务器程序，如： puppetmasterd –debug puppetca puppet认证程序，主要用于对slave的证书进行认证，如：查看需认证的slave： puppetca –list 对这些slave进行认证： puppetca -s –a puppetrun 用于连接客户端，强制运行本地配置文件，如： puppetrun -p 10 –host host1 –host host2 -t remotefile -t webserver (2) 配置文件puppet.confPuppet的主配置文件，如果是root用户，配置文件为/etc/puppet/puppet.conf，普通用户，配置文件为：~user/.puppet/puppet.conf 具体配置参数，参见：http://docs.puppetlabs.com/references/stable/configuration.html#configuration-files fileserver.confpuppet文件服务器的配置文件。用path配置文件路径，allow/deny配置访问权限 具体参见：http://docs.puppetlabs.com/guides/file_serving.html 3.3 验证安装是否成功选定一个slave与master进行验证，假设slave的host为slave00，master的host为masterhost，在slave00上输入： puppetd –test –server servername 然后在masterhost上查看待认证的slave： puppetca –list 如果没问题的话，此时可以看到slave00，对该slave的证书进行签名： puppetca -s -a 这样slave00通过了证书验证，可以与master进行进一步交互了。在masterhost的/etc/puppet/manifests目录下编写site.pp文件，内容如下： node default &#123; file &#123; “/tmp/test”: content=&gt;”hello\\n”, mode =&gt; 0644; &#125; &#125; 同时在slave00上输入：puppetd –test –server servername， 查看slave00的/tmp文件夹，生成了一个新文件test，里面的内容是hello，该文件的权限是-rw-r—r—。这样，便证明puppet安装成功，如果出现错误，查看第六节。 4. 配置脚本编写本节介绍puppet的配置脚本编写方法，主要是指puppet的manifest编写方法。puppet把需要管理的内容抽象成为资源，每种资源有不同的属性，因此puppet语言就是描述这些资源的属性以及资源之间关系的语言。为了便于管理，puppet将资源模块化，即每个功能模块的manifest单独放在一个目录下。每个模块包含一个主要的manifest文件（init.pp，它是模块的入口，类似于C语言中的main函数），里面包含若干个class对该模块的资源进行封装，常见的资源有file，package，service等，每种资源由自己的属性，如file有属性name，owner，mode等。本节主要介绍puppet中manifest的编写方法，将依次介绍资源属性，资源，节点管理，函数和模块的编写方法。 4.1 资源属性资源属性有两种，一种是资源专属属性，另一种是资源共同属性，对于资源专属属性，将在下一节介绍；而资源共同属性是所有资源共有的属性，主要有：before用于控制不同对象（资源）的执行顺序关系，表示某个对象（资源）在另一个对象之后发生（require与之相反，它表示之前发生）。如： file &#123; “/var/nagios/configuration”: source =&gt; “…”, recurse =&gt; true, before =&gt; Exec[&quot;nagios-rebuid&quot;] &#125; exec &#123; “nagios-rebuild”: command =&gt; “/usr/bin/make”, cwd =&gt; “/var/nagios/configuration” &#125; 这段代码保证用make编译之前，所有代码都是最新的。也可以before多个资源，如： before =&gt; [ File[&quot;/usr/local&quot;], File[&quot;/usr/local/scripts&quot;] ] subscribe 检测某个资源，当它发生变化时，该资源会重新加载，如： class nagios &#123; file &#123; “/etc/nagios/nagios.conf”: source =&gt; “puppet://server/module/nagios.conf”, alias =&gt; nagconf # just to make things easier for me &#125; service &#123; nagios: ensure =&gt; running, subscribe =&gt; File[nagconf] &#125; &#125; 当检测到文件nagconf被修改时，服务nagios会相应的更新。需要注意的是，目前支持subscribe的资源只有exec，service和mount。 更多资料，参见：http://docs.puppetlabs.com/references/latest/metaparameter.html 4.2 资源常用的资源主要有以下几个： file：文件管理 package：软件包管理 service：系统服务管理 cron：配置定期任务 exec：运行shell命令 (1) file资源更详细资料，可参见：http://puppet.wikidot.com/file (2) package资源更详细资料，可参见：http://puppet.wikidot.com/package (3) service资源更详细资料，可参见：http://puppet.wikidot.com/srv (4) exec资源更详细资料，可参见：http://puppet.wikidot.com/exec (5) cron资源更详细资料，可参见：http://puppet.wikidot.com/cron 4.3 节点管理puppet如何区分不同的客户端，并且给不同的服务端分配manifest呢？puppet使用node资源做这件事情，node 后面跟客户端的主机名，例如： node ‘ slave00 ‘ &#123; include ssh &#125; node ‘ slave11 ‘ &#123; $networktype=”tele” $nagioscheckport=”80,22,3306″ include apache, mysql, php &#125; 资源node中可使用变量，也可直接通过include把其他manifest包含进来。 更详细资料，可参见：http://docs.puppetlabs.com/references/latest/type.html 4.4 类和函数类可以把多个相关的资源定义在一起,组成一个类。类可以继承，具体参见：http://docs.puppetlabs.com/guides/language_guide.html#resource-collections函数（在puppet中称为“defination”）可以把多个资源包装成一个资源，或者把一个资源包装成一个模型，便于使用。例如，在debian里面管理一个apache虚拟机非常简单,把一个虚拟主机的配置文件放到/etc/sites-available/里面,然后做一个符号链接到/etc/sites-enabled目录。 你可以为你每个虚拟主机复制同样的配置代码，但是如果你使用下面的代码就会更好和更简单： define virtual_host($docroot, $ip, $order = 500, $ensure = “enabled”) &#123; $file = “/etc/sites-available/$name.conf” # The template fills in the docroot, ip, and name. file &#123; $file: content =&gt; template(“virtual_host.erb”), notify =&gt; Service[apache] &#125; file &#123; “/etc/sites-enabled/$order-$name.conf”: ensure =&gt; $ensure ? &#123; enabled =&gt; $file, disabled =&gt; absent &#125; &#125; &#125; 然后,你就可以使用这个定义来管理一个apache虚拟主机，如下面代码所示： virtual_host &#123; “reductivelabs.com”: order =&gt; 100, ip =&gt; “192.168.0.100″, docroot =&gt; “/var/www/reductivelabs.com/htdocs” &#125; 4.5 模块一个模块就是一个/etc/puppet/modules目录下面的一个目录和它的子目录，在puppet的主文件site.pp里面用import modulename可以插入模块。新版本的puppet可以自动插入/etc/puppet/modules目录下的模块。引入模块，可以结构化代码，便于分享和管理。例如关于apache的所有配置都写到apache模块下面。 一个模块目录下面通常包括三个目录：files，manifests，templates。manifests 里面必须要包括一个init.pp的文件，这是该模块的初始（入口）文件，导入一个模块的时候，会从init.pp开始执行。可以把所有的代码都写到init.pp里面，也可以分成多个pp文件，init 再去包含其他文件。files目录是该模块的文件发布目录，puppet提供一个文件分发机制，类似rsync的模块。templates 目录包含erb模型文件，这个和file资源的template属性有关。 puppet安装好以后，modules目录是没有的，自己建立一个就行，然后在里面可以新增加你的模块。 5. 编程实例5.1 Hello World本节介绍了一个非常简单的编程实例：一个slave从master中获取其manifest，该maniftest要求slave依次做以下工作：安装gcc，创建文件夹/home/dxc/test，下载文件hello.c程序，编译hello.c。 (1) 代码结构组织Master上代码的目录结构如下： |– auth.conf |– fileserver.conf #puppet文件服务器配置文件 |– manifests #puppet主文件所在目录 | |– modules.pp #puppet各个模块汇总 | |– nodes #各个slave要处理的模块 | | `– execHello.pp #hello模块对应由那些slave处理 | `– site.pp #puppet主文件（入口文件） |– modules #puppet的各个模块所在文件 | `– hello #hello模块 | |– files #该模块对应的文件资源，可能是要发送给slave的配置文件等 | | `– hello.c | |– manifests #模块的manifest文件 | | `– init.pp #模块入口文件 | `– templates #模板目录 | `– xx.sh `– ssl #puppet的证书文件目录 (2) 程序执行流程代码调用顺序是：Slave发起连接请求 &gt; manifests/site.pp &gt;&gt; nodes &gt;&gt; modules —&gt;manifests—&gt; init.pp 首先，slave向发起master连接请求，进行证书验证； 接着，证书验证通过后，master会直接找到入口文件manifests目录下的site.pp文件，该文件可能包含一些全局变量，参数缺省值（当各个模块没有设置这些参数时，它们的缺省值）以及其它pp文件的调用（在该例子中，会调用modules.pp和nodes下的各个pp文件）； 然后，master通过nodes下的各个pp文件定位到该slave要执行的模块（init.pp是各个模块的入口），汇总这些模块代码返回给slave； 最后，slave根据master发过来的manifest，配置信息。 (3) 代码解释直接在此处 下载代码。 5.2 一个更复杂的实例本节介绍了一个更为复杂的某个公司正在使用实例，puppet代码布局与上一个实例一致，只不过该实例涉及到更多模块，更复杂的依赖管理。代码具体内容本节就不解释了，具体参见代码。 6. 可能遇到的问题Q: puppet的证书机制 A: puppet证书问题是初学者最容易遇到的问题，这里讲一下怎么处理。puppet服务器端在安装或者首次启动的时候，会自动生产一个根证书和服务器证书，证书和主机名相关，因此如果证书生成后友改了主机名，那就会出问题。 puppet客户端在首次启动的时候，也会自动生成证书；但是这个证书需要得到puppet服务器端的签名才行，因此；puppet客户端第一次连接服务器的时候，会发送一个证书请求；服务器端需要对这个证书进行签名。puppet客户端在下次连接服务器的时候就会下载签名好的证书。 Q:Ubuntu下面的证书出错，怎么解决? A:本方法是提供给初学者的测试环境，生成环境不建议这么做。首先在puppetmaster(服务器端)删除/var/lib/puppet/ssl目录;然后启动puppetmasterd；然后在客户端也删除/var/lib/puppet/ssl目录。把puppetmaster机器的主机名和对应的ip地址写入客户端机器的/etc/hosts。然后执行： puppetd –test –server server.example.com. 把server.example.com替 换成你自己的服务器主机名。 执行这个命令，会有提示信息，不用理会。然后登录到puppetmaster服务器机器，执行puppetca –list 命令，看看是否有客户端的证书请求；如果没有，请检查前面的步骤是执行正确，以及网络连接是否正常。 如果puppetca –list 能看到请求，那么执行puppetca -s -a 命令；对所有的证书请求签名。最后回到puppet客户端机器，执行 puppetd –test –server server.example.com. 就能建立连接了，如果你的site.pp写好了.就可以测试puppet了。补充：如果客户端和服务器端的时间不一致也会导致证书认证失败，因此出现证书问题的时候需要检查两台机器的时间是否一致，如果不一致用date命令或者ntpdate命令让两台机器的时间一致。 Q:出现错误[Puppet Users] err: Could not retrieve catalog; skipping run A：可能是由于安装了两个版本的ruby或者facter的原因，解决方案见：https://projects.puppetlabs.com/issues/5279 7. 总结随着服务器集群规模越来越大，自动化配置和部署这些服务器能够使管理变得非常容易并大大减小管理部署成本，因而得到IT公司的高度重视。本文档介绍了puppet，一种新型的软件自动化配置和部署工具。本文主要内容涉及puppet的架构，安装和使用方法，并给出了两个使用实例。在大规模的生成环境中，如果只有一台puppetmaster会忙不过来的，因为puppet是用ruby写的，ruby是解析型语言，每个客户端来访问，都要解析一次，当客户端多了就忙不过来，所以需要扩展成一个服务器组。puppetmaster可以看作一个web服务器，实际上也是由ruby提供的web服务器模块来做的。因此可以利用web代理软件来配合puppetmaster做集群设置，具体参见：http://puppet.wikidot.com/puppetnginx 。 8. 参考资料puppet官方网站：http://www.puppetlabs.com/ puppet中文wiki：http://puppet.chinaec2.com/ puppet中文博客：http://www.comeonsa.com 9. 代码下载(1) 5.1节实例代码下载 (2) 5.2节实例代码下载 原创文章，转载请注明： 转载自董的博客 链接地址: http://dongxicheng.org/cluster-managemant/puppet/ 作者：Dong，作者介绍：http://dongxicheng.org/about/ 本博客的文章集合:http://dongxicheng.org/recommend/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"Puppet","slug":"Puppet","permalink":"http://example.com/tags/Puppet/"}]},{"title":"SELinux状态","slug":"SELinux状态","date":"2018-02-24T21:07:07.000Z","updated":"2021-01-27T02:25:26.042Z","comments":true,"path":"2018/02/25/SELinux状态/","link":"","permalink":"http://example.com/2018/02/25/SELinux%E7%8A%B6%E6%80%81/","excerpt":"","text":"查看SELinux状态：1./usr/sbin/sestatus -v ##如果SELinux status参数为enabled即为开启状态 SELinux status: enabled 2.getenforce ##也可以用这个命令检查 关闭SELinux：1、临时关闭（不用重启机器）： setenforce 0 ##设置SELinux 成为permissive模式 setenforce 1 ##设置SELinux 成为enforcing模式 2、修改配置文件需要重启机器： 修改/etc/selinux/config 文件 将SELINUX=enforcing改为SELINUX=disabled 重启机器即可","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"selinux","slug":"selinux","permalink":"http://example.com/tags/selinux/"}]},{"title":"Use the CLI to Create Azure VM Image","slug":"Use the CLI to create Azure VM image","date":"2018-02-23T05:51:37.000Z","updated":"2021-01-27T02:25:26.043Z","comments":true,"path":"2018/02/23/Use the CLI to create Azure VM image/","link":"","permalink":"http://example.com/2018/02/23/Use%20the%20CLI%20to%20create%20Azure%20VM%20image/","excerpt":"","text":"本文大概内容自定义映像类似于应用商店映像，不同的是自定义映像的创建者是你自己。 自定义映像可用于启动配置，例如预加载应用程序、应用程序配置和其他 OS 配置。 在本教程中，你将创建自己的 Azure 虚拟机自定义映像。 你将学习如何执行以下操作： 取消预配和通用化 VM 创建自定义映像 从自定义映像创建 VM 列出订阅中的所有映像 删除映像 Note 在 Azure 中国区使用 Azure CLI 2.0 之前，请先运行 az cloud set -n AzureChinaCloud 来改变云环境。如果想切回国际版 Azure，请再次运行 az cloud set -n AzureCloud。 如果选择在本地安装并使用 CLI，本教程要求运行 Azure CLI 2.0.4 或更高版本。 运行 az –version 即可查找版本。 如果需要进行安装或升级，请参阅安装 Azure CLI 2.0。 开始之前下列步骤详细说明了如何将现有 VM 转换为可重用自定义映像，用于创建新的 VM 实例。若要完成本教程中的示例，必须现有一个虚拟机。 如果需要，此脚本示例可为你创建一个虚拟机。 按照教程进行操作时，请根据需要替换资源组和 VM 名称。 注意在创建最一开始的VM时，磁盘选项选择非托管的磁盘 创建自定义映像若要创建虚拟机的映像，需通过以下方式准备 VM：取消源 VM 的预配，解除其分配，然后将其标记为通用化。 准备好 VM 后，可以创建映像。 取消预配 VM取消预配可通过删除特定于计算机的信息来通用化 VM。 实现此通用化后，即可从单个映像部署多个 VM。 在取消预配期间，主机名将重置为“localhost.localdomain”。 还会删除 SSH 主机密钥、名称服务器配置、根密码和缓存的 DHCP 租约。 若要取消预配 VM，请使用 Azure VM 代理 (waagent)。 Azure VM 代理安装在 VM 上，用于管理预配及其与 Azure 结构控制器的交互。 有关详细信息，请参阅 Azure Linux 代理用户指南。 使用 SSH 连接到 VM 并运行命令以取消预配 VM。 使用 +user 参数还会删除上次预配的用户帐户以及任何关联的数据。 将示例 IP 地址替换为 VM 的公共 IP 地址。 通过 SSH 连接到 VM。 ssh azureuser@52.174.34.95 取消预配 VM。 sudo waagent -deprovision+user -force 注意此命令里有加+user参数，如果你的数据在VM的用户家目录下，这些数据将会被删掉，建议： 建议将数据放到非用户的家目录下，或 在取消预配VM时去掉此参数 关闭 SSH 会话。 exit 解除分配 VM 并将其标记为通用化若要创建映像，需要解除分配 VM。 使用 az vm deallocate 解除分配 VM。 az vm deallocate --resource-group myResourceGroup --name myVM 最后，使用 az vm generalize 将 VM 的状态设置为“通用化”，以便 Azure 平台知道 VM 已通用化。 只能从通用化 VM 创建映像。 az vm generalize --resource-group myResourceGroup --name myVM 创建映像现在，可使用 az image create 创建 VM 的映像。 以下示例从名为 myVM 的 VM 创建名为 myImage 的映像。 az image create \\ --resource-group myResourceGroup \\ --name myImage \\ --source myVM 从映像创建 VM现在，你已有了一个映像，可以使用 az vm create 从该映像创建一个或多个新 VM。 以下示例从名为 myImage 的映像创建名为 myVMfromImage 的 VM。 az vm create \\ --resource-group myResourceGroup \\ --name myVMfromImage \\ --image myImage \\ --admin-username azureuser \\ --generate-ssh-keys 映像管理下面是一些常见映像管理任务的示例，说明了如何使用 Azure CLI 完成这些任务。以表格格式按名称列出所有映像。 az image list \\ --resource-group myResourceGroup 删除映像。 此示例将从 myResourceGroup 中删除名为 myOldImage 的映像。 az image delete \\ --name myOldImage \\ --resource-group myResourceGroup 后续步骤在本教程中，你已创建了一个自定义 VM 映像。 你已了解如何： 取消预配和通用化 VM 创建自定义映像 从自定义映像创建 VM 列出订阅中的所有映像 删除映像","categories":[{"name":"Windows","slug":"Windows","permalink":"http://example.com/categories/Windows/"}],"tags":[{"name":"cli","slug":"cli","permalink":"http://example.com/tags/cli/"},{"name":"Azure","slug":"Azure","permalink":"http://example.com/tags/Azure/"},{"name":"image","slug":"image","permalink":"http://example.com/tags/image/"}]},{"title":"nf_conntrack: table full, dropping packet","slug":"nf_conntrack: table full, dropping packet","date":"2018-02-22T07:07:22.000Z","updated":"2021-01-27T02:25:26.050Z","comments":true,"path":"2018/02/22/nf_conntrack: table full, dropping packet/","link":"","permalink":"http://example.com/2018/02/22/nf_conntrack:%20table%20full,%20dropping%20packet/","excerpt":"","text":"写在前边的话netfilter/conntrack 相关内核参数往往是用 Linux 服务器的互联网小公司业务量上去之后遇到的第 3 个“新手怪”。（第 1 位：进程可用的 FD 不足，第 2 位：IP 临时端口不足 + TIME_WAIT 状态的连接过多导致无法建立新连接） 很多人以为 Linux 经过这么多年优化，默认参数应该“足够好”，其实不是。默认参数面向“通用”服务器，不适用于连接数和访问量比较多的场景。 症状服务器负载正常，但请求大量超时，服务器／应用访问日志看不到相关请求记录。 在 dmesg 或 /var/log/messages 看到大量以下记录： kernel: nf_conntrack: table full, dropping packet. 原因服务器访问量大，内核 netfilter 模块 conntrack 相关参数配置不合理，导致 IP 包被丢掉，连接无法建立。 详细nf_conntrack 模块在 kernel 2.6.15（2006-01-03 发布） 被引入，支持 IPv4 和 IPv6，取代只支持 IPv4 的 ip_connktrack，用于跟踪连接的状态，供其他模块使用。 需要 NAT 的服务都会用到它，例如防火墙、Docker 等。以 iptables 的 nat 和 state 模块为例： nat：根据转发规则修改 IP 包的源/目标地址，靠 conntrack 记录才能让返回的包能路由到发请求的机器。 state：直接用 conntrack 记录的连接状态（NEW/ESTABLISHED/RELATED/INVALID 等）来匹配防火墙过滤规则。 nf_conntrack 跟踪所有网络连接，记录存储在 1 个哈希表里。首先根据五元组算出哈希值，分配一个桶，如果有冲突就在链表上遍历，直到找到一个精确匹配的。如果没有匹配的则新建。 即使来自客户端的访问量不多，内部请求多的话照样会塞满哈希表，例如 ping 本机也会留下这么一条记录： ipv4 2 icmp 1 29 src=127.0.0.1 dst=127.0.0.1 type=8 code=0 id=26067 src=127.0.0.1 dst=127.0.0.1 type=0 code=0 id=26067 mark=0 use=1 连接记录会在哈希表里保留一段时间，根据协议和状态有所不同，直到超时都没有收发包就会清除记录。如果服务器比较繁忙，新连接进来的速度远高于释放的速度，把哈希表塞满了，新连接的数据包就会被丢掉。此时 netfilter 变成了一个黑洞， 这发生在3层（网络层），应用程序毫无办法。 如果有人 DDoS 攻击的话情况更糟，无论是空连接攻击还是简单地用短连接发大量请求都能轻易塞满哈希表。或者更隐蔽点，研究了计算 conntrack hash 值的算法后，构造很多 hash 一致的不同五元组的数据包，让大量记录堆在同一个桶里，使得遍历超长的冲突链表的开销大得难以接受。在当前的内核 conntrack 模块实现中，这是无法避免的（除非关掉不用），因为所有鸡蛋都在一个篮子里面。 诊断netfilter 相关内核参数一览 sudo sysctl -a | grep conntrack # 如果找不到，恭喜，不用操心这问题了查看超时相关参数 sudo sysctl -a | grep conntrack | grep timeout 所谓超时是清除 conntrack 记录的秒数，从某个连接收到最后一个包后开始倒计时， 倒数到 0 就会清除记录，中间收到包会重置。 不同协议的不同状态有不同的超时时间。（注意记录里的状态只是个标识，跟连接本身的状态不一定是一一映射的关系，跟协议的标准或实现更是完全没有关系。） 哈希表设置查看哈希表大小（桶的数量） sudo sysctl net.netfilter.nf_conntrack_buckets # 只读 查看最大跟踪连接数进来的连接数超过这个值时，新连接的包会被丢弃。 sudo sysctl net.netfilter.nf_conntrack_max # 默认 nf_conntrack_buckets * 4 # max 是 bucket 的多少倍决定了每个桶里的链表有多长，因此默认链表长度为 4 比较现代的系统（Ubuntu 16+, CentOS 7+）里，64 位，8G 内存的机器，max 通常默认为 262144，bucket 为 65536。随着内存大小翻倍这 2 个值也翻倍。 【注意】云服务厂商可能有不同的默认设置： AWS 8G 以上这 2 个值似乎不再增加，64G 内存的机器和 8G 内存的机器一样。 阿里云目前（2018年）CentOS 7+ 的机器上似乎还在用 07 年 CentOS 5 时代的默认配置：max 为 65536，bucket 为 16384。因此如果生产环境用阿里云服务器又没人了解这块的话，陷阱会来得特别早。 查看 netfilter 模块加载时的默认值 sudo dmesg | grep conntrack # 找类似这样的记录： # nf_conntrack version 0.5.0 (65536 buckets, 262144 max) 哈希表使用情况 sudo sysctl net.netfilter.nf_conntrack_count # 只读 # 这个值跟 sudo conntrack -L 或 /proc/net/nf_conntrack （如果有这文件）里的条目数一致 这个值跟 net.netfilter.nf_conntrack_buckets 的值比较。 当哈希表大部分桶不为空时（计算 得出约 69%，Python 的 dict 用 2/3，Java 的 HashMap 用 75%）哈希冲突的概率会增大，性能从 O(1) 退化为读链表的 O(n)，建议及时扩容。 网上有说法 “nf_conntrack_count 的值持续超过 nf_conntrack_max 的 20% 就该考虑扩容”也是这原因。因为 bucket 的值默认是 max 的 25%，用了 max 的 20% 也就是 80% 的桶都有元素了（假设没冲突）。 跟踪连接记录 # Ubuntu 通常没有 /proc/net/nf_conntrack 文件，用 conntrack 命令代替，输出一样 sudo conntrack -L -o extended | tail -n 50 # CentOS： sudo tail -n 50 /proc/net/nf_conntrack # 输出例： # ipv4 2 tcp 6 431999 ESTABLISHED src=10.0.13.67 dst=10.0.13.109 sport=63473 dport=22 src=10.0.13.109 dst=10.0.13.67 sport=22 dport=63473 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=0 use=2 # 记录格式： # 网络层协议名、网络层协议编号、传输层协议名、传输层协议编号、记录失效前剩余秒数、连接状态（不是所有协议都有） # 之后都是 key=value 或 flag 格式，1 行里最多 2 个同名 key（如 src 和 dst），第 1 次出现的来自请求，第 2 次出现的来自响应 # flag： # [ASSURED] 请求和响应都有流量 # [UNREPLIED] 没收到响应，哈希表满的时候这些连接先扔掉 四层协议类型和连接数： sudo conntrack -L -o extended | awk &#39;&#123;sum[$3]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; # 或： sudo cat /proc/net/nf_conntrack | awk &#39;&#123;sum[$3]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; TCP 连接各状态对应的条数： sudo conntrack -L -o extended | awk &#39;/^.*tcp.*$/ &#123;sum[$6]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; # 或： sudo cat /proc/net/nf_conntrack | awk &#39;/^.*tcp.*$/ &#123;sum[$6]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; 三层协议类型和连接数： sudo conntrack -L -o extended | awk &#39;&#123;sum[$1]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; # 或： sudo cat /proc/net/nf_conntrack | awk &#39;&#123;sum[$1]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;&#39; 连接数最多的 10 个 IP 地址： sudo conntrack -L -o extended | awk &#39;&#123;print $7&#125;&#39; | cut -d &quot;=&quot; -f 2 | sort | uniq -c | sort -nr | head -n 10 # 或： sudo cat /proc/net/nf_conntrack | awk &#39;&#123;print $7&#125;&#39; | cut -d &quot;=&quot; -f 2 | sort | uniq -c | sort -nr | head -n 10 stackoverflow - details of /proc/net/ip_conntrack / nf_conntrack 配置A. 关闭使用 NAT 的程序最常见的是防火墙，目前第 2 常见的可能是 docker。依赖 netfilter 模块的服务关掉之后，通常 sudo sysctl -a | grep conntrack 就找不到相关的参数了。 对不直接暴露在公网，也不使用 NAT 转发的服务器来说，关闭 Linux 防火墙是最简单的办法，还避免了防火墙/netfilter 成为网络瓶颈。使用公有云的话可以用厂商提供的安全服务，通常是独立于你租的云服务器的，不消耗资源，比自己用系统防火墙设一大堆规则好得多。 Ubuntu 防火墙 sudo ufw disable firewalld CentOS 7.x 默认安装。 sudo systemctl stop firewalld sudo systemctl disable firewalld iptables CentOS 6.x 默认安装。 # 使用 SystemV init 管理的旧系统： sudo service iptables stop sudo chkconfig --del iptables # 网上有些老文章说关了 iptables 之后，用 &quot;iptables -L -n&quot; 等命令查看防火墙规则也会导致 nf_conntrack 重新加载，实测并不会 # 使用 systemd 管理的新系统： sudo systemctl stop iptables sudo systemctl disable iptables dockerd 系统是最小安装的话应该不会自带。如果发现系统里有 docker 的网卡在，又确定没有地方用到 docker 的话就关掉： sudo systemctl stop docker sudo systemctl disable docker 如果 conntrack 相关参数还没消失，看看模块是不是还在： lsmod | egrep &quot;Module|ip_table|iptable|ip6|ipt|nat|conntrack&quot; # 有可能会匹配到不相关的，最好对照一下这里 find /lib/modules/$(uname -r) -type f -name &#39;*.ko*&#39; | grep netfilter # 查看模块详细信息 modinfo &lt;module&gt; 禁用模块： sudo modprobe [-f] -r &lt;module&gt; [&lt;module2&gt; ...] # 或： sudo rmmod [-f] &lt;module&gt; # 未使用（Used by 栏为 0）的模块才能禁用。 # 如果 Used by 不为 0，先禁用后面列出的模块。 # 如果后面没模块名，就是被进程使用。 # 没有简单的方法能查到调用这些模块的都是什么进程，基本靠猜。 # 查看启动信息，看有没有有用的线索（多半没有） dmesg | egrep &quot;ip_table|netfilter|conn&quot; B. 调整内核参数如果调用 netfilter 的进程不能关，或查不出什么进程在用，就要靠调整参数来尽量推迟出问题的时间。 主要设置项： 哈希表扩容（nf_conntrack_buckets、nf_conntrack_max） 让里面的元素尽快释放（超时相关参数）nf_conntrack_buckets 和 nf_conntrack_max 的默认值怎么来的根据这篇 08 年的 wiki，nf_conntrack_max 的默认值算法为： CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (ARCH / 32) 其中 ARCH 为 CPU 架构，值为 32 或 64。 即：32 位系统使用内存的 1/16384，64 位系统再减半。 对于 64 位 8G 内存的机器：(8 * 1024^3) / 16384 / (64 / 32) = 262144 nf_conntrack_buckets 默认值算法为： HASHSIZE = CONNTRACK_MAX / 4 # 比较早的版本是除以 8 # 这里的 4 或 8 就是每个桶里的链表最大长度 对于 64 位 8G 内存的机器：262144 / 4 = 65536 给哈希表扩容的影响 主要是内存使用增加。32 位系统还要关心内核态的地址空间够不够。 netfilter 的哈希表存储在内核态的内存空间，这部分内存不能 swap，操作系统为了兼容 32 位，默认值往往比较保守。 32 位系统的虚拟地址空间最多 4G，其中内核态最多 1G，通常能用的只有前 896M。给 netfilter 分配太多地址空间可能会导致其他内核进程不够分配。1 条跟踪记录 300 字节左右，因此当年 nf_conntrack_max 默认 65535 条，占 20多MB。 64 位系统的虚拟地址空间有 256TB，内核态能用一半，只需要关心物理内存的使用情况。计算内存使用的公式还是来自上面的 wiki： size_of_mem_used_by_conntrack (in bytes) = CONNTRACK_MAX * sizeof(struct ip_conntrack) + HASHSIZE * sizeof(struct list_head) sizeof(struct ip_conntrack) 在不同架构、内核版本、编译选项下不一样。这里按 352 字节算。老文章说模块启动时会在 syslog 里打印这个值，但现在没有。 sizeof(struct list_head) = 2 * size_of_a_pointer（32 位系统的指针大小是 4 字节，64 位是 8 字节） 64 位系统，8G 内存的机器，按默认 CONNTRACK_MAX 为 262144，HASHSIZE 为 65536 时：262144 * 352 + 65536 * 8 = 92798976（88.5 MB）互联网公司的服务器通常内存没那么紧张，可以放开点： CONNTRACK_MAX 为 1048576，HASHSIZE 为 262144 ：1048576 * 352 + 262144 * 8 = 371195904（354 MB）等业务发展到 nf_conntrack_count 经常保持在 18万（bucket 的 2/3）以上时再考虑翻倍。 （测试方法：压测工具不用 keep-alive 发请求，调大 nf_conntrack_tcp_timeout_time_wait，单机跑一段时间就能填满哈希表。观察响应时间的变化和服务器内存的使用情况。） 调整哪些超时时间如果你的程序需要读取 conntrack 记录，或者服务器设了复杂的 iptables 规则（同样需要读取 conntrack 记录），超时时间的设置需要非常谨慎： iptables的nf_conntrack相关参数引起两个问题 , 2015-03dog250 - Operation not permitted引发的惊魂72小时, 2013-07 （前面全是错误的排查方向，拉到第 6 点开始入正题）dog250 - 再次深入到ip_conntrack的conntrack full问题, 2012-02 如果 conntrack 记录对你不重要，用之前的命令查一下哪种协议哪种状态的连接最多，尝试把对应的超时参数调小。占比很少或根本用不到的可以不管。 例如 Nginx 服务器上可能会看到 90% 以上的记录都是 TIME_WAIT 状态（Nginx 连后端服务默认用短连接）。 对于通外网的服务器，考虑调整以下参数，减少 DDoS 的危害： net.netfilter.nf_conntrack_tcp_timeout_established：默认 432000 （5天） 这个值对应的场景是 “双方建立了连接后一直不发包，直到 5 天后才发” …… 但默认 keep-alive 超时时间只有 2 小时 11 分（net.ipv4.tcp_keepalive_time + net.ipv4.tcp_keepalive_intvl * net.ipv4.tcp_keepalive_probes），由于超时关 socket 不发包，conntrack 无法根据包头的标识知道状态的变化，记录会一直处于 ESTABLISHED 状态，直到 5 天后倒计时结束才删掉。 空连接攻击的最佳目标。攻击者把 IP 包头的源地址改成随机 IP，握完手就关 socket，用一台机发请求就能把你的哈希表填满。 net.netfilter.nf_conntrack_tcp_timeout_syn_recv：默认 60 类似，故意不发握手的 ACK 即可。但这个超时时间没那么夸张，系统也有 syn cookie 机制来缓解 syn flood 攻击。其他值得注意的参数： net.netfilter.nf_conntrack_tcp_timeout_syn_sent：默认 120 你的程序的 connect timeout 有这么长吗？ net.netfilter.nf_conntrack_tcp_timeout_fin_wait：默认 120 net.ipv4.tcp_fin_timeout 默认 60 秒，通常还会参考 BSD 和 macOS 设成更小的值。这里往往也没必要这么大。 net.netfilter.nf_conntrack_icmp_timeout：默认 30 哪里的 ping 会等 30 秒才超时？这几个倒是比较合理，小于等于可能遇到的极端情况，但如果不想半关闭的连接的记录继续占着宝贵的哈希表，提早清了似乎也没什么问题： net.netfilter.nf_conntrack_tcp_timeout_time_wait：默认 120 Linux 里的 MSL 写死 60 秒（而不是 TCP 标准里拍脑袋的 120 秒），TIME_WAIT 要等 2MSL，这里 120 算是个合理的值。 但现在默认有 PAWS（net.ipv4.tcp_timestamps），不会出现标准制定时担心的迷途报文回来碰巧污染了序列号相同的新连接的数据的情况， 互联网公司基本都开 net.ipv4.tcp_tw_reuse，既然半连接都不留这么久，记录似乎也不需要留这么久。 net.netfilter.nf_conntrack_tcp_timeout_close_wait：默认 60 CLOSE_WAIT 状态是让被动关闭方把该传的数据传完。如果程序写得不好，这里抛了未捕捉的异常，也许就走不到发 FIN 那步了，一直停在这里。 net.netfilter.nf_conntrack_tcp_timeout_last_ack：默认 30 被动关闭方发 FIN 后如果一直收不到对面的 ACK 或 RST，会不断重发，直到超时才 CLOSE。net.ipv4.tcp_retries2 的默认值是 15，最多要等 924.6 秒……不过一般都会调小这个值。 TL;DR除了有关联的参数，尽量一次只改一处，记录下默认值和上次改的值，效果不明显或更差就还原。修改完要多观察一段时间，确保不会影响业务。 net.netfilter.nf_conntrack_buckets 参数是只读的，不能直接改，需要修改模块的设置： # 改为 262144 echo 262144 | sudo tee /sys/module/nf_conntrack/parameters/hashsize # 再查看，此时 bucket 已经变成刚才设置的值 sudo sysctl net.netfilter.nf_conntrack_buckets net.netfilter.nf_conntrack_max 参考默认值，设为桶的 4 倍： sudo sysctl net.netfilter.nf_conntrack_max=1048576 # 改完可以看到 net.netfilter.nf_conntrack_max 和 net.nf_conntrack_max 都变了 超时的值要根据业务和网络环境设置，这里只是举例，不要照抄（参考了 这个做路由器的公司的设置）： sudo sysctl net.netfilter.nf_conntrack_icmp_timeout=10 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_syn_recv=5 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_syn_sent=5 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_established=600 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_fin_wait=10 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_time_wait=10 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_close_wait=10 sudo sysctl net.netfilter.nf_conntrack_tcp_timeout_last_ack=10 用 sysctl [-w] 或 echo xxx &gt; /pro/sys/net/netfilter/XXX 做的修改在重启后会失效。如果测试过没问题，在 /etc/sysctl.d/ 下新建配置文件，这里以 90-conntrack.conf 为例（CentOS 6 等旧系统编辑 /etc/sysctl.conf），系统启动时会加载里面的设置： # 格式：&lt;参数&gt;=&lt;值&gt;，等号两边可以空格，支持 # 注释 net.netfilter.nf_conntrack_max=1048576 net.netfilter.nf_conntrack_icmp_timeout=10 net.netfilter.nf_conntrack_tcp_timeout_syn_recv=5 net.netfilter.nf_conntrack_tcp_timeout_syn_sent=5 net.netfilter.nf_conntrack_tcp_timeout_established=600 net.netfilter.nf_conntrack_tcp_timeout_fin_wait=10 net.netfilter.nf_conntrack_tcp_timeout_time_wait=10 net.netfilter.nf_conntrack_tcp_timeout_close_wait=10 net.netfilter.nf_conntrack_tcp_timeout_last_ack=10 如果修改了配置文件，要马上应用配置文件里的设置： C. 设置不跟踪连接的规则对需要防火墙的机器，可以设置 NOTRACK 规则，减少要跟踪的连接数。 （注意：以下未经仔细测试，当时我们生产环境选择直接关防火墙。） 以 iptables 为例，查看所有规则： sudo iptables-save 这个必须插在第1条，凡是不跟踪的肯定是你想放行的： sudo iptables -I INPUT 1 -m state --state UNTRACKED -j ACCEPT # 设置成不跟踪的连接无法拿到状态，包含状态（-m state --state）的规则统统失效。 # iptables 处理规则的顺序是从上到下，如果这条加的位置不对，可能导致请求无法通过防火墙。 不跟踪本地连接： -t raw 会加载 iptable_raw 模块（kernel 2.6+ 都有） raw 表基本就干一件事，通过 -j NOTRACK 给不需要被连接跟踪的包打标记（UNTRACKED 状态），告诉 nf_conntrack 不要跟踪连接 raw 的优先级大于 filter，mangle，nat，包含 PREROUTING（针对进入本机的包） 和 OUTPUT（针对从本机出去的包） 链不跟踪某些端口的连接： sudo iptables -t raw -A PREROUTING -p tcp -m multiport –dports 80,443 -j NOTRACK sudo iptables -t raw -A OUTPUT -p tcp -m multiport –sports 80,443 -j NOTRACK配完防火墙规则记得留意后台服务还能不能连得上、响应时间有没有异常、某些 TCP 状态有没有异常增加…… 确定没问题就保存规则（否则重启服务后失效）： # CentOS 6 等使用 SystemV init 的旧系统： sudo service iptables save # 其实就是把 iptables-save 的内容存到 /etc/sysconfig/iptables 比较新的发行版参考以下：（未验证过） Persistent Iptables Rules in Ubuntu 16.04 Xenial Xerus, 2016-08https://serverfault.com/questions/626521/centos-7-save-iptables-settings 参考 https://www.netfilter.org/documentation/官方参数说明（说得很不清楚……）Linux连接跟踪源码分析 &amp; 源码目录RAM and conntrack performance (netfilter 开发者的答疑，原页面可能已失效，看 Google cache)wikipedia - Netfilter#Connection_Tracking 这里是简单粗暴的排查和解决方法，基本不涉及原理： stackexchange - nf_conntrack: table full, dropping packetCaveats about Linux connection tracking and high traffic servers, 2014-02解决恶心的 Nf_conntrack: Table Full 问题, 2014nf_conntrack: table full, dropping packet. 终结篇, 2015 （还不错，然而并不是终结）kernel nf_conntrack: table full, dropping packet 解决办法, 2012解决 nf_conntrack: table full, dropping packet 的几种思路, 2012通过 modprobe 彻底禁用 netfilter, 2012nf_conntrack: table full, dropping packet on Nessus server, 2014nf_conntrack: table full, dropping packet — A solution for CentOS Dedicated Servers, 2015Resolving “nf_conntrack: table full, dropping packet.” flood message in dmesg Linux kernel log, 2012 （翻墙）how do I disable the nf_conntrack kernel module in CentOS 5.3 without recompiling the kernel, 2009 以下文章的作者是 dog250，搞内核网络协议栈开发的，他的博客有很多非常深入的讲网络的文章。想进一步了解 conntrack 原理的推荐过一遍下面文章的文字和图例部分：（部分内容是关于 nf_conntrack 的前身 ip_conntrack 的） 一个复杂的nf_conntrack实例全景解析, 2017-10Linux基于mark的策略路由以及nf_conntrack RELATED, 2017-10SYNPROXY抵御DDoS攻击的原理和优化, 2017-09悲哀！作为服务器，Top 1却是fib_table_lookup, 2016-05ip_conntrack的TCP状态机, 2013-11一个Netfilter nf_conntrack流表查找的优化-为conntrack增加一个per cpu cache, 2015-08Linux协议栈优化之Netfilter分类conntrack, 2014-11linux之ip_conntrack容易混淆的问题点滴, 2012-02Linux的ip_conntrack半景, 2012-02不要盲目增加ip_conntrack_max-理解Linux内核内存, 2011-12 更多 ip_conntrack 的资料（CentOS 5、6）： ip_conntrack table full dropping packet解决方案, 2013-08一次由ip_conntrack跟踪连接库满导致的大量丢包现象排除, 2009-12关于ip_conntrack的几点认识, 2007-11CentOS ip_conntrack: table full, dropping packet 的解决方法ip_conntrack的作用, 2014-01linux内核netfilter之ip_conntrack模块的作用–抽象总结, 2010-06 其他： stackoverflow - linux 64 bits memory space size?A reason for unexplained connection timeouts on Kubernetes/Docker, 2015-02 （里面有介绍 Docker 怎么用到 SNAT）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"iptables","slug":"iptables","permalink":"http://example.com/tags/iptables/"},{"name":"firewalld","slug":"firewalld","permalink":"http://example.com/tags/firewalld/"}]}],"categories":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/categories/Linux/"},{"name":"Other","slug":"Other","permalink":"http://example.com/categories/Other/"},{"name":"Windows","slug":"Windows","permalink":"http://example.com/categories/Windows/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"腾讯云","slug":"腾讯云","permalink":"http://example.com/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"captain","slug":"captain","permalink":"http://example.com/tags/captain/"},{"name":"helm","slug":"helm","permalink":"http://example.com/tags/helm/"},{"name":"direct-lvm","slug":"direct-lvm","permalink":"http://example.com/tags/direct-lvm/"},{"name":"overlay2","slug":"overlay2","permalink":"http://example.com/tags/overlay2/"},{"name":"18.09.5","slug":"18-09-5","permalink":"http://example.com/tags/18-09-5/"},{"name":"docker升级","slug":"docker升级","permalink":"http://example.com/tags/docker%E5%8D%87%E7%BA%A7/"},{"name":"xfs","slug":"xfs","permalink":"http://example.com/tags/xfs/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"etcd","slug":"etcd","permalink":"http://example.com/tags/etcd/"},{"name":"cos","slug":"cos","permalink":"http://example.com/tags/cos/"},{"name":"FTP","slug":"FTP","permalink":"http://example.com/tags/FTP/"},{"name":"五笔","slug":"五笔","permalink":"http://example.com/tags/%E4%BA%94%E7%AC%94/"},{"name":"鼠须管","slug":"鼠须管","permalink":"http://example.com/tags/%E9%BC%A0%E9%A1%BB%E7%AE%A1/"},{"name":"输入法","slug":"输入法","permalink":"http://example.com/tags/%E8%BE%93%E5%85%A5%E6%B3%95/"},{"name":"kong","slug":"kong","permalink":"http://example.com/tags/kong/"},{"name":"kafka","slug":"kafka","permalink":"http://example.com/tags/kafka/"},{"name":"prometheus","slug":"prometheus","permalink":"http://example.com/tags/prometheus/"},{"name":"Nginx","slug":"Nginx","permalink":"http://example.com/tags/Nginx/"},{"name":"性能监控工具","slug":"性能监控工具","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7/"},{"name":"top","slug":"top","permalink":"http://example.com/tags/top/"},{"name":"vmstat","slug":"vmstat","permalink":"http://example.com/tags/vmstat/"},{"name":"iostat","slug":"iostat","permalink":"http://example.com/tags/iostat/"},{"name":"sar","slug":"sar","permalink":"http://example.com/tags/sar/"},{"name":"某云","slug":"某云","permalink":"http://example.com/tags/%E6%9F%90%E4%BA%91/"},{"name":"ace2.3","slug":"ace2-3","permalink":"http://example.com/tags/ace2-3/"},{"name":"registry","slug":"registry","permalink":"http://example.com/tags/registry/"},{"name":"Docker","slug":"Docker","permalink":"http://example.com/tags/Docker/"},{"name":"Harbor","slug":"Harbor","permalink":"http://example.com/tags/Harbor/"},{"name":"镜像","slug":"镜像","permalink":"http://example.com/tags/%E9%95%9C%E5%83%8F/"},{"name":"ES","slug":"ES","permalink":"http://example.com/tags/ES/"},{"name":"aws","slug":"aws","permalink":"http://example.com/tags/aws/"},{"name":"cli","slug":"cli","permalink":"http://example.com/tags/cli/"},{"name":"iptables","slug":"iptables","permalink":"http://example.com/tags/iptables/"},{"name":"Azure","slug":"Azure","permalink":"http://example.com/tags/Azure/"},{"name":"arm","slug":"arm","permalink":"http://example.com/tags/arm/"},{"name":"Eviction Thresholds","slug":"Eviction-Thresholds","permalink":"http://example.com/tags/Eviction-Thresholds/"},{"name":"route","slug":"route","permalink":"http://example.com/tags/route/"},{"name":"hung_task_timeout_secs","slug":"hung-task-timeout-secs","permalink":"http://example.com/tags/hung-task-timeout-secs/"},{"name":"DNS","slug":"DNS","permalink":"http://example.com/tags/DNS/"},{"name":"明朝皇帝列表","slug":"明朝皇帝列表","permalink":"http://example.com/tags/%E6%98%8E%E6%9C%9D%E7%9A%87%E5%B8%9D%E5%88%97%E8%A1%A8/"},{"name":"Ansible","slug":"Ansible","permalink":"http://example.com/tags/Ansible/"},{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://example.com/tags/GitHub/"},{"name":"Hugo","slug":"Hugo","permalink":"http://example.com/tags/Hugo/"},{"name":"lvm","slug":"lvm","permalink":"http://example.com/tags/lvm/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"system","slug":"system","permalink":"http://example.com/tags/system/"},{"name":"log","slug":"log","permalink":"http://example.com/tags/log/"},{"name":"目录","slug":"目录","permalink":"http://example.com/tags/%E7%9B%AE%E5%BD%95/"},{"name":"思路","slug":"思路","permalink":"http://example.com/tags/%E6%80%9D%E8%B7%AF/"},{"name":"曾国藩","slug":"曾国藩","permalink":"http://example.com/tags/%E6%9B%BE%E5%9B%BD%E8%97%A9/"},{"name":"命令","slug":"命令","permalink":"http://example.com/tags/%E5%91%BD%E4%BB%A4/"},{"name":"pwgen","slug":"pwgen","permalink":"http://example.com/tags/pwgen/"},{"name":"LVS","slug":"LVS","permalink":"http://example.com/tags/LVS/"},{"name":"Puppet","slug":"Puppet","permalink":"http://example.com/tags/Puppet/"},{"name":"selinux","slug":"selinux","permalink":"http://example.com/tags/selinux/"},{"name":"image","slug":"image","permalink":"http://example.com/tags/image/"},{"name":"firewalld","slug":"firewalld","permalink":"http://example.com/tags/firewalld/"}]}